{"cells":[{"cell_type":"markdown","metadata":{"id":"xZ0IEwYcRzAl"},"source":["## Requirements"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25426,"status":"ok","timestamp":1706719894118,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"iCeN-zCbXqfV","outputId":"19434be4-b8d5-4f01-ad35-3b72462ee361"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706719894118,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"NI4_17gjk3dU","outputId":"adc40f67-58a5-46ae-f7dd-706a0abfbe32"},"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}],"source":["print(1)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706719894118,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"k-JXF71-k4O3"},"outputs":[],"source":["#import os\n","#os.kill(os.getpid(), 9)"]},{"cell_type":"markdown","metadata":{"id":"TlRP-duNXnpK"},"source":["## Iniciation"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":160081,"status":"ok","timestamp":1706720054195,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"uOyfTgUDztSD","outputId":"78334649-aa12-48b0-b2cc-1386f66cdaf5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 2.1.0+cu121\n","Uninstalling torch-2.1.0+cu121:\n","  Successfully uninstalled torch-2.1.0+cu121\n","Found existing installation: torchvision 0.16.0+cu121\n","Uninstalling torchvision-0.16.0+cu121:\n","  Successfully uninstalled torchvision-0.16.0+cu121\n","Found existing installation: torchaudio 2.1.0+cu121\n","Uninstalling torchaudio-2.1.0+cu121:\n","  Successfully uninstalled torchaudio-2.1.0+cu121\n","Looking in indexes: https://download.pytorch.org/whl/cu118\n","Collecting torch\n","  Downloading https://download.pytorch.org/whl/cu118/torch-2.2.0%2Bcu118-cp310-cp310-linux_x86_64.whl (811.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.7/811.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision\n","  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.0%2Bcu118-cp310-cp310-linux_x86_64.whl (6.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchaudio\n","  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.0%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Collecting typing-extensions>=4.8.0 (from torch)\n","  Downloading https://download.pytorch.org/whl/typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.7.0.84 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu11==2.19.3 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.19.3-py3-none-manylinux1_x86_64.whl (135.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting triton==2.2.0 (from torch)\n","  Downloading https://download.pytorch.org/whl/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: typing-extensions, triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.1.0\n","    Uninstalling triton-2.1.0:\n","      Successfully uninstalled triton-2.1.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.7.13 requires torch<2.2,>=1.10, but you have torch 2.2.0+cu118 which is incompatible.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.2.0+cu118 which is incompatible.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.2.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.19.3 nvidia-nvtx-cu11-11.8.86 torch-2.2.0+cu118 torchaudio-2.2.0+cu118 torchvision-0.17.0+cu118 triton-2.2.0 typing-extensions-4.8.0\n"]}],"source":["!pip uninstall torch torchvision torchaudio -y\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706720054195,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"dSxd1Sjk6_2a","outputId":"93e65404-d26e-4005-c114-1317bfd34a82"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2023 NVIDIA Corporation\n","Built on Tue_Aug_15_22:02:13_PDT_2023\n","Cuda compilation tools, release 12.2, V12.2.140\n","Build cuda_12.2.r12.2/compiler.33191640_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1596,"status":"ok","timestamp":1706720055787,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"K0WhAIMi0KX1","outputId":"730f6c71-5e7a-4ccc-8555-812907c12aef"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.2.0+cu118\n"]}],"source":["import torch\n","print(torch.__version__)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":680,"status":"ok","timestamp":1706720056464,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"ejMJGR8w0nmM","outputId":"28d0b5ea-1699-40b7-870c-5f18c78d0e4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["absl-py==1.4.0\n","aiohttp==3.9.1\n","aiosignal==1.3.1\n","alabaster==0.7.16\n","albumentations==1.3.1\n","altair==4.2.2\n","anyio==3.7.1\n","appdirs==1.4.4\n","argon2-cffi==23.1.0\n","argon2-cffi-bindings==21.2.0\n","array-record==0.5.0\n","arviz==0.15.1\n","astropy==5.3.4\n","astunparse==1.6.3\n","async-timeout==4.0.3\n","atpublic==4.0\n","attrs==23.2.0\n","audioread==3.0.1\n","autograd==1.6.2\n","Babel==2.14.0\n","backcall==0.2.0\n","beautifulsoup4==4.11.2\n","bidict==0.22.1\n","bigframes==0.19.2\n","bleach==6.1.0\n","blinker==1.4\n","blis==0.7.11\n","blosc2==2.0.0\n","bokeh==3.3.4\n","bqplot==0.12.42\n","branca==0.7.0\n","build==1.0.3\n","CacheControl==0.13.1\n","cachetools==5.3.2\n","catalogue==2.0.10\n","certifi==2023.11.17\n","cffi==1.16.0\n","chardet==5.2.0\n","charset-normalizer==3.3.2\n","chex==0.1.7\n","click==8.1.7\n","click-plugins==1.1.1\n","cligj==0.7.2\n","cloudpickle==2.2.1\n","cmake==3.27.9\n","cmdstanpy==1.2.0\n","colorcet==3.0.1\n","colorlover==0.3.0\n","colour==0.1.5\n","community==1.0.0b1\n","confection==0.1.4\n","cons==0.4.6\n","contextlib2==21.6.0\n","contourpy==1.2.0\n","cryptography==42.0.1\n","cufflinks==0.17.3\n","cupy-cuda12x==12.2.0\n","cvxopt==1.3.2\n","cvxpy==1.3.3\n","cycler==0.12.1\n","cymem==2.0.8\n","Cython==3.0.8\n","dask==2023.8.1\n","datascience==0.17.6\n","db-dtypes==1.2.0\n","dbus-python==1.2.18\n","debugpy==1.6.6\n","decorator==4.4.2\n","defusedxml==0.7.1\n","diskcache==5.6.3\n","distributed==2023.8.1\n","distro==1.7.0\n","dlib==19.24.2\n","dm-tree==0.1.8\n","docutils==0.18.1\n","dopamine-rl==4.0.6\n","duckdb==0.9.2\n","earthengine-api==0.1.386\n","easydict==1.11\n","ecos==2.0.12\n","editdistance==0.6.2\n","eerepr==0.0.4\n","en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl#sha256=83276fc78a70045627144786b52e1f2728ad5e29e5e43916ec37ea9c26a11212\n","entrypoints==0.4\n","et-xmlfile==1.1.0\n","etils==1.6.0\n","etuples==0.3.9\n","exceptiongroup==1.2.0\n","fastai==2.7.13\n","fastcore==1.5.29\n","fastdownload==0.0.7\n","fastjsonschema==2.19.1\n","fastprogress==1.0.3\n","fastrlock==0.8.2\n","filelock==3.13.1\n","fiona==1.9.5\n","firebase-admin==5.3.0\n","Flask==2.2.5\n","flatbuffers==23.5.26\n","flax==0.8.0\n","folium==0.14.0\n","fonttools==4.47.2\n","frozendict==2.4.0\n","frozenlist==1.4.1\n","fsspec==2023.6.0\n","future==0.18.3\n","gast==0.5.4\n","gcsfs==2023.6.0\n","GDAL==3.6.4\n","gdown==4.7.3\n","geemap==0.30.4\n","gensim==4.3.2\n","geocoder==1.38.1\n","geographiclib==2.0\n","geopandas==0.13.2\n","geopy==2.3.0\n","gin-config==0.5.0\n","glob2==0.7\n","google==2.0.3\n","google-ai-generativelanguage==0.4.0\n","google-api-core==2.11.1\n","google-api-python-client==2.84.0\n","google-auth==2.17.3\n","google-auth-httplib2==0.1.1\n","google-auth-oauthlib==1.2.0\n","google-cloud-aiplatform==1.39.0\n","google-cloud-bigquery==3.12.0\n","google-cloud-bigquery-connection==1.12.1\n","google-cloud-bigquery-storage==2.24.0\n","google-cloud-core==2.3.3\n","google-cloud-datastore==2.15.2\n","google-cloud-firestore==2.11.1\n","google-cloud-functions==1.13.3\n","google-cloud-iam==2.13.0\n","google-cloud-language==2.9.1\n","google-cloud-resource-manager==1.11.0\n","google-cloud-storage==2.8.0\n","google-cloud-translate==3.11.3\n","google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz#sha256=cc4656153cd78d80c0a19379ced5d635dbb4646a8bdeb9e7be03801cfe4c65f6\n","google-crc32c==1.5.0\n","google-generativeai==0.3.2\n","google-pasta==0.2.0\n","google-resumable-media==2.7.0\n","googleapis-common-protos==1.62.0\n","googledrivedownloader==0.4\n","graphviz==0.20.1\n","greenlet==3.0.3\n","grpc-google-iam-v1==0.13.0\n","grpcio==1.60.0\n","grpcio-status==1.48.2\n","gspread==3.4.2\n","gspread-dataframe==3.3.1\n","gym==0.25.2\n","gym-notices==0.0.8\n","h5netcdf==1.3.0\n","h5py==3.9.0\n","holidays==0.41\n","holoviews==1.17.1\n","html5lib==1.1\n","httpimport==1.3.1\n","httplib2==0.22.0\n","huggingface-hub==0.20.3\n","humanize==4.7.0\n","hyperopt==0.2.7\n","ibis-framework==7.1.0\n","idna==3.6\n","imageio==2.31.6\n","imageio-ffmpeg==0.4.9\n","imagesize==1.4.1\n","imbalanced-learn==0.10.1\n","imgaug==0.4.0\n","importlib-metadata==7.0.1\n","importlib-resources==6.1.1\n","imutils==0.5.4\n","inflect==7.0.0\n","iniconfig==2.0.0\n","install==1.3.5\n","intel-openmp==2023.2.3\n","ipyevents==2.0.2\n","ipyfilechooser==0.6.0\n","ipykernel==5.5.6\n","ipyleaflet==0.18.2\n","ipython==7.34.0\n","ipython-genutils==0.2.0\n","ipython-sql==0.5.0\n","ipytree==0.2.2\n","ipywidgets==7.7.1\n","itsdangerous==2.1.2\n","jax==0.4.23\n","jaxlib @ https://storage.googleapis.com/jax-releases/cuda12/jaxlib-0.4.23+cuda12.cudnn89-cp310-cp310-manylinux2014_x86_64.whl#sha256=8e42000672599e7ec0ea7f551acfcc95dcdd0e22b05a1d1f12f97b56a9fce4a8\n","jeepney==0.7.1\n","jieba==0.42.1\n","Jinja2==3.1.3\n","joblib==1.3.2\n","jsonpickle==3.0.2\n","jsonschema==4.19.2\n","jsonschema-specifications==2023.12.1\n","jupyter-client==6.1.12\n","jupyter-console==6.1.0\n","jupyter-server==1.24.0\n","jupyter_core==5.7.1\n","jupyterlab-widgets==3.0.9\n","jupyterlab_pygments==0.3.0\n","kaggle==1.5.16\n","kagglehub==0.1.6\n","keras==2.15.0\n","keyring==23.5.0\n","kiwisolver==1.4.5\n","langcodes==3.3.0\n","launchpadlib==1.10.16\n","lazr.restfulclient==0.14.4\n","lazr.uri==1.0.6\n","lazy_loader==0.3\n","libclang==16.0.6\n","librosa==0.10.1\n","lida==0.0.10\n","lightgbm==4.1.0\n","linkify-it-py==2.0.2\n","llmx==0.0.15a0\n","llvmlite==0.41.1\n","locket==1.0.0\n","logical-unification==0.4.6\n","lxml==4.9.4\n","malloy==2023.1067\n","Markdown==3.5.2\n","markdown-it-py==3.0.0\n","MarkupSafe==2.1.4\n","matplotlib==3.7.1\n","matplotlib-inline==0.1.6\n","matplotlib-venn==0.11.9\n","mdit-py-plugins==0.4.0\n","mdurl==0.1.2\n","miniKanren==1.0.3\n","missingno==0.5.2\n","mistune==0.8.4\n","mizani==0.9.3\n","mkl==2023.2.0\n","ml-dtypes==0.2.0\n","mlxtend==0.22.0\n","more-itertools==10.1.0\n","moviepy==1.0.3\n","mpmath==1.3.0\n","msgpack==1.0.7\n","multidict==6.0.4\n","multipledispatch==1.0.0\n","multitasking==0.0.11\n","murmurhash==1.0.10\n","music21==9.1.0\n","natsort==8.4.0\n","nbclassic==1.0.0\n","nbclient==0.9.0\n","nbconvert==6.5.4\n","nbformat==5.9.2\n","nest-asyncio==1.6.0\n","networkx==3.2.1\n","nibabel==4.0.2\n","nltk==3.8.1\n","notebook==6.5.5\n","notebook_shim==0.2.3\n","numba==0.58.1\n","numexpr==2.9.0\n","numpy==1.23.5\n","nvidia-cublas-cu11==11.11.3.6\n","nvidia-cuda-cupti-cu11==11.8.87\n","nvidia-cuda-nvrtc-cu11==11.8.89\n","nvidia-cuda-runtime-cu11==11.8.89\n","nvidia-cudnn-cu11==8.7.0.84\n","nvidia-cufft-cu11==10.9.0.58\n","nvidia-curand-cu11==10.3.0.86\n","nvidia-cusolver-cu11==11.4.1.48\n","nvidia-cusparse-cu11==11.7.5.86\n","nvidia-nccl-cu11==2.19.3\n","nvidia-nvtx-cu11==11.8.86\n","oauth2client==4.1.3\n","oauthlib==3.2.2\n","opencv-contrib-python==4.8.0.76\n","opencv-python==4.8.0.76\n","opencv-python-headless==4.9.0.80\n","openpyxl==3.1.2\n","opt-einsum==3.3.0\n","optax==0.1.8\n","orbax-checkpoint==0.4.4\n","osqp==0.6.2.post8\n","packaging==23.2\n","pandas==1.5.3\n","pandas-datareader==0.10.0\n","pandas-gbq==0.19.2\n","pandas-stubs==1.5.3.230304\n","pandocfilters==1.5.1\n","panel==1.3.8\n","param==2.0.2\n","parso==0.8.3\n","parsy==2.1\n","partd==1.4.1\n","pathlib==1.0.1\n","pathlib_abc==0.1.1\n","pathy==0.11.0\n","patsy==0.5.6\n","peewee==3.17.0\n","pexpect==4.9.0\n","pickleshare==0.7.5\n","Pillow==9.4.0\n","pins==0.8.4\n","pip-tools==6.13.0\n","platformdirs==4.1.0\n","plotly==5.15.0\n","plotnine==0.12.4\n","pluggy==1.4.0\n","polars==0.20.2\n","pooch==1.8.0\n","portpicker==1.5.2\n","prefetch-generator==1.0.3\n","preshed==3.0.9\n","prettytable==3.9.0\n","proglog==0.1.10\n","progressbar2==4.2.0\n","prometheus-client==0.19.0\n","promise==2.3\n","prompt-toolkit==3.0.43\n","prophet==1.1.5\n","proto-plus==1.23.0\n","protobuf==3.20.3\n","psutil==5.9.5\n","psycopg2==2.9.9\n","ptyprocess==0.7.0\n","py-cpuinfo==9.0.0\n","py4j==0.10.9.7\n","pyarrow==10.0.1\n","pyarrow-hotfix==0.6\n","pyasn1==0.5.1\n","pyasn1-modules==0.3.0\n","pycocotools==2.0.7\n","pycparser==2.21\n","pyct==0.5.0\n","pydantic==1.10.14\n","pydata-google-auth==1.8.2\n","pydot==1.4.2\n","pydot-ng==2.0.0\n","pydotplus==2.0.2\n","PyDrive==1.3.1\n","PyDrive2==1.6.3\n","pyerfa==2.0.1.1\n","pygame==2.5.2\n","Pygments==2.16.1\n","PyGObject==3.42.1\n","PyJWT==2.3.0\n","pymc==5.7.2\n","pymystem3==0.2.0\n","PyOpenGL==3.1.7\n","pyOpenSSL==24.0.0\n","pyparsing==3.1.1\n","pyperclip==1.8.2\n","pyproj==3.6.1\n","pyproject_hooks==1.0.0\n","pyshp==2.3.1\n","PySocks==1.7.1\n","pytensor==2.14.2\n","pytest==7.4.4\n","python-apt==0.0.0\n","python-box==7.1.1\n","python-dateutil==2.8.2\n","python-louvain==0.16\n","python-slugify==8.0.2\n","python-utils==3.8.2\n","pytz==2023.3.post1\n","pyviz_comms==3.0.1\n","PyWavelets==1.5.0\n","PyYAML==6.0.1\n","pyzmq==23.2.1\n","qdldl==0.1.7.post0\n","qudida==0.0.4\n","ratelim==0.1.6\n","referencing==0.32.1\n","regex==2023.6.3\n","requests==2.31.0\n","requests-oauthlib==1.3.1\n","requirements-parser==0.5.0\n","rich==13.7.0\n","rpds-py==0.17.1\n","rpy2==3.4.2\n","rsa==4.9\n","safetensors==0.4.2\n","scikit-image==0.19.3\n","scikit-learn==1.2.2\n","scipy==1.11.4\n","scooby==0.9.2\n","scs==3.2.4.post1\n","seaborn==0.13.1\n","SecretStorage==3.3.1\n","Send2Trash==1.8.2\n","sentencepiece==0.1.99\n","shapely==2.0.2\n","six==1.16.0\n","sklearn-pandas==2.2.0\n","smart-open==6.4.0\n","sniffio==1.3.0\n","snowballstemmer==2.2.0\n","sortedcontainers==2.4.0\n","soundfile==0.12.1\n","soupsieve==2.5\n","soxr==0.3.7\n","spacy==3.6.1\n","spacy-legacy==3.0.12\n","spacy-loggers==1.0.5\n","Sphinx==5.0.2\n","sphinxcontrib-applehelp==1.0.8\n","sphinxcontrib-devhelp==1.0.6\n","sphinxcontrib-htmlhelp==2.0.5\n","sphinxcontrib-jsmath==1.0.1\n","sphinxcontrib-qthelp==1.0.7\n","sphinxcontrib-serializinghtml==1.1.10\n","SQLAlchemy==2.0.24\n","sqlglot==19.9.0\n","sqlparse==0.4.4\n","srsly==2.4.8\n","stanio==0.3.0\n","statsmodels==0.14.1\n","sympy==1.12\n","tables==3.8.0\n","tabulate==0.9.0\n","tbb==2021.11.0\n","tblib==3.0.0\n","tenacity==8.2.3\n","tensorboard==2.15.1\n","tensorboard-data-server==0.7.2\n","tensorflow==2.15.0\n","tensorflow-datasets==4.9.4\n","tensorflow-estimator==2.15.0\n","tensorflow-gcs-config==2.15.0\n","tensorflow-hub==0.16.0\n","tensorflow-io-gcs-filesystem==0.35.0\n","tensorflow-metadata==1.14.0\n","tensorflow-probability==0.22.0\n","tensorstore==0.1.45\n","termcolor==2.4.0\n","terminado==0.18.0\n","text-unidecode==1.3\n","textblob==0.17.1\n","tf-slim==1.1.0\n","thinc==8.1.12\n","threadpoolctl==3.2.0\n","tifffile==2023.12.9\n","tinycss2==1.2.1\n","tokenizers==0.15.1\n","toml==0.10.2\n","tomli==2.0.1\n","toolz==0.12.1\n","torch==2.2.0+cu118\n","torchaudio==2.2.0+cu118\n","torchdata==0.7.0\n","torchsummary==1.5.1\n","torchtext==0.16.0\n","torchvision==0.17.0+cu118\n","tornado==6.3.2\n","tqdm==4.66.1\n","traitlets==5.7.1\n","traittypes==0.2.1\n","transformers==4.35.2\n","triton==2.2.0\n","tweepy==4.14.0\n","typer==0.9.0\n","types-pytz==2023.3.1.1\n","types-setuptools==69.0.0.20240125\n","typing_extensions==4.8.0\n","tzlocal==5.2\n","uc-micro-py==1.0.2\n","uritemplate==4.1.1\n","urllib3==2.0.7\n","vega-datasets==0.9.0\n","wadllib==1.3.6\n","wasabi==1.1.2\n","wcwidth==0.2.13\n","webcolors==1.13\n","webencodings==0.5.1\n","websocket-client==1.7.0\n","Werkzeug==3.0.1\n","widgetsnbextension==3.6.6\n","wordcloud==1.9.3\n","wrapt==1.14.1\n","xarray==2023.7.0\n","xarray-einstats==0.7.0\n","xgboost==2.0.3\n","xlrd==2.0.1\n","xxhash==3.4.1\n","xyzservices==2023.10.1\n","yarl==1.9.4\n","yellowbrick==1.5\n","yfinance==0.2.36\n","zict==3.0.0\n","zipp==3.17.0\n"]}],"source":["!pip freeze"]},{"cell_type":"markdown","metadata":{"id":"yUSNtuVeTUwl"},"source":["## Optimazer"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706720056464,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"HZMtCepVRDaJ"},"outputs":[],"source":["# Based on https://github.com/pytorch/pytorch/pull/3740\n","import torch\n","import math\n","\n","\n","class AdamW(torch.optim.Optimizer):\n","    \"\"\"Implements AdamW algorithm.\n","\n","    It has been proposed in `Fixing Weight Decay Regularization in Adam`_.\n","\n","    Arguments:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float, optional): learning rate (default: 1e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of gradient and its square (default: (0.9, 0.999))\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability (default: 1e-8)\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","\n","    .. Fixing Weight Decay Regularization in Adam:\n","    https://arxiv.org/abs/1711.05101\n","    \"\"\"\n","\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=0):\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay)\n","        super(AdamW, self).__init__(params, defaults)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n","\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","\n","                # according to the paper, this penalty should come after the bias correction\n","                # if group['weight_decay'] != 0:\n","                #     grad = grad.add(group['weight_decay'], p.data)\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","\n","                denom = exp_avg_sq.sqrt().add_(group['eps'])\n","\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n","\n","                # w = w - wd * lr * w\n","                if group['weight_decay'] != 0:\n","                    p.data.add_(-group['weight_decay'] * group['lr'], p.data)\n","\n","                # w = w - lr * w.grad\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                # w = w - wd * lr * w - lr * w.grad\n","                # See http://www.fast.ai/2018/07/02/adam-weight-decay/\n","\n","        return loss"]},{"cell_type":"markdown","metadata":{"id":"pTW3yt84TQMf"},"source":["## Losses"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706720056974,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"MR5-L7m_SCwH"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch.autograd import Variable\n","\n","try:\n","    from itertools import ifilterfalse\n","except ImportError:  # py3k\n","    from itertools import filterfalse\n","\n","eps = 1e-6\n","\n","def dice_round(preds, trues):\n","    preds = preds.float()\n","    return soft_dice_loss(preds, trues)\n","\n","\n","def iou_round(preds, trues):\n","    preds = preds.float()\n","    return jaccard(preds, trues)\n","\n","\n","def soft_dice_loss(outputs, targets, per_image=False):\n","    batch_size = outputs.size()[0]\n","    if not per_image:\n","        batch_size = 1\n","    dice_target = targets.contiguous().view(batch_size, -1).float()\n","    dice_output = outputs.contiguous().view(batch_size, -1)\n","    intersection = torch.sum(dice_output * dice_target, dim=1)\n","    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1) + eps\n","    loss = (1 - (2 * intersection + eps) / union).mean()\n","    return loss\n","\n","\n","def jaccard(outputs, targets, per_image=False):\n","    batch_size = outputs.size()[0]\n","    if not per_image:\n","        batch_size = 1\n","    dice_target = targets.contiguous().view(batch_size, -1).float()\n","    dice_output = outputs.contiguous().view(batch_size, -1)\n","    intersection = torch.sum(dice_output * dice_target, dim=1)\n","    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1) - intersection + eps\n","    losses = 1 - (intersection + eps) / union\n","    return losses.mean()\n","\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True, per_image=False):\n","        super().__init__()\n","        self.size_average = size_average\n","        self.register_buffer('weight', weight)\n","        self.per_image = per_image\n","\n","    def forward(self, input, target):\n","        return soft_dice_loss(input, target, per_image=self.per_image)\n","\n","\n","class JaccardLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True, per_image=False):\n","        super().__init__()\n","        self.size_average = size_average\n","        self.register_buffer('weight', weight)\n","        self.per_image = per_image\n","\n","    def forward(self, input, target):\n","        return jaccard(input, target, per_image=self.per_image)\n","\n","\n","class StableBCELoss(nn.Module):\n","    def __init__(self):\n","        super(StableBCELoss, self).__init__()\n","\n","    def forward(self, input, target):\n","        input = input.float().view(-1)\n","        target = target.float().view(-1)\n","        neg_abs = - input.abs()\n","        # todo check correctness\n","        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n","        return loss.mean()\n","\n","\n","class ComboLoss(nn.Module):\n","    def __init__(self, weights, per_image=False):\n","        super().__init__()\n","        self.weights = weights\n","        self.bce = StableBCELoss()\n","        self.dice = DiceLoss(per_image=False)\n","        self.jaccard = JaccardLoss(per_image=False)\n","        self.lovasz = LovaszLoss(per_image=per_image)\n","        self.lovasz_sigmoid = LovaszLossSigmoid(per_image=per_image)\n","        self.focal = FocalLoss2d()\n","        self.mapping = {'bce': self.bce,\n","                        'dice': self.dice,\n","                        'focal': self.focal,\n","                        'jaccard': self.jaccard,\n","                        'lovasz': self.lovasz,\n","                        'lovasz_sigmoid': self.lovasz_sigmoid}\n","        self.expect_sigmoid = {'dice', 'focal', 'jaccard', 'lovasz_sigmoid'}\n","        self.values = {}\n","\n","    def forward(self, outputs, targets):\n","        loss = 0\n","        weights = self.weights\n","        sigmoid_input = torch.sigmoid(outputs)\n","        for k, v in weights.items():\n","            if not v:\n","                continue\n","            val = self.mapping[k](sigmoid_input if k in self.expect_sigmoid else outputs, targets)\n","            self.values[k] = val\n","            loss += self.weights[k] * val\n","        return loss\n","\n","\n","def lovasz_grad(gt_sorted):\n","    \"\"\"\n","    Computes gradient of the Lovasz extension w.r.t sorted errors\n","    See Alg. 1 in paper\n","    \"\"\"\n","    p = len(gt_sorted)\n","    gts = gt_sorted.sum()\n","    intersection = gts.float() - gt_sorted.float().cumsum(0)\n","    union = gts.float() + (1 - gt_sorted).float().cumsum(0)\n","    jaccard = 1. - intersection / union\n","    if p > 1:  # cover 1-pixel case\n","        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n","    return jaccard\n","\n","\n","def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n","      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class id\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n","                    for log, lab in zip(logits, labels))\n","    else:\n","        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n","    return loss\n","\n","\n","def lovasz_hinge_flat(logits, labels):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n","      labels: [P] Tensor, binary ground truth labels (0 or 1)\n","      ignore: label to ignore\n","    \"\"\"\n","    if len(labels) == 0:\n","        # only void pixels, the gradients should be 0\n","        return logits.sum() * 0.\n","    signs = 2. * labels.float() - 1.\n","    errors = (1. - logits * Variable(signs))\n","    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n","    perm = perm.data\n","    gt_sorted = labels[perm]\n","    grad = lovasz_grad(gt_sorted)\n","    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n","    return loss\n","\n","\n","def flatten_binary_scores(scores, labels, ignore=None):\n","    \"\"\"\n","    Flattens predictions in the batch (binary case)\n","    Remove labels equal to 'ignore'\n","    \"\"\"\n","    scores = scores.view(-1)\n","    labels = labels.view(-1)\n","    if ignore is None:\n","        return scores, labels\n","    valid = (labels != ignore)\n","    vscores = scores[valid]\n","    vlabels = labels[valid]\n","    return vscores, vlabels\n","\n","\n","def lovasz_sigmoid(probas, labels, per_image=False, ignore=None):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n","      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n","      only_present: average only on classes present in ground truth\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class labels\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_sigmoid_flat(*flatten_binary_scores(prob.unsqueeze(0), lab.unsqueeze(0), ignore))\n","                          for prob, lab in zip(probas, labels))\n","    else:\n","        loss = lovasz_sigmoid_flat(*flatten_binary_scores(probas, labels, ignore))\n","    return loss\n","\n","\n","def lovasz_sigmoid_flat(probas, labels):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n","      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n","      only_present: average only on classes present in ground truth\n","    \"\"\"\n","    fg = labels.float()\n","    errors = (Variable(fg) - probas).abs()\n","    errors_sorted, perm = torch.sort(errors, 0, descending=True)\n","    perm = perm.data\n","    fg_sorted = fg[perm]\n","    loss = torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted)))\n","    return loss\n","\n","\n","def mean(l, ignore_nan=False, empty=0):\n","    \"\"\"\n","    nanmean compatible with generators.\n","    \"\"\"\n","    l = iter(l)\n","    if ignore_nan:\n","        l = ifilterfalse(np.isnan, l)\n","    try:\n","        n = 1\n","        acc = next(l)\n","    except StopIteration:\n","        if empty == 'raise':\n","            raise ValueError('Empty mean')\n","        return empty\n","    for n, v in enumerate(l, 2):\n","        acc += v\n","    if n == 1:\n","        return acc\n","    return acc / n\n","\n","\n","class LovaszLoss(nn.Module):\n","    def __init__(self, ignore_index=255, per_image=True):\n","        super().__init__()\n","        self.ignore_index = ignore_index\n","        self.per_image = per_image\n","\n","    def forward(self, outputs, targets):\n","        outputs = outputs.contiguous()\n","        targets = targets.contiguous()\n","        return lovasz_hinge(outputs, targets, per_image=self.per_image, ignore=self.ignore_index)\n","\n","\n","class LovaszLossSigmoid(nn.Module):\n","    def __init__(self, ignore_index=255, per_image=True):\n","        super().__init__()\n","        self.ignore_index = ignore_index\n","        self.per_image = per_image\n","\n","    def forward(self, outputs, targets):\n","        outputs = outputs.contiguous()\n","        targets = targets.contiguous()\n","        return lovasz_sigmoid(outputs, targets, per_image=self.per_image, ignore=self.ignore_index)\n","\n","\n","class FocalLoss2d(nn.Module):\n","    def __init__(self, gamma=2, ignore_index=255):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.ignore_index = ignore_index\n","\n","    def forward(self, outputs, targets):\n","        outputs = outputs.contiguous()\n","        targets = targets.contiguous()\n","        # eps = 1e-8\n","        non_ignored = targets.view(-1) != self.ignore_index\n","        targets = targets.view(-1)[non_ignored].float()\n","        outputs = outputs.contiguous().view(-1)[non_ignored]\n","        outputs = torch.clamp(outputs, eps, 1. - eps)\n","        targets = torch.clamp(targets, eps, 1. - eps)\n","        pt = (1 - targets) * (1 - outputs) + targets * outputs\n","        return (-(1. - pt) ** self.gamma * torch.log(pt)).mean()"]},{"cell_type":"markdown","metadata":{"id":"FXa807PbTjxn"},"source":["## Utils"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706720056974,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"7Fs2XBvfToVf"},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","#### Augmentations\n","def shift_image(img, shift_pnt):\n","    M = np.float32([[1, 0, shift_pnt[0]], [0, 1, shift_pnt[1]]])\n","    res = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]), borderMode=cv2.BORDER_REFLECT_101)\n","    return res\n","\n","\n","def rotate_image(image, angle, scale, rot_pnt):\n","    rot_mat = cv2.getRotationMatrix2D(rot_pnt, angle, scale)\n","    result = cv2.warpAffine(image, rot_mat, (image.shape[1], image.shape[0]), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101) #INTER_NEAREST\n","    return result\n","\n","\n","def gauss_noise(img, var=30):\n","    row, col, ch = img.shape\n","    mean = var\n","    sigma = var**0.5\n","    gauss = np.random.normal(mean,sigma,(row,col,ch))\n","    gauss = gauss.reshape(row,col,ch)\n","    gauss = (gauss - np.min(gauss)).astype(np.uint8)\n","    return np.clip(img.astype(np.int32) + gauss, 0, 255).astype('uint8')\n","\n","\n","def clahe(img, clipLimit=2.0, tileGridSize=(5,5)):\n","    img_yuv = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n","    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n","    img_yuv[:, :, 0] = clahe.apply(img_yuv[:, :, 0])\n","    img_output = cv2.cvtColor(img_yuv, cv2.COLOR_LAB2RGB)\n","    return img_output\n","\n","\n","def _blend(img1, img2, alpha):\n","    return np.clip(img1 * alpha + (1 - alpha) * img2, 0, 255).astype('uint8')\n","\n","\n","_alpha = np.asarray([0.114, 0.587, 0.299]).reshape((1, 1, 3))\n","def _grayscale(img):\n","    return np.sum(_alpha * img, axis=2, keepdims=True)\n","\n","\n","def saturation(img, alpha):\n","    gs = _grayscale(img)\n","    return _blend(img, gs, alpha)\n","\n","\n","def brightness(img, alpha):\n","    gs = np.zeros_like(img)\n","    return _blend(img, gs, alpha)\n","\n","\n","def contrast(img, alpha):\n","    gs = _grayscale(img)\n","    gs = np.repeat(gs.mean(), 3)\n","    return _blend(img, gs, alpha)\n","\n","\n","def change_hsv(img, h, s, v):\n","    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","    hsv = hsv.astype(int)\n","    hsv[:,:,0] += h\n","    hsv[:,:,0] = np.clip(hsv[:,:,0], 0, 255)\n","    hsv[:,:,1] += s\n","    hsv[:,:,1] = np.clip(hsv[:,:,1], 0, 255)\n","    hsv[:,:,2] += v\n","    hsv[:,:,2] = np.clip(hsv[:,:,2], 0, 255)\n","    hsv = hsv.astype('uint8')\n","    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","    return img\n","\n","def shift_channels(img, b_shift, g_shift, r_shift):\n","    img = img.astype(int)\n","    img[:,:,0] += b_shift\n","    img[:,:,0] = np.clip(img[:,:,0], 0, 255)\n","    img[:,:,1] += g_shift\n","    img[:,:,1] = np.clip(img[:,:,1], 0, 255)\n","    img[:,:,2] += r_shift\n","    img[:,:,2] = np.clip(img[:,:,2], 0, 255)\n","    img = img.astype('uint8')\n","    return img\n","\n","def invert(img):\n","    return 255 - img\n","\n","def channel_shuffle(img):\n","    ch_arr = [0, 1, 2]\n","    np.random.shuffle(ch_arr)\n","    img = img[..., ch_arr]\n","    return img\n","\n","#######\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","\n","def preprocess_inputs(x):\n","    x = np.asarray(x, dtype='float32')\n","    x /= 127\n","    x -= 1\n","    return x\n","\n","\n","def dice(im1, im2, empty_score=1.0):\n","    \"\"\"\n","    Computes the Dice coefficient, a measure of set similarity.\n","    Parameters\n","    ----------\n","    im1 : array-like, bool\n","        Any array of arbitrary size. If not boolean, will be converted.\n","    im2 : array-like, bool\n","        Any other array of identical size. If not boolean, will be converted.\n","    Returns\n","    -------\n","    dice : float\n","        Dice coefficient as a float on range [0,1].\n","        Maximum similarity = 1\n","        No similarity = 0\n","        Both are empty (sum eq to zero) = empty_score\n","\n","    Notes\n","    -----\n","    The order of inputs for `dice` is irrelevant. The result will be\n","    identical if `im1` and `im2` are switched.\n","    \"\"\"\n","    im1 = np.asarray(im1).astype(np.bool)\n","    im2 = np.asarray(im2).astype(np.bool)\n","\n","    if im1.shape != im2.shape:\n","        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n","\n","    im_sum = im1.sum() + im2.sum()\n","    if im_sum == 0:\n","        return empty_score\n","\n","    # Compute Dice coefficient\n","    intersection = np.logical_and(im1, im2)\n","\n","    return 2. * intersection.sum() / im_sum\n","\n","\n","def iou(im1, im2, empty_score=1.0):\n","    im1 = np.asarray(im1).astype(np.bool)\n","    im2 = np.asarray(im2).astype(np.bool)\n","\n","    if im1.shape != im2.shape:\n","        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n","\n","    union = np.logical_or(im1, im2)\n","    im_sum = union.sum()\n","    if im_sum == 0:\n","        return empty_score\n","\n","    # Compute Dice coefficient\n","    intersection = np.logical_and(im1, im2)\n","\n","    return intersection.sum() / im_sum"]},{"cell_type":"markdown","metadata":{"id":"Ifa7HjK2FVf3"},"source":["## modelMscale"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3052,"status":"ok","timestamp":1706720060024,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"RoOkAXB2n4nS"},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import Conv2D, Activation, UpSampling2D, Concatenate, Input\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.initializers import HeNormal\n","from tensorflow.keras import backend as K\n","\n","class ConvRelu(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3):\n","        super(ConvRelu, self).__init__()\n","        self.layer = nn.Sequential(\n","            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1),\n","            nn.ReLU(inplace=True, )\n","        )\n","    #@autocast()\n","    def forward(self, x):\n","        return self.layer(x)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":605,"status":"ok","timestamp":1706720060627,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"sDIsasbMn5W8"},"outputs":[],"source":["\"\"\"\n","ResNet code gently borrowed from\n","https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n","\"\"\"\n","\n","from collections import OrderedDict\n","import math\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils import model_zoo\n","\n","__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n","           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n","\n","pretrained_settings = {\n","    'senet154': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnet50': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnet101': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnet152': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnext50_32x4d': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnext101_32x4d': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","}\n","\n","\n","class SEModule(nn.Module):\n","\n","    def __init__(self, channels, reduction, concat=False):\n","        super(SEModule, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n","                             padding=0)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n","                             padding=0)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        module_input = x\n","        x = self.avg_pool(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.sigmoid(x)\n","        return module_input * x\n","\n","class SCSEModule(nn.Module):\n","    # according to https://arxiv.org/pdf/1808.08127.pdf concat is better\n","    def __init__(self, channels, reduction=16, concat=False):\n","        super(SCSEModule, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n","                             padding=0)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n","                             padding=0)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        self.spatial_se = nn.Sequential(nn.Conv2d(channels, 1, kernel_size=1,\n","                                                  stride=1, padding=0, bias=False),\n","                                        nn.Sigmoid())\n","        self.concat = concat\n","\n","    def forward(self, x):\n","        module_input = x\n","\n","        x = self.avg_pool(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        chn_se = self.sigmoid(x)\n","        chn_se = chn_se * module_input\n","\n","        spa_se = self.spatial_se(module_input)\n","        spa_se = module_input * spa_se\n","        if self.concat:\n","            return torch.cat([chn_se, spa_se], dim=1)\n","        else:\n","            return chn_se + spa_se\n","\n","class Bottleneck(nn.Module):\n","    \"\"\"\n","    Base class for bottlenecks that implements `forward()` method.\n","    \"\"\"\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out = self.se_module(out) + residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class SEBottleneck(Bottleneck):\n","    \"\"\"\n","    Bottleneck for SENet154.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None):\n","        super(SEBottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes * 2)\n","        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n","                               stride=stride, padding=1, groups=groups,\n","                               bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes * 4)\n","        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n","                               bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SCSEBottleneck(Bottleneck):\n","    \"\"\"\n","    Bottleneck for SENet154.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None):\n","        super(SCSEBottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes * 2)\n","        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n","                               stride=stride, padding=1, groups=groups,\n","                               bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes * 4)\n","        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n","                               bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SCSEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SEResNetBottleneck(Bottleneck):\n","    \"\"\"\n","    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n","    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n","    (the latter is used in the torchvision implementation of ResNet).\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None):\n","        super(SEResNetBottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n","                               stride=stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n","                               groups=groups, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SEResNeXtBottleneck(Bottleneck):\n","    \"\"\"\n","    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None, base_width=4):\n","        super(SEResNeXtBottleneck, self).__init__()\n","        width = math.floor(planes * (base_width / 64)) * groups\n","        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n","                               stride=1)\n","        self.bn1 = nn.BatchNorm2d(width)\n","        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n","                               padding=1, groups=groups, bias=False)\n","        self.bn2 = nn.BatchNorm2d(width)\n","        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","\n","class SCSEResNeXtBottleneck(Bottleneck):\n","    \"\"\"\n","    ResNeXt bottleneck type C with a Concurrent Spatial Squeeze-and-Excitation module.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None, base_width=4, final=False):\n","        super(SCSEResNeXtBottleneck, self).__init__()\n","        width = math.floor(planes * (base_width / 64)) * groups\n","        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n","                               stride=1)\n","        self.bn1 = nn.BatchNorm2d(width)\n","        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n","                               padding=1, groups=groups, bias=False)\n","        self.bn2 = nn.BatchNorm2d(width)\n","        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SCSEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SENet(nn.Module):\n","\n","    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n","                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n","                 downsample_padding=1, num_classes=1000):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        block (nn.Module): Bottleneck class.\n","            - For SENet154: SEBottleneck\n","            - For SE-ResNet models: SEResNetBottleneck\n","            - For SE-ResNeXt models:  SEResNeXtBottleneck\n","        layers (list of ints): Number of residual blocks for 4 layers of the\n","            network (layer1...layer4).\n","        groups (int): Number of groups for the 3x3 convolution in each\n","            bottleneck block.\n","            - For SENet154: 64\n","            - For SE-ResNet models: 1\n","            - For SE-ResNeXt models:  32\n","        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n","            - For all models: 16\n","        dropout_p (float or None): Drop probability for the Dropout layer.\n","            If `None` the Dropout layer is not used.\n","            - For SENet154: 0.2\n","            - For SE-ResNet models: None\n","            - For SE-ResNeXt models: None\n","        inplanes (int):  Number of input channels for layer1.\n","            - For SENet154: 128\n","            - For SE-ResNet models: 64\n","            - For SE-ResNeXt models: 64\n","        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n","            a single 7x7 convolution in layer0.\n","            - For SENet154: True\n","            - For SE-ResNet models: False\n","            - For SE-ResNeXt models: False\n","        downsample_kernel_size (int): Kernel size for downsampling convolutions\n","            in layer2, layer3 and layer4.\n","            - For SENet154: 3\n","            - For SE-ResNet models: 1\n","            - For SE-ResNeXt models: 1\n","        downsample_padding (int): Padding for downsampling convolutions in\n","            layer2, layer3 and layer4.\n","            - For SENet154: 1\n","            - For SE-ResNet models: 0\n","            - For SE-ResNeXt models: 0\n","        num_classes (int): Number of outputs in `last_linear` layer.\n","            - For all models: 1000\n","        \"\"\"\n","        super(SENet, self).__init__()\n","        self.inplanes = inplanes\n","        if input_3x3:\n","            layer0_modules = [\n","                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n","                                    bias=False)),\n","                ('bn1', nn.BatchNorm2d(64)),\n","                ('relu1', nn.ReLU(inplace=True)),\n","                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n","                                    bias=False)),\n","                ('bn2', nn.BatchNorm2d(64)),\n","                ('relu2', nn.ReLU(inplace=True)),\n","                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n","                                    bias=False)),\n","                ('bn3', nn.BatchNorm2d(inplanes)),\n","                ('relu3', nn.ReLU(inplace=True)),\n","            ]\n","        else:\n","            layer0_modules = [\n","                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n","                                    padding=3, bias=False)),\n","                ('bn1', nn.BatchNorm2d(inplanes)),\n","                ('relu1', nn.ReLU(inplace=True)),\n","            ]\n","        # To preserve compatibility with Caffe weights `ceil_mode=True`\n","        # is used instead of `padding=1`.\n","        self.pool = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n","        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n","        self.layer1 = self._make_layer(\n","            block,\n","            planes=64,\n","            blocks=layers[0],\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=1,\n","            downsample_padding=0\n","        )\n","        self.layer2 = self._make_layer(\n","            block,\n","            planes=128,\n","            blocks=layers[1],\n","            stride=2,\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=downsample_kernel_size,\n","            downsample_padding=downsample_padding\n","        )\n","        self.layer3 = self._make_layer(\n","            block,\n","            planes=256,\n","            blocks=layers[2],\n","            stride=2,\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=downsample_kernel_size,\n","            downsample_padding=downsample_padding\n","        )\n","        self.layer4 = self._make_layer(\n","            block,\n","            planes=512,\n","            blocks=layers[3],\n","            stride=2,\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=downsample_kernel_size,\n","            downsample_padding=downsample_padding\n","        )\n","        self.avg_pool = nn.AvgPool2d(7, stride=1)\n","        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n","        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n","        self._initialize_weights()\n","\n","    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n","                    downsample_kernel_size=1, downsample_padding=0):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=downsample_kernel_size, stride=stride,\n","                          padding=downsample_padding, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n","                            downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, groups, reduction))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","                m.weight.data = nn.init.kaiming_normal_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def features(self, x):\n","        x = self.layer0(x)\n","        x = self.pool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        return x\n","\n","    def logits(self, x):\n","        x = self.avg_pool(x)\n","        if self.dropout is not None:\n","            x = self.dropout(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.last_linear(x)\n","        return x\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.logits(x)\n","        return x\n","\n","\n","def initialize_pretrained_model(model, num_classes, settings):\n","    assert num_classes == settings['num_classes'], \\\n","        'num_classes should be {}, but is {}'.format(\n","            settings['num_classes'], num_classes)\n","    model.load_state_dict(model_zoo.load_url(settings['url']), strict=False)\n","    model.input_space = settings['input_space']\n","    model.input_size = settings['input_size']\n","    model.input_range = settings['input_range']\n","    model.mean = settings['mean']\n","    model.std = settings['std']\n","\n","\n","def senet154(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n","                  dropout_p=0.2, num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['senet154'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","def scsenet154(num_classes=1000, pretrained='imagenet'):\n","    print(\"scsenet154\")\n","    model = SENet(SCSEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n","                  dropout_p=0.2, num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['senet154'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnet50(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnet50'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnet101(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnet101'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnet152(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnet152'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def scse_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SCSEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1706720060627,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"YEmiREpPoCWI"},"outputs":[],"source":["class SeResNext50_Unet_2Ssum(nn.Module):\n","    def __init__(self, pretrained='imagenet', **kwargs):\n","        super(SeResNext50_Unet_2Ssum, self).__init__()\n","\n","        encoder_filters = [32, 256, 512, 1024, 2048]\n","        decoder_filters = np.asarray([32, 96, 128, 256, 512]) // 2\n","\n","        self.conv6 = ConvRelu(encoder_filters[-1], decoder_filters[-1])\n","        self.conv6_2 = ConvRelu(decoder_filters[-1] + encoder_filters[-2], decoder_filters[-1])\n","        self.conv7 = ConvRelu(decoder_filters[-1], decoder_filters[-2])\n","        self.conv7_2 = ConvRelu(decoder_filters[-2] + encoder_filters[-3] , decoder_filters[-2] )\n","        self.conv8 = ConvRelu(decoder_filters[-2], decoder_filters[-3])\n","        self.conv8_2 = ConvRelu(decoder_filters[-3] + encoder_filters[-4] , decoder_filters[-3])\n","        self.conv9 = ConvRelu(decoder_filters[-3], decoder_filters[-4])\n","        self.conv9_2 = ConvRelu(decoder_filters[-4] + encoder_filters[-5], decoder_filters[-4])\n","\n","        # self.convx9_3 = ConvRelu(encoder_filters[-4], encoder_filters[-4])\n","\n","\n","        self.conv10 = ConvRelu(decoder_filters[-4], decoder_filters[-5])\n","        self.conv10_s = nn.Sequential(ConvRelu(decoder_filters[-5], decoder_filters[-5]),\n","                                      nn.Conv2d(decoder_filters[-5] , 1, 1, stride=1, padding=0),\n","                                      nn.Sigmoid())\n","        # self.convxx = nn.Sequential(ConvRelu(decoder_filters[-5]*2, decoder_filters[-5]*2),\n","        #                             nn.Conv2d(decoder_filters[-5] * 2, decoder_filters[-5], 1, stride=1, padding=0))\n","\n","        self.res = nn.Conv2d(decoder_filters[-5] * 2, 5, 1, stride=1, padding=0)\n","\n","        self._initialize_weights()\n","\n","        encoder = se_resnext50_32x4d(pretrained=pretrained)\n","        #encoder = torchvision.models.resnet50(pretrained=pretrained)\n","        self.conv1 = nn.Sequential(encoder.layer0.conv1, encoder.layer0.bn1, encoder.layer0.relu1) #encoder.layer0.conv1\n","        self.conv2 = nn.Sequential(encoder.pool, encoder.layer1)\n","        self.conv3 = encoder.layer2\n","        self.conv4 = encoder.layer3\n","        self.conv5 = encoder.layer4\n","\n","\n","    def forward1(self, x):\n","        batch_size, C, H, W = x.shape\n","        xx = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n","\n","        enc1 = self.conv1(x)\n","        enc2 = self.conv2(enc1)\n","        enc3 = self.conv3(enc2)\n","        enc4 = self.conv4(enc3)\n","        enc5 = self.conv5(enc4)\n","\n","        encx1 = self.conv1(xx)   # 64 128 128\n","        encx2 = self.conv2(encx1) # 64\n","        encx3 = self.conv3(encx2) # 32\n","        encx4 = self.conv4(encx3) # 16\n","        encx5 = self.conv5(encx4) # 8\n","\n","        dec6 = self.conv6(F.interpolate(enc5, scale_factor=2))\n","        dec6 = self.conv6_2(torch.cat([dec6, enc4 ], 1))\n","        dec7 = self.conv7(F.interpolate(dec6, scale_factor=2))\n","        dec7 = self.conv7_2(torch.cat([dec7, enc3], 1))\n","        dec8 = self.conv8(F.interpolate(dec7, scale_factor=2))\n","        dec8 = self.conv8_2(torch.cat([dec8, enc2], 1))\n","        dec9 = self.conv9(F.interpolate(dec8, scale_factor=2))\n","        dec9 = self.conv9_2(torch.cat([dec9,  enc1], 1))   #256\n","\n","        decx6 = self.conv6(F.interpolate(encx5, scale_factor=2))\n","        decx6 = self.conv6_2(torch.cat([decx6, encx4 ], 1))\n","        decx7 = self.conv7(F.interpolate(decx6, scale_factor=2))\n","        decx7 = self.conv7_2(torch.cat([decx7, encx3], 1))\n","        decx8 = self.conv8(F.interpolate(decx7, scale_factor=2))\n","        decx8 = self.conv8_2(torch.cat([decx8, encx2], 1))\n","        decx9 = self.conv9(F.interpolate(decx8, scale_factor=2))\n","        decx9 = self.conv9_2(torch.cat([decx9,  encx1], 1))   #128\n","        #decx9 = self.convx9_3(F.interpolate(decx9, scale_factor=4))\n","\n","\n","        dec10 = self.conv10(F.interpolate(dec9, scale_factor=2))\n","        alpha = self.conv10_s(dec10)\n","        decx10 = self.conv10(F.interpolate(decx9, scale_factor=4))\n","\n","        dec = alpha * dec10 + (1-alpha)*decx10\n","\n","        return dec\n","\n","    def forward(self, x):\n","        dec10_0 = self.forward1(x[:, :3, :, :])\n","        dec10_1 = self.forward1(x[:, 3:, :, :])\n","\n","        dec10 = torch.cat([dec10_0, dec10_1], 1)\n","\n","        return self.res(dec10)\n","\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n","                m.weight.data = nn.init.kaiming_normal_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1706720060627,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"Wmr-AZJCuHAl"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MorphologyOptimized(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=15, type=None):\n","        super(MorphologyOptimized, self).__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.soft_max = soft_max\n","        self.beta = beta\n","        self.type = type\n","\n","        # Mudança aqui: Cada canal de saída tem um conjunto de pesos por canal de entrada\n","        self.weight = nn.Parameter(torch.zeros(out_channels, in_channels, kernel_size, kernel_size), requires_grad=True)\n","\n","    def forward(self, x):\n","        pad = (self.kernel_size - 1) // 2\n","        x = F.pad(x, (pad, pad, pad, pad), mode='constant', value=float('inf') if self.type == 'erosion2d' else 0)\n","\n","        output = torch.zeros_like(x) if self.type == 'dilation2d' else torch.full_like(x, float('inf'))\n","\n","        for i in range(self.kernel_size):\n","            for j in range(self.kernel_size):\n","                x_temp = x[:, :, i:i+x.size(2)-self.kernel_size+1, j:j+x.size(3)-self.kernel_size+1]\n","                weight_temp = self.weight[:, :, i, j].unsqueeze(-1).unsqueeze(-1)\n","\n","                if self.type == 'erosion2d':\n","                    if i == 0 and j == 0:\n","                        print(weight_temp.shape)\n","                        print(x_temp.shape)\n","                        output = weight_temp - x_temp\n","                    else:\n","                        output = torch.min(output, weight_temp - x_temp)\n","                elif self.type == 'dilation2d':\n","                    if i == 0 and j == 0:\n","                        output = weight_temp + x_temp\n","                    else:\n","                        output = torch.max(output, weight_temp + x_temp)\n","\n","        # Reduzindo a saída para o número desejado de canais de saída\n","        if self.type == 'erosion2d':\n","            output = torch.min(output, dim=1, keepdim=True)[0]\n","        elif self.type == 'dilation2d':\n","            output = torch.max(output, dim=1, keepdim=True)[0]\n","\n","        # Aplicar soft max se necessário\n","        if self.soft_max:\n","            output = torch.logsumexp(output * self.beta, dim=1, keepdim=True) / self.beta\n","\n","        return output\n","\n","class Dilation2d(MorphologyOptimized):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=20):\n","        super(Dilation2d, self).__init__(in_channels, out_channels, kernel_size, soft_max, beta, 'dilation2d')\n","\n","class Erosion2d(MorphologyOptimized):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=20):\n","        super(Erosion2d, self).__init__(in_channels, out_channels, kernel_size, soft_max, beta, 'erosion2d')"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1706720060627,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"QcG3LjB8vskU"},"outputs":[],"source":["import math\n","import pdb\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Morphology(nn.Module):\n","    '''\n","    Base class for morpholigical operators\n","    For now, only supports stride=1, dilation=1, kernel_size H==W, and padding='same'.\n","    '''\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=15, type=None):\n","        '''\n","        in_channels: scalar\n","        out_channels: scalar, the number of the morphological neure.\n","        kernel_size: scalar, the spatial size of the morphological neure.\n","        soft_max: bool, using the soft max rather the torch.max(), ref: Dense Morphological Networks: An Universal Function Approximator (Mondal et al. (2019)).\n","        beta: scalar, used by soft_max.\n","        type: str, dilation2d or erosion2d.\n","        '''\n","        super(Morphology, self).__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.soft_max = soft_max\n","        self.beta = beta\n","        self.type = type\n","\n","        self.weight = nn.Parameter(torch.zeros(out_channels, in_channels, kernel_size, kernel_size), requires_grad=True)\n","        self.unfold = nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)\n","\n","    def forward(self, x):\n","        '''\n","        x: tensor of shape (B,C,H,W)\n","        '''\n","        # padding\n","        x = fixed_padding(x, self.kernel_size, dilation=1)\n","\n","        # unfold\n","        x = self.unfold(x)  # (B, Cin*kH*kW, L), where L is the numbers of patches\n","        x = x.unsqueeze(1)  # (B, 1, Cin*kH*kW, L)\n","        L = x.size(-1)\n","        L_sqrt = int(math.sqrt(L))\n","\n","        # erosion\n","        weight = self.weight.view(self.out_channels, -1) # (Cout, Cin*kH*kW)\n","        weight = weight.unsqueeze(0).unsqueeze(-1)  # (1, Cout, Cin*kH*kW, 1)\n","\n","        if self.type == 'erosion2d':\n","            x = weight - x # (B, Cout, Cin*kH*kW, L)\n","        elif self.type == 'dilation2d':\n","            x = weight + x # (B, Cout, Cin*kH*kW, L)\n","        else:\n","            raise ValueError\n","\n","        if not self.soft_max:\n","            x, _ = torch.max(x, dim=2, keepdim=False) # (B, Cout, L)\n","        else:\n","            x = torch.logsumexp(x*self.beta, dim=2, keepdim=False) / self.beta # (B, Cout, L)\n","\n","        if self.type == 'erosion2d':\n","            x = -1 * x\n","\n","        # instead of fold, we use view to avoid copy\n","        x = x.view(-1, self.out_channels, L_sqrt, L_sqrt)  # (B, Cout, L/2, L/2)\n","\n","        return x\n","\n","class Dilation2d(Morphology):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=20):\n","        super(Dilation2d, self).__init__(in_channels, out_channels, kernel_size, soft_max, beta, 'dilation2d')\n","\n","class Erosion2d(Morphology):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=20):\n","        super(Erosion2d, self).__init__(in_channels, out_channels, kernel_size, soft_max, beta, 'erosion2d')\n","\n","\n","\n","def fixed_padding(inputs, kernel_size, dilation):\n","    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n","    pad_total = kernel_size_effective - 1\n","    pad_beg = pad_total // 2\n","    pad_end = pad_total - pad_beg\n","    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n","    return padded_inputs"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1706720060627,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"czeuj9b2nkCI"},"outputs":[],"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","def fixed_padding(inputs, kernel_size, dilation):\n","    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n","    pad_total = kernel_size_effective - 1\n","    pad_beg = pad_total // 2\n","    pad_end = pad_total - pad_beg\n","    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n","    return padded_inputs\n","\n","class Morphology(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=15, type=None):\n","        super(Morphology, self).__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.soft_max = soft_max\n","        self.beta = beta\n","        self.type = type\n","\n","        self.weight = nn.Parameter(torch.zeros(out_channels, in_channels, kernel_size, kernel_size), requires_grad=True)\n","        self.unfold = nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)\n","\n","        # Inicialização dos pesos\n","        if self.type == 'erosion2d':\n","            nn.init.constant_(self.weight, 0.0001)  # Peso alto para erosão\n","        elif self.type == 'dilation2d':\n","            nn.init.constant_(self.weight, -0.0001)  # Peso baixo para dilatação\n","\n","    def forward(self, x):\n","        x = fixed_padding(x, self.kernel_size, dilation=1)\n","        x = self.unfold(x)\n","        x = x.unsqueeze(1)\n","        L = x.size(-1)\n","        L_sqrt = int(math.sqrt(L))\n","\n","        weight = self.weight.view(self.out_channels, -1)\n","        weight = weight.unsqueeze(0).unsqueeze(-1)\n","\n","        if self.type == 'erosion2d':\n","            x = weight - x\n","        elif self.type == 'dilation2d':\n","            x = weight + x\n","        else:\n","            raise ValueError\n","\n","        if not self.soft_max:\n","            x, _ = torch.max(x, dim=2, keepdim=False)\n","        else:\n","            x = torch.logsumexp(x*self.beta, dim=2, keepdim=False) / self.beta\n","\n","        if self.type == 'erosion2d':\n","            x = -1 * x\n","\n","        x = x.view(-1, self.out_channels, L_sqrt, L_sqrt)\n","        return x\n","\n","class Erosion2d(Morphology):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=20):\n","        super(Erosion2d, self).__init__(in_channels, out_channels, kernel_size, soft_max, beta, 'erosion2d')\n","\n","class Dilation2d(Morphology):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=20):\n","        super(Dilation2d, self).__init__(in_channels, out_channels, kernel_size, soft_max, beta, 'dilation2d')"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1706720060627,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"yWPYulTYg9zd","outputId":"2f2feb2e-c1cd-477e-cc8a-f732a070f251"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Erosion2d(\n","  (unfold): Unfold(kernel_size=5, dilation=1, padding=0, stride=1)\n",")"]},"metadata":{},"execution_count":17}],"source":["Erosion2d(in_channels=10,out_channels=10)"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Morphology(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, operation):\n","        super(Morphology, self).__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.operation = operation\n","        self.padding = kernel_size // 2\n","\n","        self.kernel = torch.ones((1, 1, kernel_size, kernel_size))\n","\n","    def erosion(self, x):\n","        print(x.shape)\n","        x_unfold = F.unfold(x, kernel_size=self.kernel_size, padding=self.padding)\n","        print(x_unfold.shape)\n","        x_min = x_unfold.min(dim=1, keepdim=True)[0]\n","        return x_min.view_as(x)\n","\n","    def dilation(self, x):\n","        print(x.shape)\n","        x_unfold = F.unfold(x, kernel_size=self.kernel_size, padding=self.padding)\n","        print(x_unfold.shape)\n","        x_max = x_unfold.max(dim=1, keepdim=True)[0]\n","        return x_max.view_as(x)\n","\n","    def opening(self, x):\n","        return self.dilation(self.erosion(x))\n","\n","    def closing(self, x):\n","        return self.erosion(self.dilation(x))\n","\n","    def forward(self, x, morph_op='erosion'):\n","        if morph_op == 'erosion':\n","            return self.erosion(x)\n","        elif morph_op == 'dilation':\n","            return self.dilation(x)\n","        elif morph_op == 'opening':\n","            return self.opening(x)\n","        elif morph_op == 'closing':\n","            return self.closing(x)\n","        else:\n","            raise ValueError(\"Invalid morphological operation\")\n","\n","class Erosion2d(Morphology):\n","    def __init__(self, in_channels, out_channels, kernel_size=5):\n","        super(Erosion2d, self).__init__(in_channels, out_channels, kernel_size, 'erosion')\n","\n","class Dilation2d(Morphology):\n","    def __init__(self, in_channels, out_channels, kernel_size=5):\n","        super(Dilation2d, self).__init__(in_channels, out_channels, kernel_size, 'dilation')\n","\n","class Opening2d(Morphology):\n","    def __init__(self, in_channels, out_channels, kernel_size=5):\n","        super(Opening2d, self).__init__(in_channels, out_channels, kernel_size, 'opening')\n","\n","class Closing2d(Morphology):\n","    def __init__(self, in_channels, out_channels, kernel_size=5):\n","        super(Closing2d, self).__init__(in_channels, out_channels, kernel_size, 'closing')"],"metadata":{"id":"OrKcnW-1QEQ6","executionInfo":{"status":"ok","timestamp":1706720060628,"user_tz":180,"elapsed":6,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","def fixed_padding(inputs, kernel_size, dilation):\n","    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n","    pad_total = kernel_size_effective - 1\n","    pad_beg = pad_total // 2\n","    pad_end = pad_total - pad_beg\n","    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n","    return padded_inputs\n","\n","class Morphology(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=15, type=None):\n","        super(Morphology, self).__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.soft_max = soft_max\n","        self.beta = beta\n","        self.type = type\n","\n","        self.weight = nn.Parameter(torch.zeros(out_channels, in_channels, kernel_size, kernel_size), requires_grad=True)\n","        self.unfold = nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)\n","\n","    def forward(self, x):\n","        x = fixed_padding(x, self.kernel_size, dilation=1)\n","        x = self.unfold(x)\n","        x = x.unsqueeze(1)\n","        L = x.size(-1)\n","        L_sqrt = int(math.sqrt(L))\n","\n","        if self.type == 'erosion2d':\n","            x, _ = torch.min(x, dim=2, keepdim=False)\n","        elif self.type == 'dilation2d':\n","            x, _ = torch.max(x, dim=2, keepdim=False)\n","        else:\n","            raise ValueError\n","\n","        print(x.shape)\n","        x = x.view(-1, self.out_channels, L_sqrt, L_sqrt)\n","        return x\n","\n","class Erosion2d(Morphology):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=20):\n","        super(Erosion2d, self).__init__(in_channels, out_channels, kernel_size, soft_max, beta, 'erosion2d')\n","\n","class Dilation2d(Morphology):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=20):\n","        super(Dilation2d, self).__init__(in_channels, out_channels, kernel_size, soft_max, beta, 'dilation2d')"],"metadata":{"id":"BadplIgIcCd5","executionInfo":{"status":"ok","timestamp":1706720060628,"user_tz":180,"elapsed":6,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Morphology(nn.Module):\n","    '''\n","    Base class for morphological operators\n","    For now, only supports stride=1, dilation=1, kernel_size H==W, and padding='same'.\n","    '''\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=15, type=None):\n","        '''\n","        in_channels: scalar\n","        out_channels: scalar, the number of the morphological neuron.\n","        kernel_size: scalar, the spatial size of the morphological neuron.\n","        soft_max: bool, using the soft max rather than torch.max(), ref: Dense Morphological Networks: An Universal Function Approximator (Mondal et al. (2019)).\n","        beta: scalar, used by soft_max.\n","        type: str, dilation2d or erosion2d.\n","        '''\n","        super(Morphology, self).__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.soft_max = soft_max\n","        self.beta = beta\n","        self.type = type\n","\n","        self.unfold = nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)\n","\n","    def forward(self, x):\n","        '''\n","        x: tensor of shape (B,C,H,W)\n","        '''\n","        # padding\n","        x = fixed_padding(x, self.kernel_size, dilation=1)\n","\n","        # unfold\n","        x = self.unfold(x)  # (B, Cin*kH*kW, L), where L is the number of patches\n","        x = x.unsqueeze(1)  # (B, 1, Cin*kH*kW, L)\n","        L = x.size(-1)\n","        L_sqrt = int(L**0.5)\n","\n","        # Determine operation (erosion or dilation)\n","        if self.type == 'erosion2d':\n","            operation = torch.min\n","        elif self.type == 'dilation2d':\n","            operation = torch.max\n","        else:\n","            raise ValueError\n","\n","        x = x.view(x.size(0), self.in_channels, self.kernel_size, self.kernel_size, L_sqrt, L_sqrt)\n","        # print(x.shape)\n","        x = x.permute(0, 1, 4, 5, 2, 3).contiguous().view(x.size(0), self.in_channels, L_sqrt, L_sqrt, -1)\n","        # print(x.shape)\n","        # Apply the operation\n","        x = operation(x, dim=4).values\n","        # print(x.shape)\n","        x = x.squeeze()\n","        # print(x.shape)\n","\n","        # if not self.soft_max:\n","        #     x, _ = torch.max(x, dim=2, keepdim=False)  # (B, Cout, L)\n","        # else:\n","        #     x = torch.logsumexp(x * self.beta, dim=2, keepdim=False) / self.beta  # (B, Cout, L)\n","\n","        if self.type == 'erosion2d':\n","            x = -1 * x\n","        # print(\"tensor_morf\",x.shape)\n","        return x\n","\n","class Dilation2d(Morphology):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=20):\n","        super(Dilation2d, self).__init__(in_channels, out_channels, kernel_size, soft_max, beta, 'dilation2d')\n","\n","class Erosion2d(Morphology):\n","    def __init__(self, in_channels, out_channels, kernel_size=5, soft_max=True, beta=20):\n","        super(Erosion2d, self).__init__(in_channels, out_channels, kernel_size, soft_max, beta, 'erosion2d')\n","\n","def fixed_padding(inputs, kernel_size, dilation):\n","    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n","    pad_total = kernel_size_effective - 1\n","    pad_beg = pad_total // 2\n","    pad_end = pad_total - pad_beg\n","    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n","    return padded_inputs"],"metadata":{"id":"87j_rr53iNot","executionInfo":{"status":"ok","timestamp":1706720060628,"user_tz":180,"elapsed":6,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["Dilation2d(in_channels=10,out_channels=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-WdmP5OQL58","executionInfo":{"status":"ok","timestamp":1706720060628,"user_tz":180,"elapsed":5,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"bc76f50f-2889-406a-c0ca-52ed3dd14344"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dilation2d(\n","  (unfold): Unfold(kernel_size=5, dilation=1, padding=0, stride=1)\n",")"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt"],"metadata":{"id":"3QzLjJT4qL-l","executionInfo":{"status":"ok","timestamp":1706720060628,"user_tz":180,"elapsed":5,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706720060628,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"anOG7SLeltag"},"outputs":[],"source":["class SeResNext50_Unet_Double(nn.Module):\n","    def __init__(self, pretrained='imagenet', **kwargs):\n","        super(SeResNext50_Unet_Double, self).__init__()\n","\n","        encoder_filters = [64, 256, 512, 1024, 2048]\n","        decoder_filters = np.asarray([64, 96, 128, 256, 512]) // 2\n","\n","        self.conv6 = ConvRelu(encoder_filters[-1], decoder_filters[-1])\n","        self.conv6_2 = ConvRelu(decoder_filters[-1] + encoder_filters[-2], decoder_filters[-1])\n","        self.conv7 = ConvRelu(decoder_filters[-1], decoder_filters[-2])\n","        self.conv7_2 = ConvRelu(decoder_filters[-2] + encoder_filters[-3], decoder_filters[-2])\n","        self.conv8 = ConvRelu(decoder_filters[-2], decoder_filters[-3])\n","        self.conv8_2 = ConvRelu(decoder_filters[-3] + encoder_filters[-4], decoder_filters[-3])\n","        self.conv9 = ConvRelu(decoder_filters[-3], decoder_filters[-4])\n","        self.conv9_2 = ConvRelu(decoder_filters[-4] + encoder_filters[-5], decoder_filters[-4])\n","        self.conv10 = ConvRelu(decoder_filters[-4], decoder_filters[-5])\n","\n","\n","        self.res = nn.Conv2d(decoder_filters[-5] * 2, 5, 1, stride=1, padding=0)\n","        self.closing_dilation = Dilation2d(in_channels=5, out_channels=5, kernel_size=3)\n","\n","        self._initialize_weights()\n","\n","        encoder = se_resnext50_32x4d(pretrained=pretrained)\n","        self.conv1 = nn.Sequential(encoder.layer0.conv1, encoder.layer0.bn1, encoder.layer0.relu1) #encoder.layer0.conv1\n","        self.conv2 = nn.Sequential(encoder.pool, encoder.layer1)\n","        self.conv3 = encoder.layer2\n","        self.conv4 = encoder.layer3\n","        self.conv5 = encoder.layer4\n","\n","\n","    def forward1(self, x):\n","        batch_size, C, H, W = x.shape\n","\n","        enc1 = self.conv1(x)\n","        enc2 = self.conv2(enc1)\n","        enc3 = self.conv3(enc2)\n","        enc4 = self.conv4(enc3)\n","        enc5 = self.conv5(enc4)\n","\n","        dec6 = self.conv6(F.interpolate(enc5, scale_factor=2))\n","        dec6 = self.conv6_2(torch.cat([dec6, enc4 ], 1))\n","\n","        dec7 = self.conv7(F.interpolate(dec6, scale_factor=2))\n","        dec7 = self.conv7_2(torch.cat([dec7, enc3 ], 1))\n","\n","        dec8 = self.conv8(F.interpolate(dec7, scale_factor=2))\n","        dec8 = self.conv8_2(torch.cat([dec8, enc2 ], 1))\n","\n","        dec9 = self.conv9(F.interpolate(dec8, scale_factor=2))\n","        dec9 = self.conv9_2(torch.cat([dec9,  enc1  ], 1))\n","\n","        dec10 = self.conv10(F.interpolate(dec9, scale_factor=2))\n","\n","        return dec10\n","\n","\n","    def forward(self, x):\n","        dec10_0 = self.forward1(x[:, :3, :, :])\n","        dec10_1 = self.forward1(x[:, 3:, :, :])\n","\n","        dec10 = torch.cat([dec10_0, dec10_1], 1)\n","\n","        #print(\"Finish\")\n","        res = self.res(dec10)\n","        # print(res.shape)\n","        # print(res)\n","\n","        # imagem = res[0, 0, :, :].cpu().detach().numpy()\n","        # print(imagem.shape)\n","\n","        # plt.imshow(imagem, cmap='gray')  # Use 'cmap' para escolher o mapa de cores, 'gray' é para imagens em escala de cinza\n","        # plt.title('Antes da dilatação')  # Adicione um título se desejar\n","        # plt.axis('off')  # Para remover os eixos x e y\n","        # plt.show()\n","\n","        #print(res.shape)\n","        res = self.closing_dilation(res)\n","        #res = self.closing_erosion(res)\n","        # print(res.shape)\n","        # print(res)\n","\n","        # imagem = res[0, 0, :, :].cpu().detach().numpy()\n","\n","        # plt.imshow(imagem, cmap='gray')  # Use 'cmap' para escolher o mapa de cores, 'gray' é para imagens em escala de cinza\n","        # plt.title('Depois da dilatação')  # Adicione um título se desejar\n","        # plt.axis('off')  # Para remover os eixos x e y\n","        # plt.show()\n","\n","        return res\n","\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n","                m.weight.data = nn.init.kaiming_normal_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":312656,"status":"ok","timestamp":1706720373280,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"FPD0sjUZKzeA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d4233ca-3b1f-483c-ed4f-421f15e379bb"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\" to /root/.cache/torch/hub/checkpoints/se_resnext50_32x4d-a260b3a4.pth\n","100%|██████████| 105M/105M [05:10<00:00, 356kB/s]\n"]}],"source":["import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","model = SeResNext50_Unet_Double()"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1706720373280,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"Gb4iblW8gp65","outputId":"8608b0c3-054a-440d-f827-076ca774ed8d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["SeResNext50_Unet_Double(\n","  (conv6): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv6_2): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(1280, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv7): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv7_2): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(640, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv8): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv8_2): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv9): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(64, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv9_2): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(112, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv10): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(48, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (res): Conv2d(64, 5, kernel_size=(1, 1), stride=(1, 1))\n","  (closing_dilation): Dilation2d(\n","    (unfold): Unfold(kernel_size=3, dilation=1, padding=0, stride=1)\n","  )\n","  (conv1): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (conv2): Sequential(\n","    (0): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n","    (1): Sequential(\n","      (0): SEResNeXtBottleneck(\n","        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (se_module): SEModule(\n","          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (sigmoid): Sigmoid()\n","        )\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): SEResNeXtBottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (se_module): SEModule(\n","          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (sigmoid): Sigmoid()\n","        )\n","      )\n","      (2): SEResNeXtBottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (se_module): SEModule(\n","          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (sigmoid): Sigmoid()\n","        )\n","      )\n","    )\n","  )\n","  (conv3): Sequential(\n","    (0): SEResNeXtBottleneck(\n","      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): SEResNeXtBottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (2): SEResNeXtBottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (3): SEResNeXtBottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","  )\n","  (conv4): Sequential(\n","    (0): SEResNeXtBottleneck(\n","      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): SEResNeXtBottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (2): SEResNeXtBottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (3): SEResNeXtBottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (4): SEResNeXtBottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (5): SEResNeXtBottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","  )\n","  (conv5): Sequential(\n","    (0): SEResNeXtBottleneck(\n","      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): SEResNeXtBottleneck(\n","      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (2): SEResNeXtBottleneck(\n","      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":25}],"source":["model"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1706720373280,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"U82xenQ4OpxB","outputId":"f9c914e8-64a4-4d15-c998-216427778d96"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[-1.8885e-01]],\n","\n","         [[-6.3893e-02]],\n","\n","         [[ 1.6567e-01]],\n","\n","         [[ 1.2261e-01]],\n","\n","         [[-6.2493e-02]],\n","\n","         [[ 1.3239e-01]],\n","\n","         [[ 1.0396e-01]],\n","\n","         [[ 3.8246e-02]],\n","\n","         [[ 1.3766e-01]],\n","\n","         [[-8.9844e-02]],\n","\n","         [[ 1.7480e-01]],\n","\n","         [[-1.0344e-01]],\n","\n","         [[-1.0325e-01]],\n","\n","         [[ 1.2384e-01]],\n","\n","         [[ 2.0107e-01]],\n","\n","         [[-3.6932e-01]],\n","\n","         [[ 6.0066e-02]],\n","\n","         [[-4.9116e-02]],\n","\n","         [[-1.4875e-02]],\n","\n","         [[-1.8719e-01]],\n","\n","         [[ 2.5731e-01]],\n","\n","         [[ 1.0678e-02]],\n","\n","         [[-9.3630e-02]],\n","\n","         [[-1.0012e-02]],\n","\n","         [[-1.1587e-01]],\n","\n","         [[-1.2012e-01]],\n","\n","         [[ 2.6815e-01]],\n","\n","         [[ 8.6037e-02]],\n","\n","         [[ 1.9505e-01]],\n","\n","         [[ 7.1160e-03]],\n","\n","         [[ 3.3359e-02]],\n","\n","         [[ 3.1630e-01]],\n","\n","         [[ 4.3848e-02]],\n","\n","         [[ 9.0458e-02]],\n","\n","         [[ 9.5191e-02]],\n","\n","         [[ 1.9698e-01]],\n","\n","         [[-1.6624e-01]],\n","\n","         [[-1.4250e-01]],\n","\n","         [[-1.6359e-01]],\n","\n","         [[ 1.0601e-01]],\n","\n","         [[-1.4827e-01]],\n","\n","         [[-9.8440e-02]],\n","\n","         [[-2.6907e-03]],\n","\n","         [[-1.1041e-01]],\n","\n","         [[ 2.3397e-01]],\n","\n","         [[-1.5042e-01]],\n","\n","         [[-6.8599e-02]],\n","\n","         [[ 8.0228e-02]],\n","\n","         [[-1.7488e-01]],\n","\n","         [[ 1.9770e-01]],\n","\n","         [[ 8.4291e-02]],\n","\n","         [[-5.9977e-02]],\n","\n","         [[ 1.5326e-01]],\n","\n","         [[-4.7887e-02]],\n","\n","         [[ 2.8293e-01]],\n","\n","         [[ 2.1015e-01]],\n","\n","         [[ 1.1610e-01]],\n","\n","         [[ 3.3116e-01]],\n","\n","         [[-9.9673e-02]],\n","\n","         [[ 1.5487e-01]],\n","\n","         [[-1.7413e-01]],\n","\n","         [[ 1.0629e-01]],\n","\n","         [[ 1.8400e-01]],\n","\n","         [[-2.4361e-01]]],\n","\n","\n","        [[[-1.8801e-01]],\n","\n","         [[-3.0925e-03]],\n","\n","         [[ 3.3714e-02]],\n","\n","         [[-8.0658e-02]],\n","\n","         [[ 1.3991e-02]],\n","\n","         [[ 1.0405e-01]],\n","\n","         [[ 1.1745e-01]],\n","\n","         [[ 3.1359e-02]],\n","\n","         [[-3.2658e-01]],\n","\n","         [[-5.6747e-02]],\n","\n","         [[-1.1949e-02]],\n","\n","         [[ 2.0285e-02]],\n","\n","         [[ 1.0335e-01]],\n","\n","         [[ 2.2726e-01]],\n","\n","         [[-1.2393e-01]],\n","\n","         [[ 5.4377e-02]],\n","\n","         [[-2.1949e-02]],\n","\n","         [[-5.3177e-02]],\n","\n","         [[ 2.7204e-01]],\n","\n","         [[ 1.2573e-01]],\n","\n","         [[-1.8698e-01]],\n","\n","         [[ 1.0655e-01]],\n","\n","         [[-1.3912e-01]],\n","\n","         [[ 1.1102e-01]],\n","\n","         [[-2.0319e-01]],\n","\n","         [[ 4.4647e-02]],\n","\n","         [[-2.3991e-02]],\n","\n","         [[ 1.9973e-01]],\n","\n","         [[-5.0772e-01]],\n","\n","         [[ 2.7203e-01]],\n","\n","         [[ 4.6053e-02]],\n","\n","         [[ 4.0959e-02]],\n","\n","         [[-1.0114e-01]],\n","\n","         [[-5.4045e-02]],\n","\n","         [[-2.8435e-01]],\n","\n","         [[ 5.3233e-02]],\n","\n","         [[ 1.7952e-01]],\n","\n","         [[-8.2942e-02]],\n","\n","         [[ 2.1723e-01]],\n","\n","         [[ 9.1485e-02]],\n","\n","         [[ 8.0893e-02]],\n","\n","         [[ 9.3411e-02]],\n","\n","         [[ 1.7888e-04]],\n","\n","         [[-1.2094e-01]],\n","\n","         [[-1.3753e-01]],\n","\n","         [[ 1.7324e-01]],\n","\n","         [[ 1.9209e-01]],\n","\n","         [[-3.3490e-01]],\n","\n","         [[ 6.6980e-02]],\n","\n","         [[ 2.3379e-01]],\n","\n","         [[ 3.2179e-01]],\n","\n","         [[-1.7812e-02]],\n","\n","         [[-1.1589e-01]],\n","\n","         [[-4.9730e-02]],\n","\n","         [[ 6.1109e-02]],\n","\n","         [[-1.9890e-01]],\n","\n","         [[ 2.4589e-02]],\n","\n","         [[ 2.4390e-01]],\n","\n","         [[-1.3436e-01]],\n","\n","         [[ 3.8924e-01]],\n","\n","         [[ 5.3193e-01]],\n","\n","         [[-2.3423e-01]],\n","\n","         [[ 8.5526e-02]],\n","\n","         [[-4.8425e-02]]],\n","\n","\n","        [[[-9.9627e-02]],\n","\n","         [[ 1.4835e-01]],\n","\n","         [[ 5.1779e-02]],\n","\n","         [[ 2.2356e-02]],\n","\n","         [[ 2.3217e-01]],\n","\n","         [[ 2.1185e-01]],\n","\n","         [[-2.1464e-01]],\n","\n","         [[-3.1451e-01]],\n","\n","         [[-2.2008e-01]],\n","\n","         [[ 1.2769e-01]],\n","\n","         [[ 1.7447e-01]],\n","\n","         [[-2.5446e-02]],\n","\n","         [[-2.6533e-02]],\n","\n","         [[ 2.7311e-02]],\n","\n","         [[ 1.9833e-01]],\n","\n","         [[-7.9191e-03]],\n","\n","         [[ 5.9947e-02]],\n","\n","         [[-1.0996e-01]],\n","\n","         [[ 1.0966e-01]],\n","\n","         [[-1.5399e-02]],\n","\n","         [[-1.5764e-01]],\n","\n","         [[ 1.1210e-01]],\n","\n","         [[-2.6519e-01]],\n","\n","         [[ 1.7885e-01]],\n","\n","         [[-1.3751e-02]],\n","\n","         [[-1.7147e-01]],\n","\n","         [[-1.3720e-01]],\n","\n","         [[-3.5960e-02]],\n","\n","         [[-4.9258e-02]],\n","\n","         [[-6.4615e-02]],\n","\n","         [[ 1.3339e-02]],\n","\n","         [[-9.9154e-02]],\n","\n","         [[ 2.3616e-02]],\n","\n","         [[ 1.3439e-01]],\n","\n","         [[-9.1214e-02]],\n","\n","         [[-1.1425e-01]],\n","\n","         [[-1.0881e-01]],\n","\n","         [[ 3.8645e-02]],\n","\n","         [[-1.3188e-01]],\n","\n","         [[-9.6189e-02]],\n","\n","         [[ 4.8086e-02]],\n","\n","         [[-6.2774e-02]],\n","\n","         [[ 8.2796e-02]],\n","\n","         [[-1.1562e-01]],\n","\n","         [[-1.8030e-02]],\n","\n","         [[ 4.5661e-02]],\n","\n","         [[ 1.2424e-03]],\n","\n","         [[-3.0530e-02]],\n","\n","         [[-4.9806e-02]],\n","\n","         [[ 1.8889e-02]],\n","\n","         [[-1.4806e-01]],\n","\n","         [[-2.2912e-01]],\n","\n","         [[ 1.8087e-01]],\n","\n","         [[-1.8233e-01]],\n","\n","         [[ 2.7355e-02]],\n","\n","         [[-2.1971e-01]],\n","\n","         [[ 1.3783e-01]],\n","\n","         [[ 1.8030e-01]],\n","\n","         [[ 3.2839e-01]],\n","\n","         [[ 2.6341e-01]],\n","\n","         [[ 2.5693e-01]],\n","\n","         [[ 1.1799e-01]],\n","\n","         [[-1.9336e-01]],\n","\n","         [[-8.7920e-02]]],\n","\n","\n","        [[[-2.3534e-01]],\n","\n","         [[-1.5003e-01]],\n","\n","         [[-3.0140e-01]],\n","\n","         [[-9.5910e-02]],\n","\n","         [[-4.2193e-02]],\n","\n","         [[-2.4573e-01]],\n","\n","         [[ 7.8915e-02]],\n","\n","         [[-1.2328e-01]],\n","\n","         [[ 4.2609e-02]],\n","\n","         [[-4.1973e-01]],\n","\n","         [[-1.3046e-01]],\n","\n","         [[-1.7322e-01]],\n","\n","         [[-2.7695e-01]],\n","\n","         [[ 8.0325e-02]],\n","\n","         [[ 3.8012e-02]],\n","\n","         [[-2.1020e-01]],\n","\n","         [[-5.3312e-02]],\n","\n","         [[ 6.8187e-02]],\n","\n","         [[-1.7428e-03]],\n","\n","         [[-2.7303e-01]],\n","\n","         [[ 2.4670e-01]],\n","\n","         [[ 2.0546e-01]],\n","\n","         [[ 6.8730e-02]],\n","\n","         [[-1.4521e-02]],\n","\n","         [[-1.5847e-01]],\n","\n","         [[ 1.5693e-01]],\n","\n","         [[ 4.7742e-02]],\n","\n","         [[ 1.4390e-02]],\n","\n","         [[-3.8968e-03]],\n","\n","         [[-2.5234e-02]],\n","\n","         [[-2.1362e-02]],\n","\n","         [[-4.3336e-01]],\n","\n","         [[ 2.9421e-02]],\n","\n","         [[-2.4015e-02]],\n","\n","         [[-8.3714e-02]],\n","\n","         [[-3.0776e-01]],\n","\n","         [[-9.8220e-02]],\n","\n","         [[-3.1322e-01]],\n","\n","         [[ 1.5636e-01]],\n","\n","         [[-9.7639e-02]],\n","\n","         [[ 3.9391e-03]],\n","\n","         [[-5.1125e-02]],\n","\n","         [[ 4.1110e-01]],\n","\n","         [[ 4.4826e-02]],\n","\n","         [[ 5.4968e-02]],\n","\n","         [[ 2.3589e-02]],\n","\n","         [[ 3.9877e-01]],\n","\n","         [[ 3.3478e-01]],\n","\n","         [[ 3.1760e-02]],\n","\n","         [[ 1.1633e-02]],\n","\n","         [[ 3.1126e-01]],\n","\n","         [[-1.6483e-02]],\n","\n","         [[-5.9891e-02]],\n","\n","         [[-2.9537e-02]],\n","\n","         [[ 1.0161e-01]],\n","\n","         [[ 2.2975e-01]],\n","\n","         [[-3.2903e-02]],\n","\n","         [[ 1.7945e-01]],\n","\n","         [[-2.2460e-01]],\n","\n","         [[ 1.9334e-02]],\n","\n","         [[ 3.2692e-01]],\n","\n","         [[ 2.0463e-01]],\n","\n","         [[ 2.1347e-01]],\n","\n","         [[ 2.7246e-01]]],\n","\n","\n","        [[[ 6.2045e-02]],\n","\n","         [[-1.9324e-01]],\n","\n","         [[ 4.0082e-02]],\n","\n","         [[-1.4239e-01]],\n","\n","         [[ 3.4110e-01]],\n","\n","         [[ 5.6949e-02]],\n","\n","         [[-2.3309e-02]],\n","\n","         [[ 9.0308e-02]],\n","\n","         [[ 8.3139e-02]],\n","\n","         [[-3.1779e-01]],\n","\n","         [[-2.0372e-01]],\n","\n","         [[ 3.1372e-01]],\n","\n","         [[-1.0201e-01]],\n","\n","         [[ 4.7387e-02]],\n","\n","         [[-9.5782e-02]],\n","\n","         [[-6.5062e-02]],\n","\n","         [[-1.8472e-02]],\n","\n","         [[ 1.8357e-01]],\n","\n","         [[ 4.6757e-02]],\n","\n","         [[-3.2489e-01]],\n","\n","         [[-2.2100e-02]],\n","\n","         [[-5.9705e-02]],\n","\n","         [[-1.9778e-01]],\n","\n","         [[-2.5800e-01]],\n","\n","         [[-2.6099e-01]],\n","\n","         [[ 2.1291e-01]],\n","\n","         [[-1.1714e-01]],\n","\n","         [[ 6.9630e-02]],\n","\n","         [[-2.1249e-01]],\n","\n","         [[ 1.1416e-01]],\n","\n","         [[ 1.8049e-01]],\n","\n","         [[-2.3266e-01]],\n","\n","         [[-1.7522e-01]],\n","\n","         [[-1.1779e-01]],\n","\n","         [[-3.7744e-01]],\n","\n","         [[-4.3090e-02]],\n","\n","         [[-2.0325e-02]],\n","\n","         [[ 2.8399e-01]],\n","\n","         [[ 6.1537e-02]],\n","\n","         [[-1.4342e-02]],\n","\n","         [[-1.8268e-01]],\n","\n","         [[-2.6351e-02]],\n","\n","         [[-3.8110e-01]],\n","\n","         [[ 2.5367e-01]],\n","\n","         [[ 2.2661e-01]],\n","\n","         [[ 2.7907e-01]],\n","\n","         [[ 5.4716e-02]],\n","\n","         [[ 1.0936e-01]],\n","\n","         [[-5.1050e-02]],\n","\n","         [[-2.7352e-02]],\n","\n","         [[-1.4935e-01]],\n","\n","         [[ 5.7706e-02]],\n","\n","         [[ 5.2385e-02]],\n","\n","         [[-3.4821e-02]],\n","\n","         [[ 2.0189e-02]],\n","\n","         [[ 4.5870e-02]],\n","\n","         [[ 1.1419e-01]],\n","\n","         [[ 6.5434e-02]],\n","\n","         [[-3.3120e-02]],\n","\n","         [[-1.1367e-01]],\n","\n","         [[-1.6037e-01]],\n","\n","         [[ 5.7446e-02]],\n","\n","         [[ 1.5364e-01]],\n","\n","         [[ 2.0997e-01]]]])"]},"metadata":{},"execution_count":26}],"source":["model.state_dict()['res.weight']"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1706720373280,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"IlbQLZxLnUwW","outputId":"ce9de4eb-9f29-4cf0-eab8-a429605884b6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0., 0., 0., 0., 0.])"]},"metadata":{},"execution_count":27}],"source":["model.state_dict()['res.bias']"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1706720373280,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"RgNLMD_pnw_P"},"outputs":[],"source":["# model.state_dict()['closing_dilation.weight']"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":9646,"status":"ok","timestamp":1706720382915,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"icSPduIiOOS_"},"outputs":[],"source":["weights_path = '/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/download_weights/weights/res50_cls_cce_0_tuned_best'\n","\n","# Carrega os pesos\n","weights = torch.load(weights_path, map_location='cuda')\n","\n","# Remove as chaves das novas camadas\n","keys_to_remove = ['closing_dilation.weight', 'closing_erosion.weight']\n","for key in keys_to_remove:\n","    if key in weights['state_dict']:\n","        del weights['state_dict'][key]\n","\n","# Atualiza o estado do modelo\n","new_state_dict = {key[7:]: value for key, value in weights['state_dict'].items()}\n","model.load_state_dict(new_state_dict, strict=False)\n","\n","model = model.cuda()"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1706720382915,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"QtVC9BZOOQqG","outputId":"bf255a0b-9fa9-4e55-a1a8-c276f09aaf75"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['conv6.layer.0.weight', 'conv6.layer.0.bias', 'conv6_2.layer.0.weight', 'conv6_2.layer.0.bias', 'conv7.layer.0.weight', 'conv7.layer.0.bias', 'conv7_2.layer.0.weight', 'conv7_2.layer.0.bias', 'conv8.layer.0.weight', 'conv8.layer.0.bias', 'conv8_2.layer.0.weight', 'conv8_2.layer.0.bias', 'conv9.layer.0.weight', 'conv9.layer.0.bias', 'conv9_2.layer.0.weight', 'conv9_2.layer.0.bias', 'conv10.layer.0.weight', 'conv10.layer.0.bias', 'res.weight', 'res.bias', 'conv1.0.weight', 'conv1.1.weight', 'conv1.1.bias', 'conv1.1.running_mean', 'conv1.1.running_var', 'conv1.1.num_batches_tracked', 'conv2.1.0.conv1.weight', 'conv2.1.0.bn1.weight', 'conv2.1.0.bn1.bias', 'conv2.1.0.bn1.running_mean', 'conv2.1.0.bn1.running_var', 'conv2.1.0.bn1.num_batches_tracked', 'conv2.1.0.conv2.weight', 'conv2.1.0.bn2.weight', 'conv2.1.0.bn2.bias', 'conv2.1.0.bn2.running_mean', 'conv2.1.0.bn2.running_var', 'conv2.1.0.bn2.num_batches_tracked', 'conv2.1.0.conv3.weight', 'conv2.1.0.bn3.weight', 'conv2.1.0.bn3.bias', 'conv2.1.0.bn3.running_mean', 'conv2.1.0.bn3.running_var', 'conv2.1.0.bn3.num_batches_tracked', 'conv2.1.0.se_module.fc1.weight', 'conv2.1.0.se_module.fc1.bias', 'conv2.1.0.se_module.fc2.weight', 'conv2.1.0.se_module.fc2.bias', 'conv2.1.0.downsample.0.weight', 'conv2.1.0.downsample.1.weight', 'conv2.1.0.downsample.1.bias', 'conv2.1.0.downsample.1.running_mean', 'conv2.1.0.downsample.1.running_var', 'conv2.1.0.downsample.1.num_batches_tracked', 'conv2.1.1.conv1.weight', 'conv2.1.1.bn1.weight', 'conv2.1.1.bn1.bias', 'conv2.1.1.bn1.running_mean', 'conv2.1.1.bn1.running_var', 'conv2.1.1.bn1.num_batches_tracked', 'conv2.1.1.conv2.weight', 'conv2.1.1.bn2.weight', 'conv2.1.1.bn2.bias', 'conv2.1.1.bn2.running_mean', 'conv2.1.1.bn2.running_var', 'conv2.1.1.bn2.num_batches_tracked', 'conv2.1.1.conv3.weight', 'conv2.1.1.bn3.weight', 'conv2.1.1.bn3.bias', 'conv2.1.1.bn3.running_mean', 'conv2.1.1.bn3.running_var', 'conv2.1.1.bn3.num_batches_tracked', 'conv2.1.1.se_module.fc1.weight', 'conv2.1.1.se_module.fc1.bias', 'conv2.1.1.se_module.fc2.weight', 'conv2.1.1.se_module.fc2.bias', 'conv2.1.2.conv1.weight', 'conv2.1.2.bn1.weight', 'conv2.1.2.bn1.bias', 'conv2.1.2.bn1.running_mean', 'conv2.1.2.bn1.running_var', 'conv2.1.2.bn1.num_batches_tracked', 'conv2.1.2.conv2.weight', 'conv2.1.2.bn2.weight', 'conv2.1.2.bn2.bias', 'conv2.1.2.bn2.running_mean', 'conv2.1.2.bn2.running_var', 'conv2.1.2.bn2.num_batches_tracked', 'conv2.1.2.conv3.weight', 'conv2.1.2.bn3.weight', 'conv2.1.2.bn3.bias', 'conv2.1.2.bn3.running_mean', 'conv2.1.2.bn3.running_var', 'conv2.1.2.bn3.num_batches_tracked', 'conv2.1.2.se_module.fc1.weight', 'conv2.1.2.se_module.fc1.bias', 'conv2.1.2.se_module.fc2.weight', 'conv2.1.2.se_module.fc2.bias', 'conv3.0.conv1.weight', 'conv3.0.bn1.weight', 'conv3.0.bn1.bias', 'conv3.0.bn1.running_mean', 'conv3.0.bn1.running_var', 'conv3.0.bn1.num_batches_tracked', 'conv3.0.conv2.weight', 'conv3.0.bn2.weight', 'conv3.0.bn2.bias', 'conv3.0.bn2.running_mean', 'conv3.0.bn2.running_var', 'conv3.0.bn2.num_batches_tracked', 'conv3.0.conv3.weight', 'conv3.0.bn3.weight', 'conv3.0.bn3.bias', 'conv3.0.bn3.running_mean', 'conv3.0.bn3.running_var', 'conv3.0.bn3.num_batches_tracked', 'conv3.0.se_module.fc1.weight', 'conv3.0.se_module.fc1.bias', 'conv3.0.se_module.fc2.weight', 'conv3.0.se_module.fc2.bias', 'conv3.0.downsample.0.weight', 'conv3.0.downsample.1.weight', 'conv3.0.downsample.1.bias', 'conv3.0.downsample.1.running_mean', 'conv3.0.downsample.1.running_var', 'conv3.0.downsample.1.num_batches_tracked', 'conv3.1.conv1.weight', 'conv3.1.bn1.weight', 'conv3.1.bn1.bias', 'conv3.1.bn1.running_mean', 'conv3.1.bn1.running_var', 'conv3.1.bn1.num_batches_tracked', 'conv3.1.conv2.weight', 'conv3.1.bn2.weight', 'conv3.1.bn2.bias', 'conv3.1.bn2.running_mean', 'conv3.1.bn2.running_var', 'conv3.1.bn2.num_batches_tracked', 'conv3.1.conv3.weight', 'conv3.1.bn3.weight', 'conv3.1.bn3.bias', 'conv3.1.bn3.running_mean', 'conv3.1.bn3.running_var', 'conv3.1.bn3.num_batches_tracked', 'conv3.1.se_module.fc1.weight', 'conv3.1.se_module.fc1.bias', 'conv3.1.se_module.fc2.weight', 'conv3.1.se_module.fc2.bias', 'conv3.2.conv1.weight', 'conv3.2.bn1.weight', 'conv3.2.bn1.bias', 'conv3.2.bn1.running_mean', 'conv3.2.bn1.running_var', 'conv3.2.bn1.num_batches_tracked', 'conv3.2.conv2.weight', 'conv3.2.bn2.weight', 'conv3.2.bn2.bias', 'conv3.2.bn2.running_mean', 'conv3.2.bn2.running_var', 'conv3.2.bn2.num_batches_tracked', 'conv3.2.conv3.weight', 'conv3.2.bn3.weight', 'conv3.2.bn3.bias', 'conv3.2.bn3.running_mean', 'conv3.2.bn3.running_var', 'conv3.2.bn3.num_batches_tracked', 'conv3.2.se_module.fc1.weight', 'conv3.2.se_module.fc1.bias', 'conv3.2.se_module.fc2.weight', 'conv3.2.se_module.fc2.bias', 'conv3.3.conv1.weight', 'conv3.3.bn1.weight', 'conv3.3.bn1.bias', 'conv3.3.bn1.running_mean', 'conv3.3.bn1.running_var', 'conv3.3.bn1.num_batches_tracked', 'conv3.3.conv2.weight', 'conv3.3.bn2.weight', 'conv3.3.bn2.bias', 'conv3.3.bn2.running_mean', 'conv3.3.bn2.running_var', 'conv3.3.bn2.num_batches_tracked', 'conv3.3.conv3.weight', 'conv3.3.bn3.weight', 'conv3.3.bn3.bias', 'conv3.3.bn3.running_mean', 'conv3.3.bn3.running_var', 'conv3.3.bn3.num_batches_tracked', 'conv3.3.se_module.fc1.weight', 'conv3.3.se_module.fc1.bias', 'conv3.3.se_module.fc2.weight', 'conv3.3.se_module.fc2.bias', 'conv4.0.conv1.weight', 'conv4.0.bn1.weight', 'conv4.0.bn1.bias', 'conv4.0.bn1.running_mean', 'conv4.0.bn1.running_var', 'conv4.0.bn1.num_batches_tracked', 'conv4.0.conv2.weight', 'conv4.0.bn2.weight', 'conv4.0.bn2.bias', 'conv4.0.bn2.running_mean', 'conv4.0.bn2.running_var', 'conv4.0.bn2.num_batches_tracked', 'conv4.0.conv3.weight', 'conv4.0.bn3.weight', 'conv4.0.bn3.bias', 'conv4.0.bn3.running_mean', 'conv4.0.bn3.running_var', 'conv4.0.bn3.num_batches_tracked', 'conv4.0.se_module.fc1.weight', 'conv4.0.se_module.fc1.bias', 'conv4.0.se_module.fc2.weight', 'conv4.0.se_module.fc2.bias', 'conv4.0.downsample.0.weight', 'conv4.0.downsample.1.weight', 'conv4.0.downsample.1.bias', 'conv4.0.downsample.1.running_mean', 'conv4.0.downsample.1.running_var', 'conv4.0.downsample.1.num_batches_tracked', 'conv4.1.conv1.weight', 'conv4.1.bn1.weight', 'conv4.1.bn1.bias', 'conv4.1.bn1.running_mean', 'conv4.1.bn1.running_var', 'conv4.1.bn1.num_batches_tracked', 'conv4.1.conv2.weight', 'conv4.1.bn2.weight', 'conv4.1.bn2.bias', 'conv4.1.bn2.running_mean', 'conv4.1.bn2.running_var', 'conv4.1.bn2.num_batches_tracked', 'conv4.1.conv3.weight', 'conv4.1.bn3.weight', 'conv4.1.bn3.bias', 'conv4.1.bn3.running_mean', 'conv4.1.bn3.running_var', 'conv4.1.bn3.num_batches_tracked', 'conv4.1.se_module.fc1.weight', 'conv4.1.se_module.fc1.bias', 'conv4.1.se_module.fc2.weight', 'conv4.1.se_module.fc2.bias', 'conv4.2.conv1.weight', 'conv4.2.bn1.weight', 'conv4.2.bn1.bias', 'conv4.2.bn1.running_mean', 'conv4.2.bn1.running_var', 'conv4.2.bn1.num_batches_tracked', 'conv4.2.conv2.weight', 'conv4.2.bn2.weight', 'conv4.2.bn2.bias', 'conv4.2.bn2.running_mean', 'conv4.2.bn2.running_var', 'conv4.2.bn2.num_batches_tracked', 'conv4.2.conv3.weight', 'conv4.2.bn3.weight', 'conv4.2.bn3.bias', 'conv4.2.bn3.running_mean', 'conv4.2.bn3.running_var', 'conv4.2.bn3.num_batches_tracked', 'conv4.2.se_module.fc1.weight', 'conv4.2.se_module.fc1.bias', 'conv4.2.se_module.fc2.weight', 'conv4.2.se_module.fc2.bias', 'conv4.3.conv1.weight', 'conv4.3.bn1.weight', 'conv4.3.bn1.bias', 'conv4.3.bn1.running_mean', 'conv4.3.bn1.running_var', 'conv4.3.bn1.num_batches_tracked', 'conv4.3.conv2.weight', 'conv4.3.bn2.weight', 'conv4.3.bn2.bias', 'conv4.3.bn2.running_mean', 'conv4.3.bn2.running_var', 'conv4.3.bn2.num_batches_tracked', 'conv4.3.conv3.weight', 'conv4.3.bn3.weight', 'conv4.3.bn3.bias', 'conv4.3.bn3.running_mean', 'conv4.3.bn3.running_var', 'conv4.3.bn3.num_batches_tracked', 'conv4.3.se_module.fc1.weight', 'conv4.3.se_module.fc1.bias', 'conv4.3.se_module.fc2.weight', 'conv4.3.se_module.fc2.bias', 'conv4.4.conv1.weight', 'conv4.4.bn1.weight', 'conv4.4.bn1.bias', 'conv4.4.bn1.running_mean', 'conv4.4.bn1.running_var', 'conv4.4.bn1.num_batches_tracked', 'conv4.4.conv2.weight', 'conv4.4.bn2.weight', 'conv4.4.bn2.bias', 'conv4.4.bn2.running_mean', 'conv4.4.bn2.running_var', 'conv4.4.bn2.num_batches_tracked', 'conv4.4.conv3.weight', 'conv4.4.bn3.weight', 'conv4.4.bn3.bias', 'conv4.4.bn3.running_mean', 'conv4.4.bn3.running_var', 'conv4.4.bn3.num_batches_tracked', 'conv4.4.se_module.fc1.weight', 'conv4.4.se_module.fc1.bias', 'conv4.4.se_module.fc2.weight', 'conv4.4.se_module.fc2.bias', 'conv4.5.conv1.weight', 'conv4.5.bn1.weight', 'conv4.5.bn1.bias', 'conv4.5.bn1.running_mean', 'conv4.5.bn1.running_var', 'conv4.5.bn1.num_batches_tracked', 'conv4.5.conv2.weight', 'conv4.5.bn2.weight', 'conv4.5.bn2.bias', 'conv4.5.bn2.running_mean', 'conv4.5.bn2.running_var', 'conv4.5.bn2.num_batches_tracked', 'conv4.5.conv3.weight', 'conv4.5.bn3.weight', 'conv4.5.bn3.bias', 'conv4.5.bn3.running_mean', 'conv4.5.bn3.running_var', 'conv4.5.bn3.num_batches_tracked', 'conv4.5.se_module.fc1.weight', 'conv4.5.se_module.fc1.bias', 'conv4.5.se_module.fc2.weight', 'conv4.5.se_module.fc2.bias', 'conv5.0.conv1.weight', 'conv5.0.bn1.weight', 'conv5.0.bn1.bias', 'conv5.0.bn1.running_mean', 'conv5.0.bn1.running_var', 'conv5.0.bn1.num_batches_tracked', 'conv5.0.conv2.weight', 'conv5.0.bn2.weight', 'conv5.0.bn2.bias', 'conv5.0.bn2.running_mean', 'conv5.0.bn2.running_var', 'conv5.0.bn2.num_batches_tracked', 'conv5.0.conv3.weight', 'conv5.0.bn3.weight', 'conv5.0.bn3.bias', 'conv5.0.bn3.running_mean', 'conv5.0.bn3.running_var', 'conv5.0.bn3.num_batches_tracked', 'conv5.0.se_module.fc1.weight', 'conv5.0.se_module.fc1.bias', 'conv5.0.se_module.fc2.weight', 'conv5.0.se_module.fc2.bias', 'conv5.0.downsample.0.weight', 'conv5.0.downsample.1.weight', 'conv5.0.downsample.1.bias', 'conv5.0.downsample.1.running_mean', 'conv5.0.downsample.1.running_var', 'conv5.0.downsample.1.num_batches_tracked', 'conv5.1.conv1.weight', 'conv5.1.bn1.weight', 'conv5.1.bn1.bias', 'conv5.1.bn1.running_mean', 'conv5.1.bn1.running_var', 'conv5.1.bn1.num_batches_tracked', 'conv5.1.conv2.weight', 'conv5.1.bn2.weight', 'conv5.1.bn2.bias', 'conv5.1.bn2.running_mean', 'conv5.1.bn2.running_var', 'conv5.1.bn2.num_batches_tracked', 'conv5.1.conv3.weight', 'conv5.1.bn3.weight', 'conv5.1.bn3.bias', 'conv5.1.bn3.running_mean', 'conv5.1.bn3.running_var', 'conv5.1.bn3.num_batches_tracked', 'conv5.1.se_module.fc1.weight', 'conv5.1.se_module.fc1.bias', 'conv5.1.se_module.fc2.weight', 'conv5.1.se_module.fc2.bias', 'conv5.2.conv1.weight', 'conv5.2.bn1.weight', 'conv5.2.bn1.bias', 'conv5.2.bn1.running_mean', 'conv5.2.bn1.running_var', 'conv5.2.bn1.num_batches_tracked', 'conv5.2.conv2.weight', 'conv5.2.bn2.weight', 'conv5.2.bn2.bias', 'conv5.2.bn2.running_mean', 'conv5.2.bn2.running_var', 'conv5.2.bn2.num_batches_tracked', 'conv5.2.conv3.weight', 'conv5.2.bn3.weight', 'conv5.2.bn3.bias', 'conv5.2.bn3.running_mean', 'conv5.2.bn3.running_var', 'conv5.2.bn3.num_batches_tracked', 'conv5.2.se_module.fc1.weight', 'conv5.2.se_module.fc1.bias', 'conv5.2.se_module.fc2.weight', 'conv5.2.se_module.fc2.bias'])"]},"metadata":{},"execution_count":30}],"source":["model.state_dict().keys()"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1706720382915,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"1rieJIR7OjBO","outputId":"8765d5a9-d9be-4c30-ac28-b758706eeab0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[-4.3354e-01]],\n","\n","         [[ 1.1854e-01]],\n","\n","         [[-6.2532e-02]],\n","\n","         [[-7.4680e-03]],\n","\n","         [[-3.2818e-01]],\n","\n","         [[ 2.3132e-02]],\n","\n","         [[-1.6245e-01]],\n","\n","         [[-1.2871e-01]],\n","\n","         [[ 5.2355e-01]],\n","\n","         [[ 5.5180e-01]],\n","\n","         [[-8.8305e-02]],\n","\n","         [[ 3.4802e-01]],\n","\n","         [[ 1.3210e-01]],\n","\n","         [[ 5.4608e-02]],\n","\n","         [[-6.5139e-02]],\n","\n","         [[ 7.7679e-02]],\n","\n","         [[ 1.2447e-01]],\n","\n","         [[ 1.1359e-01]],\n","\n","         [[ 1.9833e-01]],\n","\n","         [[-2.4283e-01]],\n","\n","         [[-2.1449e-02]],\n","\n","         [[-3.5256e-01]],\n","\n","         [[-5.2697e-02]],\n","\n","         [[-9.3440e-03]],\n","\n","         [[ 2.3950e-01]],\n","\n","         [[-1.8876e-01]],\n","\n","         [[-6.6172e-02]],\n","\n","         [[-2.1799e-01]],\n","\n","         [[ 5.3918e-02]],\n","\n","         [[-7.6494e-02]],\n","\n","         [[-2.1749e-01]],\n","\n","         [[-2.7243e-01]],\n","\n","         [[-5.9430e-02]],\n","\n","         [[ 8.1167e-02]],\n","\n","         [[ 1.1169e-01]],\n","\n","         [[ 2.2475e-01]],\n","\n","         [[ 1.2826e-01]],\n","\n","         [[-6.0674e-03]],\n","\n","         [[ 7.9660e-02]],\n","\n","         [[ 7.0872e-02]],\n","\n","         [[-4.0548e-01]],\n","\n","         [[-6.9598e-03]],\n","\n","         [[-8.3900e-03]],\n","\n","         [[ 6.0983e-02]],\n","\n","         [[ 9.3007e-02]],\n","\n","         [[ 2.6094e-01]],\n","\n","         [[-2.0032e-01]],\n","\n","         [[-2.4468e-01]],\n","\n","         [[-1.3662e-02]],\n","\n","         [[-3.0293e-02]],\n","\n","         [[ 3.9404e-01]],\n","\n","         [[-1.4784e-01]],\n","\n","         [[-3.9344e-01]],\n","\n","         [[ 8.1629e-02]],\n","\n","         [[-9.2382e-03]],\n","\n","         [[-4.8830e-02]],\n","\n","         [[ 4.0819e-03]],\n","\n","         [[ 1.7284e-01]],\n","\n","         [[ 2.8891e-01]],\n","\n","         [[ 6.4364e-02]],\n","\n","         [[ 9.7241e-03]],\n","\n","         [[-4.3127e-03]],\n","\n","         [[ 2.0864e-02]],\n","\n","         [[-1.4447e-01]]],\n","\n","\n","        [[[-1.5073e-02]],\n","\n","         [[ 1.7218e-01]],\n","\n","         [[ 1.4099e-01]],\n","\n","         [[-1.7079e-01]],\n","\n","         [[ 1.4604e-01]],\n","\n","         [[-8.2875e-03]],\n","\n","         [[-1.8723e-02]],\n","\n","         [[ 1.4027e-01]],\n","\n","         [[-2.3438e-02]],\n","\n","         [[ 1.8729e-01]],\n","\n","         [[-7.9539e-02]],\n","\n","         [[ 5.8697e-02]],\n","\n","         [[ 1.4579e-01]],\n","\n","         [[ 3.5112e-02]],\n","\n","         [[ 6.9031e-02]],\n","\n","         [[-4.4309e-01]],\n","\n","         [[-2.9562e-01]],\n","\n","         [[ 8.0587e-02]],\n","\n","         [[ 4.5755e-02]],\n","\n","         [[-1.3257e-01]],\n","\n","         [[ 2.7351e-01]],\n","\n","         [[-5.7528e-02]],\n","\n","         [[ 2.7899e-01]],\n","\n","         [[ 1.5494e-01]],\n","\n","         [[ 4.2190e-02]],\n","\n","         [[-4.6118e-02]],\n","\n","         [[-1.1320e-01]],\n","\n","         [[ 6.7532e-02]],\n","\n","         [[ 1.6729e-02]],\n","\n","         [[ 1.5234e-01]],\n","\n","         [[ 2.7691e-02]],\n","\n","         [[ 1.3734e-01]],\n","\n","         [[-1.6335e-01]],\n","\n","         [[-4.5403e-01]],\n","\n","         [[ 1.8639e-01]],\n","\n","         [[-1.0494e-01]],\n","\n","         [[-1.2255e-01]],\n","\n","         [[-1.5140e-02]],\n","\n","         [[-5.0884e-03]],\n","\n","         [[-6.1105e-02]],\n","\n","         [[-1.0906e-01]],\n","\n","         [[ 6.4806e-02]],\n","\n","         [[ 1.0697e-01]],\n","\n","         [[-1.6162e-01]],\n","\n","         [[ 8.4007e-02]],\n","\n","         [[ 1.1349e-01]],\n","\n","         [[ 2.5902e-02]],\n","\n","         [[ 2.9854e-02]],\n","\n","         [[-1.2332e-01]],\n","\n","         [[ 1.9306e-01]],\n","\n","         [[ 8.2273e-02]],\n","\n","         [[ 1.7707e-01]],\n","\n","         [[ 1.6874e-01]],\n","\n","         [[-2.4231e-01]],\n","\n","         [[ 4.6272e-01]],\n","\n","         [[-7.2146e-02]],\n","\n","         [[-1.9653e-02]],\n","\n","         [[-1.9879e-02]],\n","\n","         [[ 7.9753e-02]],\n","\n","         [[ 3.7800e-02]],\n","\n","         [[-1.2317e-03]],\n","\n","         [[ 1.5439e-01]],\n","\n","         [[-1.4187e-01]],\n","\n","         [[ 3.0357e-01]]],\n","\n","\n","        [[[-8.1274e-02]],\n","\n","         [[-1.2115e-01]],\n","\n","         [[ 9.0038e-02]],\n","\n","         [[-2.0705e-02]],\n","\n","         [[-8.0606e-02]],\n","\n","         [[-4.3633e-02]],\n","\n","         [[-6.5715e-04]],\n","\n","         [[ 4.0547e-02]],\n","\n","         [[ 2.5490e-01]],\n","\n","         [[-2.0587e-01]],\n","\n","         [[-2.1576e-01]],\n","\n","         [[ 3.0213e-02]],\n","\n","         [[-5.5137e-01]],\n","\n","         [[-1.4072e-01]],\n","\n","         [[ 3.5657e-01]],\n","\n","         [[-2.1189e-01]],\n","\n","         [[-2.4054e-01]],\n","\n","         [[-2.9964e-02]],\n","\n","         [[-1.6883e-01]],\n","\n","         [[-1.7307e-01]],\n","\n","         [[ 1.2388e-01]],\n","\n","         [[ 2.3392e-01]],\n","\n","         [[ 1.0846e-02]],\n","\n","         [[ 1.7110e-01]],\n","\n","         [[-9.8278e-02]],\n","\n","         [[ 2.0429e-01]],\n","\n","         [[ 1.7845e-02]],\n","\n","         [[ 7.6444e-02]],\n","\n","         [[-4.4732e-02]],\n","\n","         [[ 2.1049e-01]],\n","\n","         [[-5.7009e-02]],\n","\n","         [[-9.5260e-02]],\n","\n","         [[-5.2409e-01]],\n","\n","         [[-2.3660e-01]],\n","\n","         [[ 2.0872e-01]],\n","\n","         [[-1.0706e-01]],\n","\n","         [[-2.7017e-01]],\n","\n","         [[-7.8988e-02]],\n","\n","         [[ 4.8644e-01]],\n","\n","         [[ 3.5586e-01]],\n","\n","         [[-1.7581e-01]],\n","\n","         [[-2.0958e-01]],\n","\n","         [[-3.0995e-02]],\n","\n","         [[ 4.3338e-02]],\n","\n","         [[-1.8398e-01]],\n","\n","         [[ 5.5616e-02]],\n","\n","         [[-3.4873e-02]],\n","\n","         [[ 1.0645e-02]],\n","\n","         [[ 6.0759e-02]],\n","\n","         [[ 4.5056e-02]],\n","\n","         [[ 1.5035e-01]],\n","\n","         [[-3.1413e-02]],\n","\n","         [[-9.2127e-02]],\n","\n","         [[-1.0811e-02]],\n","\n","         [[-2.0923e-01]],\n","\n","         [[ 1.9082e-01]],\n","\n","         [[ 2.1567e-01]],\n","\n","         [[-4.2229e-01]],\n","\n","         [[-8.6973e-02]],\n","\n","         [[-2.4267e-01]],\n","\n","         [[ 2.0825e-01]],\n","\n","         [[ 5.5739e-02]],\n","\n","         [[-9.3503e-03]],\n","\n","         [[-3.4135e-01]]],\n","\n","\n","        [[[ 1.6026e-02]],\n","\n","         [[ 9.9415e-03]],\n","\n","         [[-4.4850e-04]],\n","\n","         [[-8.8971e-02]],\n","\n","         [[-2.2265e-01]],\n","\n","         [[-1.5780e-02]],\n","\n","         [[-6.2411e-02]],\n","\n","         [[ 2.5459e-01]],\n","\n","         [[ 2.2560e-02]],\n","\n","         [[ 1.5418e-01]],\n","\n","         [[-6.7886e-02]],\n","\n","         [[-3.2301e-02]],\n","\n","         [[ 3.6090e-02]],\n","\n","         [[ 3.9058e-02]],\n","\n","         [[ 6.3038e-02]],\n","\n","         [[-2.4491e-01]],\n","\n","         [[-6.4581e-02]],\n","\n","         [[ 1.8481e-01]],\n","\n","         [[-1.9181e-01]],\n","\n","         [[-1.3183e-01]],\n","\n","         [[ 1.6555e-01]],\n","\n","         [[-4.3145e-01]],\n","\n","         [[ 1.5551e-01]],\n","\n","         [[ 4.9790e-02]],\n","\n","         [[-1.9366e-02]],\n","\n","         [[ 5.4230e-02]],\n","\n","         [[ 4.4984e-02]],\n","\n","         [[ 1.1965e-01]],\n","\n","         [[ 1.8808e-02]],\n","\n","         [[-6.3928e-02]],\n","\n","         [[ 5.3593e-02]],\n","\n","         [[ 1.0410e-01]],\n","\n","         [[-2.4974e-01]],\n","\n","         [[ 3.8699e-02]],\n","\n","         [[-1.6091e-01]],\n","\n","         [[ 3.7672e-02]],\n","\n","         [[ 1.9682e-01]],\n","\n","         [[ 1.0808e-01]],\n","\n","         [[ 3.4720e-02]],\n","\n","         [[ 1.2312e-01]],\n","\n","         [[-1.4560e-01]],\n","\n","         [[ 3.5802e-02]],\n","\n","         [[-5.6741e-02]],\n","\n","         [[ 1.4783e-01]],\n","\n","         [[-1.1353e-01]],\n","\n","         [[-1.3278e-01]],\n","\n","         [[ 1.2443e-02]],\n","\n","         [[-8.0618e-02]],\n","\n","         [[ 1.9515e-01]],\n","\n","         [[ 6.4459e-02]],\n","\n","         [[-1.7473e-01]],\n","\n","         [[-2.7266e-01]],\n","\n","         [[-3.8064e-01]],\n","\n","         [[ 5.8671e-02]],\n","\n","         [[-4.1294e-02]],\n","\n","         [[-2.3254e-01]],\n","\n","         [[-1.9261e-01]],\n","\n","         [[-1.1018e-01]],\n","\n","         [[-2.9091e-01]],\n","\n","         [[ 1.6196e-01]],\n","\n","         [[ 1.7518e-01]],\n","\n","         [[ 1.5446e-02]],\n","\n","         [[-1.2041e-01]],\n","\n","         [[ 1.4992e-01]]],\n","\n","\n","        [[[ 6.4808e-02]],\n","\n","         [[-8.2181e-02]],\n","\n","         [[ 9.0925e-02]],\n","\n","         [[-2.8126e-01]],\n","\n","         [[ 1.0492e-01]],\n","\n","         [[ 5.1515e-02]],\n","\n","         [[-1.6367e-01]],\n","\n","         [[-1.1448e-01]],\n","\n","         [[-1.0020e-01]],\n","\n","         [[-1.3495e-01]],\n","\n","         [[-1.7871e-01]],\n","\n","         [[-2.2786e-02]],\n","\n","         [[ 1.6446e-01]],\n","\n","         [[-1.4844e-01]],\n","\n","         [[-1.8442e-02]],\n","\n","         [[-3.8095e-02]],\n","\n","         [[-1.3853e-01]],\n","\n","         [[-3.0516e-01]],\n","\n","         [[ 2.3483e-01]],\n","\n","         [[ 1.1817e-01]],\n","\n","         [[-5.2727e-02]],\n","\n","         [[-2.2241e-02]],\n","\n","         [[-8.1775e-02]],\n","\n","         [[-1.0958e-01]],\n","\n","         [[ 7.0224e-03]],\n","\n","         [[-2.6517e-02]],\n","\n","         [[-8.4014e-03]],\n","\n","         [[-2.2203e-02]],\n","\n","         [[ 1.6838e-01]],\n","\n","         [[ 7.5508e-02]],\n","\n","         [[ 1.6535e-01]],\n","\n","         [[ 1.6369e-01]],\n","\n","         [[ 1.0449e-01]],\n","\n","         [[-3.4381e-02]],\n","\n","         [[ 1.2948e-02]],\n","\n","         [[ 5.5593e-02]],\n","\n","         [[-5.1461e-02]],\n","\n","         [[ 1.2052e-01]],\n","\n","         [[-6.9093e-02]],\n","\n","         [[-1.3145e-02]],\n","\n","         [[-5.6251e-02]],\n","\n","         [[-2.1807e-01]],\n","\n","         [[-2.1292e-02]],\n","\n","         [[-3.4704e-02]],\n","\n","         [[-3.1734e-01]],\n","\n","         [[-2.5073e-01]],\n","\n","         [[-5.2557e-03]],\n","\n","         [[ 7.0487e-02]],\n","\n","         [[-1.7649e-01]],\n","\n","         [[-1.6199e-01]],\n","\n","         [[ 1.1288e-01]],\n","\n","         [[-3.9214e-01]],\n","\n","         [[-9.8702e-02]],\n","\n","         [[ 2.3606e-01]],\n","\n","         [[ 7.7995e-02]],\n","\n","         [[ 9.3870e-02]],\n","\n","         [[ 5.0831e-02]],\n","\n","         [[ 8.3405e-03]],\n","\n","         [[-3.6271e-01]],\n","\n","         [[-2.6111e-01]],\n","\n","         [[-1.0280e-01]],\n","\n","         [[ 9.5358e-02]],\n","\n","         [[ 2.1594e-02]],\n","\n","         [[-3.1488e-01]]]], device='cuda:0')"]},"metadata":{},"execution_count":31}],"source":["model.state_dict()['res.weight']"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1706720382915,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"TZxlK6VindUe","outputId":"44e7a719-1d05-4b6c-fa29-255f09e054fa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.0032,  0.0072,  0.0092, -0.0165, -0.0033], device='cuda:0')"]},"metadata":{},"execution_count":32}],"source":["model.state_dict()['res.bias']"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1706720382915,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"aJdKggmLGHxG"},"outputs":[],"source":["# Verificar o número de parametros da rede para ver se aumentou mais q o normal\n","# Comparar o número de parametros das redes\n","# Trocar uma camada convolucional por uma camada morfologica\n","# No pior dos casos, tentar dilatação e erosão 3x3. Se der tempo abertura e fechamento."]},{"cell_type":"markdown","metadata":{"id":"WRj40gRtIhtg"},"source":["## LoadData Part"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153803,"status":"ok","timestamp":1706717972282,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"MtDxWfXnIk_j","outputId":"c003e911-06de-443f-e3b3-883e3af8e5cf"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [02:17<00:00, 68.99s/it]\n","100%|██████████| 1/1 [00:13<00:00, 13.90s/it]\n"]}],"source":["import os\n","os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n","os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n","os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n","\n","from os import path, makedirs, listdir\n","import sys\n","import numpy as np\n","np.random.seed(1)\n","import random\n","random.seed(1)\n","\n","import torch\n","from torch import nn\n","from torch.backends import cudnn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.optim.lr_scheduler as lr_scheduler\n","\n","#from apex import amp\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import timeit\n","import cv2\n","\n","from imgaug import augmenters as iaa\n","\n","from skimage.morphology import square, dilation\n","\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import accuracy_score\n","\n","import gc\n","\n","cv2.setNumThreads(0)\n","cv2.ocl.setUseOpenCL(False)\n","\n","train_dirs = ['/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/tier3','/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/train']\n","\n","val_dirs = ['/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/test']\n","\n","models_folder = '/content/drive/MyDrive/Modeling Satelities Images Building Damaged/src/train/weights'\n","\n","loc_folder = 'pred_loc_val'\n","\n","input_shape = (512, 512)\n","\n","\n","all_files = []\n","for d in tqdm(train_dirs):\n","    for f in sorted(listdir(path.join(d, 'images'))):\n","        if '_pre_disaster.png' in f:\n","            post_disaster_file = f.replace('_pre_disaster.png', '_post_disaster.png')\n","            if path.exists(path.join(d, 'images', post_disaster_file)):\n","                all_files.append(path.join(d, 'images', f))\n","\n","all_files2 = []\n","for d in tqdm(val_dirs):\n","    for f in sorted(listdir(path.join(d, 'images'))):\n","        if '_pre_disaster.png' in f:\n","            post_disaster_file = f.replace('_pre_disaster.png', '_post_disaster.png')\n","            if path.exists(path.join(d, 'images', post_disaster_file)):\n","                all_files2.append(path.join(d, 'images', f))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLQbZktxIxLC"},"outputs":[],"source":["def rand_bbox(size, lam):\n","    W = size[0]\n","    H = size[1]\n","    cut_rat = np.sqrt(1. - lam)\n","    cut_w = int(W * cut_rat)\n","    cut_h = int(H * cut_rat)\n","    # if cut_rat > 0.9:\n","    #   cut_w = np.int(W * cut_rat * 0.9)\n","    #   cut_h = np.int(H * cut_rat * 0.9)\n","\n","    # uniform\n","    cx = np.random.randint(W)\n","    cy = np.random.randint(H)\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n","\n","    return bbx1, bby1, bbx2, bby2\n","\n","class TrainData(Dataset):\n","    def __init__(self, train_idxs, low, high):\n","        super().__init__()\n","        self.train_idxs = train_idxs\n","        self.elastic = iaa.ElasticTransformation(alpha=(0.25, 1.2), sigma=0.2)\n","        self.low =low\n","        self.high = high\n","\n","    def __len__(self):\n","        return len(self.train_idxs)\n","\n","    def __getitem__(self, idx):\n","        _idx = self.train_idxs[idx]\n","\n","        fn = all_files[_idx]\n","\n","        img = cv2.imread(fn, cv2.IMREAD_COLOR)\n","        img2 = cv2.imread(fn.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","        if img is None and img2 is None:\n","          sample = {'img': None, 'msk': None, 'lbl_msk': None, 'fn': None}\n","        elif img is None or img2 is None:\n","          sample = {'img': None, 'msk': None, 'lbl_msk': None, 'fn': None}\n","\n","        msk0 = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        lbl_msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        msk1 = np.zeros_like(lbl_msk1)\n","        msk2 = np.zeros_like(lbl_msk1)\n","        msk3 = np.zeros_like(lbl_msk1)\n","        msk4 = np.zeros_like(lbl_msk1)\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","        msk1[lbl_msk1 == 1] = 255\n","\n","        try:\n","          if random.random() > 0.87:\n","              lam = np.random.beta(2, 1.8)\n","              rand_inx = torch.randint(low=self.low,high=self.high,size=(1,))\n","              ttt = self.train_idxs[rand_inx]\n","              fn_rand = all_files[ttt]\n","              img_random = cv2.imread(fn_rand, cv2.IMREAD_COLOR)\n","              img2_random = cv2.imread(fn_rand.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","              msk0_random = cv2.imread(fn_rand.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","              lbl_msk1_random = cv2.imread(fn_rand.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","              bbx1, bby1, bbx2, bby2 = rand_bbox((1024, 1024), lam)\n","              img[bbx1:bbx2, bby1:bby2, :] = img_random[bbx1:bbx2, bby1:bby2, :]\n","              img2[bbx1:bbx2, bby1:bby2, :] = img2_random[bbx1:bbx2, bby1:bby2, :]\n","              msk0[bbx1:bbx2, bby1:bby2] = msk0_random[bbx1:bbx2, bby1:bby2]\n","              lbl_msk1[bbx1:bbx2, bby1:bby2] = lbl_msk1_random[bbx1:bbx2, bby1:bby2]\n","              # lbl_msk1[lbl_msk1==1]=70\n","              # lbl_msk1[lbl_msk1==2]=130\n","              # lbl_msk1[lbl_msk1==3]=190\n","              # lbl_msk1[lbl_msk1==4]=255\n","              # cv2.imshow('input_image', lbl_msk1)\n","              # cv2.waitKey(5000)\n","        except:\n","          None\n","\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","        msk1[lbl_msk1 == 1] = 255\n","\n","        try:\n","          if random.random() > 0.5:\n","              img = img[::-1, ...]\n","              img2 = img2[::-1, ...]\n","              msk0 = msk0[::-1, ...]\n","              msk1 = msk1[::-1, ...]\n","              msk2 = msk2[::-1, ...]\n","              msk3 = msk3[::-1, ...]\n","              msk4 = msk4[::-1, ...]\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.05:\n","              rot = random.randrange(4)\n","              if rot > 0:\n","                img = np.rot90(img, k=rot)\n","                img2 = np.rot90(img2, k=rot)\n","                msk0 = np.rot90(msk0, k=rot)\n","                msk1 = np.rot90(msk1, k=rot)\n","                msk2 = np.rot90(msk2, k=rot)\n","                msk3 = np.rot90(msk3, k=rot)\n","                msk4 = np.rot90(msk4, k=rot)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.8:\n","              shift_pnt = (random.randint(-320, 320), random.randint(-320, 320))\n","              img = shift_image(img, shift_pnt)\n","              img2 = shift_image(img2, shift_pnt)\n","              msk0 = shift_image(msk0, shift_pnt)\n","              msk1 = shift_image(msk1, shift_pnt)\n","              msk2 = shift_image(msk2, shift_pnt)\n","              msk3 = shift_image(msk3, shift_pnt)\n","              msk4 = shift_image(msk4, shift_pnt)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.2:\n","              rot_pnt =  (img.shape[0] // 2 + random.randint(-320, 320), img.shape[1] // 2 + random.randint(-320, 320))\n","              scale = 0.9 + random.random() * 0.2\n","              angle = random.randint(0, 20) - 10\n","              if (angle != 0) or (scale != 1):\n","                  img = rotate_image(img, angle, scale, rot_pnt)\n","                  img2 = rotate_image(img2, angle, scale, rot_pnt)\n","                  msk0 = rotate_image(msk0, angle, scale, rot_pnt)\n","                  msk1 = rotate_image(msk1, angle, scale, rot_pnt)\n","                  msk2 = rotate_image(msk2, angle, scale, rot_pnt)\n","                  msk3 = rotate_image(msk3, angle, scale, rot_pnt)\n","                  msk4 = rotate_image(msk4, angle, scale, rot_pnt)\n","        except:\n","          None\n","\n","        crop_size = input_shape[0]\n","\n","        try:\n","          if random.random() > 0.1:\n","              crop_size = random.randint(int(input_shape[0] / 1.15), int(input_shape[0] / 0.85))\n","        except:\n","          None\n","\n","        try:\n","          bst_x0 = random.randint(0, img.shape[1] - crop_size)\n","          bst_y0 = random.randint(0, img.shape[0] - crop_size)\n","          bst_sc = -1\n","          try_cnt = random.randint(1, 10)\n","          for i in range(try_cnt):\n","              x0 = random.randint(0, img.shape[1] - crop_size)\n","              y0 = random.randint(0, img.shape[0] - crop_size)\n","              _sc = msk2[y0:y0+crop_size, x0:x0+crop_size].sum() * 5 + msk3[y0:y0+crop_size, x0:x0+crop_size].sum() * 5 + msk4[y0:y0+crop_size, x0:x0+crop_size].sum() * 2 + msk1[y0:y0+crop_size, x0:x0+crop_size].sum()\n","              if _sc > bst_sc:\n","                  bst_sc = _sc\n","                  bst_x0 = x0\n","                  bst_y0 = y0\n","          x0 = bst_x0\n","          y0 = bst_y0\n","          img = img[y0:y0+crop_size, x0:x0+crop_size, :]\n","          img2 = img2[y0:y0+crop_size, x0:x0+crop_size, :]\n","          msk0 = msk0[y0:y0+crop_size, x0:x0+crop_size]\n","          msk1 = msk1[y0:y0+crop_size, x0:x0+crop_size]\n","          msk2 = msk2[y0:y0+crop_size, x0:x0+crop_size]\n","          msk3 = msk3[y0:y0+crop_size, x0:x0+crop_size]\n","          msk4 = msk4[y0:y0+crop_size, x0:x0+crop_size]\n","\n","          if crop_size != input_shape[0]:\n","            img = cv2.resize(img, input_shape, interpolation=cv2.INTER_LINEAR)\n","            img2 = cv2.resize(img2, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk0 = cv2.resize(msk0, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk1 = cv2.resize(msk1, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk2 = cv2.resize(msk2, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk3 = cv2.resize(msk3, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk4 = cv2.resize(msk4, input_shape, interpolation=cv2.INTER_LINEAR)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.96:\n","              img = shift_channels(img, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","          elif random.random() > 0.96:\n","              img2 = shift_channels(img2, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","\n","          if random.random() > 0.96:\n","              img = change_hsv(img, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","          elif random.random() > 0.96:\n","              img2 = change_hsv(img2, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img = clahe(img)\n","              elif random.random() > 0.96:\n","                  img = gauss_noise(img)\n","              elif random.random() > 0.96:\n","                  img = cv2.blur(img, (3, 3))\n","          elif random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img = saturation(img, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img = brightness(img, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img = contrast(img, 0.9 + random.random() * 0.2)\n","\n","          if random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img2 = clahe(img2)\n","              elif random.random() > 0.96:\n","                  img2 = gauss_noise(img2)\n","              elif random.random() > 0.96:\n","                  img2 = cv2.blur(img2, (3, 3))\n","          elif random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img2 = saturation(img2, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img2 = brightness(img2, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img2 = contrast(img2, 0.9 + random.random() * 0.2)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.96:\n","              el_det = self.elastic.to_deterministic()\n","              img = el_det.augment_image(img)\n","\n","          if random.random() > 0.96:\n","              el_det = self.elastic.to_deterministic()\n","              img2 = el_det.augment_image(img2)\n","        except:\n","          None\n","\n","        msk0 = msk0[..., np.newaxis]\n","        msk1 = msk1[..., np.newaxis]\n","        msk2 = msk2[..., np.newaxis]\n","        msk3 = msk3[..., np.newaxis]\n","        msk4 = msk4[..., np.newaxis]\n","\n","        msk = np.concatenate([msk0, msk1, msk2, msk3, msk4], axis=2)\n","        msk = (msk > 127)\n","\n","        msk[..., 0] = True\n","        msk[..., 1] = dilation(msk[..., 1], square(5))\n","        msk[..., 2] = dilation(msk[..., 2], square(5))\n","        msk[..., 3] = dilation(msk[..., 3], square(5))\n","        msk[..., 4] = dilation(msk[..., 4], square(5))\n","        msk[..., 1][msk[..., 2:].max(axis=2)] = False\n","        msk[..., 3][msk[..., 2]] = False\n","        msk[..., 4][msk[..., 2]] = False\n","        msk[..., 4][msk[..., 3]] = False\n","        msk[..., 0][msk[..., 1:].max(axis=2)] = False\n","        msk = msk * 1\n","\n","        lbl_msk = msk.argmax(axis=2)\n","\n","        img = np.concatenate([img, img2], axis=2)\n","        img = preprocess_inputs(img)\n","\n","        img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n","        msk = torch.from_numpy(msk.transpose((2, 0, 1))).long()\n","\n","        sample = {'img': img, 'msk': msk, 'lbl_msk': lbl_msk, 'fn': fn}\n","        return sample\n","\n","\n","class ValData(Dataset):\n","    def __init__(self, image_idxs):\n","        super().__init__()\n","        self.image_idxs = image_idxs\n","\n","    def __len__(self):\n","        return len(self.image_idxs)\n","\n","    def __getitem__(self, idx):\n","        _idx = self.image_idxs[idx]\n","\n","        fn = all_files2[_idx]\n","\n","        img = cv2.imread(fn, cv2.IMREAD_COLOR)\n","        img2 = cv2.imread(fn.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","        msk_loc = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)# > (0.3*255)\n","\n","        msk0 = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        lbl_msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        msk1 = np.zeros_like(lbl_msk1)\n","        msk2 = np.zeros_like(lbl_msk1)\n","        msk3 = np.zeros_like(lbl_msk1)\n","        msk4 = np.zeros_like(lbl_msk1)\n","        msk1[lbl_msk1 == 1] = 255\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","\n","        msk0 = msk0[..., np.newaxis]\n","        msk1 = msk1[..., np.newaxis]\n","        msk2 = msk2[..., np.newaxis]\n","        msk3 = msk3[..., np.newaxis]\n","        msk4 = msk4[..., np.newaxis]\n","\n","        msk = np.concatenate([msk0, msk1, msk2, msk3, msk4], axis=2)\n","        msk = (msk > 127)\n","\n","        msk = msk * 1\n","\n","        lbl_msk = msk[..., 1:].argmax(axis=2)\n","\n","        img = np.concatenate([img, img2], axis=2)\n","        img = preprocess_inputs(img)\n","\n","        img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n","        msk = torch.from_numpy(msk.transpose((2, 0, 1))).long()\n","\n","        sample = {'img': img, 'msk': msk, 'lbl_msk': lbl_msk, 'fn': fn, 'msk_loc': msk_loc}\n","        return sample\n","\n","\n","def validate(net, data_loader):\n","    dices0 = []\n","\n","    tp = np.zeros((5,))\n","    fp = np.zeros((5,))\n","    fn = np.zeros((5,))\n","\n","    _thr = 0.3\n","\n","    with torch.no_grad():\n","        for i, sample in enumerate(tqdm(data_loader)):\n","            msks = sample[\"msk\"].numpy()\n","            lbl_msk = sample[\"lbl_msk\"].numpy()\n","            imgs = sample[\"img\"].cuda(non_blocking=True)\n","            msk_loc = sample[\"msk_loc\"].numpy() * 1\n","            out = model(imgs)\n","\n","            msk_pred = msk_loc\n","            msk_damage_pred = torch.softmax(out, dim=1).cpu().numpy()[:, 1:, ...]\n","\n","            for j in range(msks.shape[0]):\n","                tp[4] += np.logical_and(msks[j, 0] > 0, msk_pred[j] > 0).sum()\n","                fn[4] += np.logical_and(msks[j, 0] < 1, msk_pred[j] > 0).sum()\n","                fp[4] += np.logical_and(msks[j, 0] > 0, msk_pred[j] < 1).sum()\n","\n","\n","                targ = lbl_msk[j][msks[j, 0] > 0]\n","                pred = msk_damage_pred[j].argmax(axis=0)\n","                pred = pred * (msk_pred[j] > _thr)\n","                pred = pred[msks[j, 0] > 0]\n","                for c in range(4):\n","                    tp[c] += np.logical_and(pred == c, targ == c).sum()\n","                    fn[c] += np.logical_and(pred != c, targ == c).sum()\n","                    fp[c] += np.logical_and(pred == c, targ != c).sum()\n","\n","    d0 = 2 * tp[4] / (2 * tp[4] + fp[4] + fn[4])\n","\n","    f1_sc = np.zeros((4,))\n","    for c in range(4):\n","        f1_sc[c] = 2 * tp[c] / (2 * tp[c] + fp[c] + fn[c])\n","\n","    f1 = 4 / np.sum(1.0 / (f1_sc + 1e-6))\n","\n","    sc = 0.3 * d0 + 0.7 * f1\n","    print(\"Val Score: {}, Dice: {}, F1: {}, F1_0: {}, F1_1: {}, F1_2: {}, F1_3: {}\".format(sc, d0, f1, f1_sc[0], f1_sc[1], f1_sc[2], f1_sc[3]))\n","    return [sc, d0, f1, f1_sc[0], f1_sc[1], f1_sc[2], f1_sc[3]]\n","\n","\n","def evaluate_val(data_val, best_score, model, snapshot_name, current_epoch):\n","    model = model.eval()\n","    d = validate(model, data_loader=data_val)\n","    score = d[0]\n","\n","    if score > best_score:\n","        torch.save({\n","            'epoch': current_epoch + 1,\n","            'state_dict': model.state_dict(),\n","            'best_score': d,\n","        }, path.join(models_folder, snapshot_name + '_best'))\n","        best_score = score\n","\n","    print(\"score: {}\\tscore_best: {}\".format(d, best_score))\n","    return d[0],d\n","\n","\n","def train_epoch(current_epoch, seg_loss, ce_loss, seg_seesaw, model, optimizer, scheduler, train_data_loader):\n","    losses = AverageMeter()\n","    losses1 = AverageMeter()\n","\n","    dices = AverageMeter()\n","\n","    iterator = tqdm(train_data_loader)\n","    model.train()\n","    for i, sample in enumerate(iterator):\n","        imgs = sample[\"img\"].cuda(non_blocking=True)\n","        msks = sample[\"msk\"].cuda(non_blocking=True)\n","        lbl_msk = sample[\"lbl_msk\"].cuda(non_blocking=True)\n","\n","        out = model(imgs)\n","\n","        # loss0 = seg_loss(out[:, 0, ...], msks[:, 0, ...])\n","        # loss1 = seg_loss(out[:, 1, ...], msks[:, 1, ...])\n","        # loss2 = seg_loss(out[:, 2, ...], msks[:, 2, ...])\n","        # loss3 = seg_loss(out[:, 3, ...], msks[:, 3, ...])\n","        # loss4 = seg_loss(out[:, 4, ...], msks[:, 4, ...])\n","\n","        loss5 = ce_loss(out, lbl_msk)\n","        #loss5 = seg_seesaw(out, lbl_msk)\n","        loss = loss5\n","        #loss = 0.1 * loss0 + 0.1 * loss1 + 0.3 * loss2 + 0.3 * loss3 + 0.2 * loss4 + loss5 * 2\n","\n","        with torch.no_grad():\n","            _probs = 1 - torch.sigmoid(out[:, 0, ...])\n","            dice_sc = 1 - dice_round(_probs, 1 - msks[:, 0, ...])\n","\n","        losses.update(loss.item(), imgs.size(0))\n","        losses1.update(loss5.item(), imgs.size(0))\n","\n","        dices.update(dice_sc, imgs.size(0))\n","\n","        iterator.set_description(\n","            \"epoch: {}; lr {:.7f}; Loss {loss.val:.4f} ({loss.avg:.4f}); cce_loss {loss1.val:.4f} ({loss1.avg:.4f}); Dice {dice.val:.4f} ({dice.avg:.4f})\".format(\n","                current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","\n","        optimizer.zero_grad()\n","        # loss.backward()\n","        with amp.scale_loss(loss, optimizer) as scaled_loss:\n","            scaled_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), 0.999)\n","        optimizer.step()\n","\n","    scheduler.step()\n","\n","    print(\"epoch: {}; lr {:.7f}; Loss {loss.avg:.4f}; CCE_loss {loss1.avg:.4f}; Dice {dice.avg:.4f}\".format(\n","            current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","    return scheduler.get_lr()[-1], losses, losses1, dices\n","\n","def train_epoch(current_epoch, seg_loss, ce_loss, seg_seesaw, model, optimizer, scheduler, train_data_loader):\n","    losses = AverageMeter()\n","    losses1 = AverageMeter()\n","\n","    dices = AverageMeter()\n","\n","    iterator = tqdm(train_data_loader)\n","    model.train()\n","    for i, sample in enumerate(iterator):\n","        if sample[\"img\"] is None or sample[\"msk\"] is None or sample[\"lbl_msk\"] is None:\n","              continue\n","        imgs = sample[\"img\"].cuda(non_blocking=True)\n","        msks = sample[\"msk\"].cuda(non_blocking=True)\n","        lbl_msk = sample[\"lbl_msk\"].cuda(non_blocking=True)\n","\n","        out = model(imgs)\n","\n","        loss5 = ce_loss(out, lbl_msk)\n","        loss = loss5\n","\n","        with torch.no_grad():\n","            _probs = 1 - torch.sigmoid(out[:, 0, ...])\n","            dice_sc = 1 - dice_round(_probs, 1 - msks[:, 0, ...])\n","\n","        losses.update(loss.item(), imgs.size(0))\n","        losses1.update(loss5.item(), imgs.size(0))\n","\n","        dices.update(dice_sc, imgs.size(0))\n","\n","        iterator.set_description(\n","            \"epoch: {}; lr {:.7f}; Loss {loss.val:.4f} ({loss.avg:.4f}); cce_loss {loss1.val:.4f} ({loss1.avg:.4f}); Dice {dice.val:.4f} ({dice.avg:.4f})\".format(\n","                current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.999)\n","        optimizer.step()\n","\n","    scheduler.step()\n","\n","    print(\"epoch: {}; lr {:.7f}; Loss {loss.avg:.4f}; CCE_loss {loss1.avg:.4f}; Dice {dice.avg:.4f}\".format(\n","            current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","    return scheduler.get_lr()[-1], losses, losses1, dices"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1706717972283,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"0rHK9dViIyjQ","outputId":"bbad10dc-2c6b-472d-c971-d3cb4cdf5245"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                    images        disaster\n","0     joplin-tornado_00000000_pre_disaster  joplin-tornado\n","1     joplin-tornado_00000001_pre_disaster  joplin-tornado\n","2     joplin-tornado_00000002_pre_disaster  joplin-tornado\n","3     joplin-tornado_00000003_pre_disaster  joplin-tornado\n","4     joplin-tornado_00000004_pre_disaster  joplin-tornado\n","...                                    ...             ...\n","9157      socal-fire_00001396_pre_disaster      socal-fire\n","9158      socal-fire_00001397_pre_disaster      socal-fire\n","9159      socal-fire_00001398_pre_disaster      socal-fire\n","9160      socal-fire_00001399_pre_disaster      socal-fire\n","9161      socal-fire_00001402_pre_disaster      socal-fire\n","\n","[9162 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-e1f4f8f3-ded9-4f24-bfa4-ccc79e47cb8a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>images</th>\n","      <th>disaster</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>joplin-tornado_00000000_pre_disaster</td>\n","      <td>joplin-tornado</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>joplin-tornado_00000001_pre_disaster</td>\n","      <td>joplin-tornado</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>joplin-tornado_00000002_pre_disaster</td>\n","      <td>joplin-tornado</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>joplin-tornado_00000003_pre_disaster</td>\n","      <td>joplin-tornado</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>joplin-tornado_00000004_pre_disaster</td>\n","      <td>joplin-tornado</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9157</th>\n","      <td>socal-fire_00001396_pre_disaster</td>\n","      <td>socal-fire</td>\n","    </tr>\n","    <tr>\n","      <th>9158</th>\n","      <td>socal-fire_00001397_pre_disaster</td>\n","      <td>socal-fire</td>\n","    </tr>\n","    <tr>\n","      <th>9159</th>\n","      <td>socal-fire_00001398_pre_disaster</td>\n","      <td>socal-fire</td>\n","    </tr>\n","    <tr>\n","      <th>9160</th>\n","      <td>socal-fire_00001399_pre_disaster</td>\n","      <td>socal-fire</td>\n","    </tr>\n","    <tr>\n","      <th>9161</th>\n","      <td>socal-fire_00001402_pre_disaster</td>\n","      <td>socal-fire</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9162 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1f4f8f3-ded9-4f24-bfa4-ccc79e47cb8a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e1f4f8f3-ded9-4f24-bfa4-ccc79e47cb8a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e1f4f8f3-ded9-4f24-bfa4-ccc79e47cb8a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a3dfc976-fb1f-4a4f-9963-eaeef7bee712\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a3dfc976-fb1f-4a4f-9963-eaeef7bee712')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a3dfc976-fb1f-4a4f-9963-eaeef7bee712 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_3713e429-0fae-4b88-9bb4-ee373ba0ddea\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('list_images')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_3713e429-0fae-4b88-9bb4-ee373ba0ddea button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('list_images');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":13}],"source":["list_images = pd.DataFrame(None)\n","list_images['images'] = [n.split('/')[-1].split('.')[0] for n in all_files]\n","list_images['disaster'] = [n.split('/')[-1].split('_')[0] for n in all_files]\n","list_images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1706717972283,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"U1RaA7wYIz_o","outputId":"67fc72a0-e867-4c02-91d5-cbe52c5491e0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                       images           disaster\n","6369  guatemala-volcano_00000010_pre_disaster  guatemala-volcano\n","6366  guatemala-volcano_00000006_pre_disaster  guatemala-volcano\n","6376  guatemala-volcano_00000023_pre_disaster  guatemala-volcano\n","6365  guatemala-volcano_00000002_pre_disaster  guatemala-volcano\n","6377  guatemala-volcano_00000024_pre_disaster  guatemala-volcano\n","...                                       ...                ...\n","5552      woolsey-fire_00000061_post_disaster       woolsey-fire\n","5695      woolsey-fire_00000204_post_disaster       woolsey-fire\n","6210      woolsey-fire_00000725_post_disaster       woolsey-fire\n","5775      woolsey-fire_00000284_post_disaster       woolsey-fire\n","5491      woolsey-fire_00000000_post_disaster       woolsey-fire\n","\n","[380 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-d21ad340-3838-4942-a4b3-24c225e01634\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>images</th>\n","      <th>disaster</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6369</th>\n","      <td>guatemala-volcano_00000010_pre_disaster</td>\n","      <td>guatemala-volcano</td>\n","    </tr>\n","    <tr>\n","      <th>6366</th>\n","      <td>guatemala-volcano_00000006_pre_disaster</td>\n","      <td>guatemala-volcano</td>\n","    </tr>\n","    <tr>\n","      <th>6376</th>\n","      <td>guatemala-volcano_00000023_pre_disaster</td>\n","      <td>guatemala-volcano</td>\n","    </tr>\n","    <tr>\n","      <th>6365</th>\n","      <td>guatemala-volcano_00000002_pre_disaster</td>\n","      <td>guatemala-volcano</td>\n","    </tr>\n","    <tr>\n","      <th>6377</th>\n","      <td>guatemala-volcano_00000024_pre_disaster</td>\n","      <td>guatemala-volcano</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5552</th>\n","      <td>woolsey-fire_00000061_post_disaster</td>\n","      <td>woolsey-fire</td>\n","    </tr>\n","    <tr>\n","      <th>5695</th>\n","      <td>woolsey-fire_00000204_post_disaster</td>\n","      <td>woolsey-fire</td>\n","    </tr>\n","    <tr>\n","      <th>6210</th>\n","      <td>woolsey-fire_00000725_post_disaster</td>\n","      <td>woolsey-fire</td>\n","    </tr>\n","    <tr>\n","      <th>5775</th>\n","      <td>woolsey-fire_00000284_post_disaster</td>\n","      <td>woolsey-fire</td>\n","    </tr>\n","    <tr>\n","      <th>5491</th>\n","      <td>woolsey-fire_00000000_post_disaster</td>\n","      <td>woolsey-fire</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>380 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d21ad340-3838-4942-a4b3-24c225e01634')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d21ad340-3838-4942-a4b3-24c225e01634 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d21ad340-3838-4942-a4b3-24c225e01634');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-14ef6268-39de-41e0-8b73-6ef125adf118\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-14ef6268-39de-41e0-8b73-6ef125adf118')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-14ef6268-39de-41e0-8b73-6ef125adf118 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_8ac5b603-3eb5-4340-8550-e127c7934e6a\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('final_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_8ac5b603-3eb5-4340-8550-e127c7934e6a button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('final_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":14}],"source":["# Número de pares de imagens a serem selecionados de cada desastre\n","num_pairs_per_disaster = 10  # Isso dará 2 imagens (pré e pós) para cada desastre\n","\n","# Lista para armazenar os DataFrames filtrados\n","filtered_dfs = []\n","\n","# Agrupar por desastre e selecionar imagens\n","for name, group in list_images.groupby('disaster'):\n","    # Escolha o mínimo entre o número desejado de pares e o número disponível de pares\n","    num_pairs = min(num_pairs_per_disaster, len(group))\n","    if num_pairs > 0:\n","        selected_pairs = group.sample(n=num_pairs)\n","        # Adicionar as imagens pós-desastre correspondentes\n","        post_disaster_images = selected_pairs['images'].str.replace('pre_disaster', 'post_disaster')\n","        selected_pairs = pd.concat([selected_pairs, pd.DataFrame({'images': post_disaster_images, 'disaster': name})])\n","        filtered_dfs.append(selected_pairs)\n","\n","# Concatenar todos os DataFrames filtrados para obter o DataFrame final\n","final_df = pd.concat(filtered_dfs)#.reset_index(drop=True)\n","final_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fysHje1CI1s5"},"outputs":[],"source":["all_files = [all_files[i] for i in final_df.index]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":921971,"status":"ok","timestamp":1706718894230,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"rP7_xNHAI3JJ","outputId":"6111d409-1077-4b9f-a43f-11a5ceb61611"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 380/380 [03:09<00:00,  2.01it/s]\n","100%|██████████| 906/906 [12:11<00:00,  1.24it/s]\n","100%|██████████| 906/906 [00:00<00:00, 1918243.02it/s]\n","100%|██████████| 376/376 [00:00<00:00, 372211.07it/s]\n","100%|██████████| 376/376 [00:00<00:00, 389300.99it/s]\n"]}],"source":["# ttt = np.asarray([True, False, True, True])\n","# p1=ttt[0:].max()\n","# p2=ttt[1].max()\n","# ttt2 = np.asarray([True, True, False, False])\n","# p3=ttt2[1].max()\n","# p4=ttt2[2:3].max()\n","t0 = timeit.default_timer()\n","\n","makedirs(models_folder, exist_ok=True)\n","\n","seed = 13\n","#seed=0\n","# vis_dev = sys.argv[2]\n","\n","# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = vis_dev\n","\n","cudnn.benchmark = True\n","\n","batch_size = 8\n","val_batch_size = 8\n","\n","snapshot_name = 'res50_cls_2Ssum_{}_0'.format(seed)\n","\n","file_classes = []\n","for fn in tqdm(all_files):\n","    fl = np.zeros((4,), dtype=bool)\n","    msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","    for c in range(1, 5):\n","        fl[c-1] = c in msk1\n","    file_classes.append(fl)\n","file_classes = np.asarray(file_classes)\n","\n","file_classes2 = []\n","for fn in tqdm(all_files2):\n","    fl = np.zeros((4,), dtype=bool)\n","    msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","    for c in range(1, 5):\n","        fl[c-1] = c in msk1\n","    file_classes2.append(fl)\n","file_classes2 = np.asarray(file_classes2)\n","\n","train_idxs0, val_idxs = train_test_split(np.arange(len(all_files)), test_size=0.01, random_state=seed)\n","\n","val_idxs0 = np.arange(len(all_files2))\n","\n","val_idxs = []\n","for i in tqdm(val_idxs0):\n","    val_idxs.append(i)\n","\n","np.random.seed(seed + 1234)\n","random.seed(seed + 1234)\n","\n","train_idxs = []\n","for i in tqdm(train_idxs0):\n","    train_idxs.append(i)\n","    if file_classes[i, 1:].max():\n","        train_idxs.append(i)\n","    # if file_classes[i, 2].max():\n","    #     train_idxs.append(i)\n","low1 = len(train_idxs)\n","for i in tqdm(train_idxs0):\n","    if file_classes[i, 1:3].max():\n","        train_idxs.append(i)\n","# for i in train_idxs0:\n","#     if file_classes[i, 1].max():\n","#         train_idxs.append(i)\n","high1 = len(train_idxs)\n","\n","train_idxs = np.asarray(train_idxs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1706718894230,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"TWYouGDQI5hq","outputId":"90d6725b-08df-4a53-a705-fff984fa7ff8"},"outputs":[{"output_type":"stream","name":"stdout","text":["steps_per_epoch 86 validation_steps 113\n"]}],"source":["steps_per_epoch = int(len(train_idxs) // batch_size)\n","validation_steps = int(len(val_idxs) // val_batch_size)\n","\n","print('steps_per_epoch', steps_per_epoch, 'validation_steps', validation_steps)\n","\n","data_train = TrainData(train_idxs, low1, high1)\n","val_train = ValData(val_idxs)\n","\n","train_data_loader = DataLoader(data_train, batch_size=int(batch_size), num_workers=6, shuffle=True, pin_memory=False, drop_last=True)\n","val_data_loader = DataLoader(val_train, batch_size=int(val_batch_size), num_workers=6, shuffle=False, pin_memory=False)"]},{"cell_type":"markdown","metadata":{"id":"tHCAM2vfRK7e"},"source":["## LoadData all"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":160716,"status":"ok","timestamp":1706720543618,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"Un2B5Ov0F8f-","outputId":"39420e2f-052f-4811-ddb6-5fdbd4cb9095"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [02:25<00:00, 72.92s/it]\n","100%|██████████| 1/1 [00:13<00:00, 13.40s/it]\n"]}],"source":["import os\n","os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n","os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n","os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n","\n","from os import path, makedirs, listdir\n","import sys\n","import numpy as np\n","np.random.seed(1)\n","import random\n","random.seed(1)\n","\n","import torch\n","from torch import nn\n","from torch.backends import cudnn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.optim.lr_scheduler as lr_scheduler\n","\n","#from apex import amp\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import timeit\n","import cv2\n","\n","from imgaug import augmenters as iaa\n","\n","from skimage.morphology import square, dilation\n","\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import accuracy_score\n","\n","import gc\n","\n","cv2.setNumThreads(0)\n","cv2.ocl.setUseOpenCL(False)\n","\n","train_dirs = ['/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/tier3','/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/train']\n","\n","val_dirs = ['/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/test']\n","\n","models_folder = '/content/drive/MyDrive/Modeling Satelities Images Building Damaged/src/train/weights'\n","\n","loc_folder = 'pred_loc_val'\n","\n","input_shape = (512, 512)\n","\n","\n","all_files = []\n","for d in tqdm(train_dirs):\n","    for f in sorted(listdir(path.join(d, 'images'))):\n","        if '_pre_disaster.png' in f:\n","            post_disaster_file = f.replace('_pre_disaster.png', '_post_disaster.png')\n","            if path.exists(path.join(d, 'images', post_disaster_file)):\n","                all_files.append(path.join(d, 'images', f))\n","\n","all_files2 = []\n","for d in tqdm(val_dirs):\n","    for f in sorted(listdir(path.join(d, 'images'))):\n","        if '_pre_disaster.png' in f:\n","            post_disaster_file = f.replace('_pre_disaster.png', '_post_disaster.png')\n","            if path.exists(path.join(d, 'images', post_disaster_file)):\n","                all_files2.append(path.join(d, 'images', f))"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"3_V8N6ykQ2g0","executionInfo":{"status":"ok","timestamp":1706720543618,"user_tz":180,"elapsed":13,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["def rand_bbox(size, lam):\n","    W = size[0]\n","    H = size[1]\n","    cut_rat = np.sqrt(1. - lam)\n","    cut_w = int(W * cut_rat)\n","    cut_h = int(H * cut_rat)\n","    # if cut_rat > 0.9:\n","    #   cut_w = np.int(W * cut_rat * 0.9)\n","    #   cut_h = np.int(H * cut_rat * 0.9)\n","\n","    # uniform\n","    cx = np.random.randint(W)\n","    cy = np.random.randint(H)\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n","\n","    return bbx1, bby1, bbx2, bby2\n","\n","class TrainData(Dataset):\n","    def __init__(self, train_idxs, low, high):\n","        super().__init__()\n","        self.train_idxs = train_idxs\n","        self.elastic = iaa.ElasticTransformation(alpha=(0.25, 1.2), sigma=0.2)\n","        self.low =low\n","        self.high = high\n","\n","    def __len__(self):\n","        return len(self.train_idxs)\n","\n","    def __getitem__(self, idx):\n","        _idx = self.train_idxs[idx]\n","\n","        fn = all_files[_idx]\n","\n","        img = cv2.imread(fn, cv2.IMREAD_COLOR)\n","        img2 = cv2.imread(fn.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","        if img is None and img2 is None:\n","          sample = {'img': None, 'msk': None, 'lbl_msk': None, 'fn': None}\n","        elif img is None or img2 is None:\n","          sample = {'img': None, 'msk': None, 'lbl_msk': None, 'fn': None}\n","\n","        msk0 = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        lbl_msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        msk1 = np.zeros_like(lbl_msk1)\n","        msk2 = np.zeros_like(lbl_msk1)\n","        msk3 = np.zeros_like(lbl_msk1)\n","        msk4 = np.zeros_like(lbl_msk1)\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","        msk1[lbl_msk1 == 1] = 255\n","\n","        try:\n","          if random.random() > 0.87:\n","              lam = np.random.beta(2, 1.8)\n","              rand_inx = torch.randint(low=self.low,high=self.high,size=(1,))\n","              ttt = self.train_idxs[rand_inx]\n","              fn_rand = all_files[ttt]\n","              img_random = cv2.imread(fn_rand, cv2.IMREAD_COLOR)\n","              img2_random = cv2.imread(fn_rand.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","              msk0_random = cv2.imread(fn_rand.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","              lbl_msk1_random = cv2.imread(fn_rand.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","              bbx1, bby1, bbx2, bby2 = rand_bbox((1024, 1024), lam)\n","              img[bbx1:bbx2, bby1:bby2, :] = img_random[bbx1:bbx2, bby1:bby2, :]\n","              img2[bbx1:bbx2, bby1:bby2, :] = img2_random[bbx1:bbx2, bby1:bby2, :]\n","              msk0[bbx1:bbx2, bby1:bby2] = msk0_random[bbx1:bbx2, bby1:bby2]\n","              lbl_msk1[bbx1:bbx2, bby1:bby2] = lbl_msk1_random[bbx1:bbx2, bby1:bby2]\n","              # lbl_msk1[lbl_msk1==1]=70\n","              # lbl_msk1[lbl_msk1==2]=130\n","              # lbl_msk1[lbl_msk1==3]=190\n","              # lbl_msk1[lbl_msk1==4]=255\n","              # cv2.imshow('input_image', lbl_msk1)\n","              # cv2.waitKey(5000)\n","        except:\n","          None\n","\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","        msk1[lbl_msk1 == 1] = 255\n","\n","        try:\n","          if random.random() > 0.5:\n","              img = img[::-1, ...]\n","              img2 = img2[::-1, ...]\n","              msk0 = msk0[::-1, ...]\n","              msk1 = msk1[::-1, ...]\n","              msk2 = msk2[::-1, ...]\n","              msk3 = msk3[::-1, ...]\n","              msk4 = msk4[::-1, ...]\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.05:\n","              rot = random.randrange(4)\n","              if rot > 0:\n","                img = np.rot90(img, k=rot)\n","                img2 = np.rot90(img2, k=rot)\n","                msk0 = np.rot90(msk0, k=rot)\n","                msk1 = np.rot90(msk1, k=rot)\n","                msk2 = np.rot90(msk2, k=rot)\n","                msk3 = np.rot90(msk3, k=rot)\n","                msk4 = np.rot90(msk4, k=rot)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.8:\n","              shift_pnt = (random.randint(-320, 320), random.randint(-320, 320))\n","              img = shift_image(img, shift_pnt)\n","              img2 = shift_image(img2, shift_pnt)\n","              msk0 = shift_image(msk0, shift_pnt)\n","              msk1 = shift_image(msk1, shift_pnt)\n","              msk2 = shift_image(msk2, shift_pnt)\n","              msk3 = shift_image(msk3, shift_pnt)\n","              msk4 = shift_image(msk4, shift_pnt)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.2:\n","              rot_pnt =  (img.shape[0] // 2 + random.randint(-320, 320), img.shape[1] // 2 + random.randint(-320, 320))\n","              scale = 0.9 + random.random() * 0.2\n","              angle = random.randint(0, 20) - 10\n","              if (angle != 0) or (scale != 1):\n","                  img = rotate_image(img, angle, scale, rot_pnt)\n","                  img2 = rotate_image(img2, angle, scale, rot_pnt)\n","                  msk0 = rotate_image(msk0, angle, scale, rot_pnt)\n","                  msk1 = rotate_image(msk1, angle, scale, rot_pnt)\n","                  msk2 = rotate_image(msk2, angle, scale, rot_pnt)\n","                  msk3 = rotate_image(msk3, angle, scale, rot_pnt)\n","                  msk4 = rotate_image(msk4, angle, scale, rot_pnt)\n","        except:\n","          None\n","\n","        crop_size = input_shape[0]\n","\n","        try:\n","          if random.random() > 0.1:\n","              crop_size = random.randint(int(input_shape[0] / 1.15), int(input_shape[0] / 0.85))\n","        except:\n","          None\n","\n","        try:\n","          bst_x0 = random.randint(0, img.shape[1] - crop_size)\n","          bst_y0 = random.randint(0, img.shape[0] - crop_size)\n","          bst_sc = -1\n","          try_cnt = random.randint(1, 10)\n","          for i in range(try_cnt):\n","              x0 = random.randint(0, img.shape[1] - crop_size)\n","              y0 = random.randint(0, img.shape[0] - crop_size)\n","              _sc = msk2[y0:y0+crop_size, x0:x0+crop_size].sum() * 5 + msk3[y0:y0+crop_size, x0:x0+crop_size].sum() * 5 + msk4[y0:y0+crop_size, x0:x0+crop_size].sum() * 2 + msk1[y0:y0+crop_size, x0:x0+crop_size].sum()\n","              if _sc > bst_sc:\n","                  bst_sc = _sc\n","                  bst_x0 = x0\n","                  bst_y0 = y0\n","          x0 = bst_x0\n","          y0 = bst_y0\n","          img = img[y0:y0+crop_size, x0:x0+crop_size, :]\n","          img2 = img2[y0:y0+crop_size, x0:x0+crop_size, :]\n","          msk0 = msk0[y0:y0+crop_size, x0:x0+crop_size]\n","          msk1 = msk1[y0:y0+crop_size, x0:x0+crop_size]\n","          msk2 = msk2[y0:y0+crop_size, x0:x0+crop_size]\n","          msk3 = msk3[y0:y0+crop_size, x0:x0+crop_size]\n","          msk4 = msk4[y0:y0+crop_size, x0:x0+crop_size]\n","\n","          if crop_size != input_shape[0]:\n","            img = cv2.resize(img, input_shape, interpolation=cv2.INTER_LINEAR)\n","            img2 = cv2.resize(img2, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk0 = cv2.resize(msk0, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk1 = cv2.resize(msk1, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk2 = cv2.resize(msk2, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk3 = cv2.resize(msk3, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk4 = cv2.resize(msk4, input_shape, interpolation=cv2.INTER_LINEAR)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.96:\n","              img = shift_channels(img, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","          elif random.random() > 0.96:\n","              img2 = shift_channels(img2, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","\n","          if random.random() > 0.96:\n","              img = change_hsv(img, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","          elif random.random() > 0.96:\n","              img2 = change_hsv(img2, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img = clahe(img)\n","              elif random.random() > 0.96:\n","                  img = gauss_noise(img)\n","              elif random.random() > 0.96:\n","                  img = cv2.blur(img, (3, 3))\n","          elif random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img = saturation(img, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img = brightness(img, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img = contrast(img, 0.9 + random.random() * 0.2)\n","\n","          if random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img2 = clahe(img2)\n","              elif random.random() > 0.96:\n","                  img2 = gauss_noise(img2)\n","              elif random.random() > 0.96:\n","                  img2 = cv2.blur(img2, (3, 3))\n","          elif random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img2 = saturation(img2, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img2 = brightness(img2, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img2 = contrast(img2, 0.9 + random.random() * 0.2)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.96:\n","              el_det = self.elastic.to_deterministic()\n","              img = el_det.augment_image(img)\n","\n","          if random.random() > 0.96:\n","              el_det = self.elastic.to_deterministic()\n","              img2 = el_det.augment_image(img2)\n","        except:\n","          None\n","\n","        msk0 = msk0[..., np.newaxis]\n","        msk1 = msk1[..., np.newaxis]\n","        msk2 = msk2[..., np.newaxis]\n","        msk3 = msk3[..., np.newaxis]\n","        msk4 = msk4[..., np.newaxis]\n","\n","        msk = np.concatenate([msk0, msk1, msk2, msk3, msk4], axis=2)\n","        msk = (msk > 127)\n","\n","        msk[..., 0] = True\n","        msk[..., 1] = dilation(msk[..., 1], square(5))\n","        msk[..., 2] = dilation(msk[..., 2], square(5))\n","        msk[..., 3] = dilation(msk[..., 3], square(5))\n","        msk[..., 4] = dilation(msk[..., 4], square(5))\n","        msk[..., 1][msk[..., 2:].max(axis=2)] = False\n","        msk[..., 3][msk[..., 2]] = False\n","        msk[..., 4][msk[..., 2]] = False\n","        msk[..., 4][msk[..., 3]] = False\n","        msk[..., 0][msk[..., 1:].max(axis=2)] = False\n","        msk = msk * 1\n","\n","        lbl_msk = msk.argmax(axis=2)\n","\n","        img = np.concatenate([img, img2], axis=2)\n","        img = preprocess_inputs(img)\n","\n","        img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n","        msk = torch.from_numpy(msk.transpose((2, 0, 1))).long()\n","\n","        sample = {'img': img, 'msk': msk, 'lbl_msk': lbl_msk, 'fn': fn}\n","        return sample\n","\n","\n","class ValData(Dataset):\n","    def __init__(self, image_idxs):\n","        super().__init__()\n","        self.image_idxs = image_idxs\n","\n","    def __len__(self):\n","        return len(self.image_idxs)\n","\n","    def __getitem__(self, idx):\n","        _idx = self.image_idxs[idx]\n","\n","        fn = all_files2[_idx]\n","\n","        img = cv2.imread(fn, cv2.IMREAD_COLOR)\n","        img2 = cv2.imread(fn.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","        msk_loc = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)# > (0.3*255)\n","\n","        msk0 = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        lbl_msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        msk1 = np.zeros_like(lbl_msk1)\n","        msk2 = np.zeros_like(lbl_msk1)\n","        msk3 = np.zeros_like(lbl_msk1)\n","        msk4 = np.zeros_like(lbl_msk1)\n","        msk1[lbl_msk1 == 1] = 255\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","\n","        msk0 = msk0[..., np.newaxis]\n","        msk1 = msk1[..., np.newaxis]\n","        msk2 = msk2[..., np.newaxis]\n","        msk3 = msk3[..., np.newaxis]\n","        msk4 = msk4[..., np.newaxis]\n","\n","        msk = np.concatenate([msk0, msk1, msk2, msk3, msk4], axis=2)\n","        msk = (msk > 127)\n","\n","        msk = msk * 1\n","\n","        lbl_msk = msk[..., 1:].argmax(axis=2)\n","\n","        img = np.concatenate([img, img2], axis=2)\n","        img = preprocess_inputs(img)\n","\n","        img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n","        msk = torch.from_numpy(msk.transpose((2, 0, 1))).long()\n","\n","        sample = {'img': img, 'msk': msk, 'lbl_msk': lbl_msk, 'fn': fn, 'msk_loc': msk_loc}\n","        return sample\n","\n","\n","def validate(net, data_loader):\n","    dices0 = []\n","\n","    tp = np.zeros((5,))\n","    fp = np.zeros((5,))\n","    fn = np.zeros((5,))\n","\n","    _thr = 0.3\n","\n","    with torch.no_grad():\n","        for i, sample in enumerate(tqdm(data_loader)):\n","            msks = sample[\"msk\"].numpy()\n","            lbl_msk = sample[\"lbl_msk\"].numpy()\n","            imgs = sample[\"img\"].cuda(non_blocking=True)\n","            msk_loc = sample[\"msk_loc\"].numpy() * 1\n","            out = model(imgs)\n","\n","            msk_pred = msk_loc\n","            msk_damage_pred = torch.softmax(out, dim=1).cpu().numpy()[:, 1:, ...]\n","\n","            for j in range(msks.shape[0]):\n","                tp[4] += np.logical_and(msks[j, 0] > 0, msk_pred[j] > 0).sum()\n","                fn[4] += np.logical_and(msks[j, 0] < 1, msk_pred[j] > 0).sum()\n","                fp[4] += np.logical_and(msks[j, 0] > 0, msk_pred[j] < 1).sum()\n","\n","\n","                targ = lbl_msk[j][msks[j, 0] > 0]\n","                pred = msk_damage_pred[j].argmax(axis=0)\n","                pred = pred * (msk_pred[j] > _thr)\n","                pred = pred[msks[j, 0] > 0]\n","                for c in range(4):\n","                    tp[c] += np.logical_and(pred == c, targ == c).sum()\n","                    fn[c] += np.logical_and(pred != c, targ == c).sum()\n","                    fp[c] += np.logical_and(pred == c, targ != c).sum()\n","\n","    d0 = 2 * tp[4] / (2 * tp[4] + fp[4] + fn[4])\n","\n","    f1_sc = np.zeros((4,))\n","    for c in range(4):\n","        f1_sc[c] = 2 * tp[c] / (2 * tp[c] + fp[c] + fn[c])\n","\n","    f1 = 4 / np.sum(1.0 / (f1_sc + 1e-6))\n","\n","    sc = 0.3 * d0 + 0.7 * f1\n","    print(\"Val Score: {}, Dice: {}, F1: {}, F1_0: {}, F1_1: {}, F1_2: {}, F1_3: {}\".format(sc, d0, f1, f1_sc[0], f1_sc[1], f1_sc[2], f1_sc[3]))\n","    return [sc, d0, f1, f1_sc[0], f1_sc[1], f1_sc[2], f1_sc[3]]\n","\n","\n","def evaluate_val(data_val, best_score, model, snapshot_name, current_epoch):\n","    model = model.eval()\n","    d = validate(model, data_loader=data_val)\n","    score = d[0]\n","\n","    if score > best_score:\n","        torch.save({\n","            'epoch': current_epoch + 1,\n","            'state_dict': model.state_dict(),\n","            'best_score': d,\n","        }, path.join(models_folder, snapshot_name + '_best'))\n","        best_score = score\n","\n","    print(\"score: {}\\tscore_best: {}\".format(d, best_score))\n","    return d[0],d\n","\n","\n","def train_epoch(current_epoch, seg_loss, ce_loss, seg_seesaw, model, optimizer, scheduler, train_data_loader):\n","    losses = AverageMeter()\n","    losses1 = AverageMeter()\n","\n","    dices = AverageMeter()\n","\n","    iterator = tqdm(train_data_loader)\n","    model.train()\n","    for i, sample in enumerate(iterator):\n","        imgs = sample[\"img\"].cuda(non_blocking=True)\n","        msks = sample[\"msk\"].cuda(non_blocking=True)\n","        lbl_msk = sample[\"lbl_msk\"].cuda(non_blocking=True)\n","\n","        out = model(imgs)\n","\n","        # loss0 = seg_loss(out[:, 0, ...], msks[:, 0, ...])\n","        # loss1 = seg_loss(out[:, 1, ...], msks[:, 1, ...])\n","        # loss2 = seg_loss(out[:, 2, ...], msks[:, 2, ...])\n","        # loss3 = seg_loss(out[:, 3, ...], msks[:, 3, ...])\n","        # loss4 = seg_loss(out[:, 4, ...], msks[:, 4, ...])\n","\n","        loss5 = ce_loss(out, lbl_msk)\n","        #loss5 = seg_seesaw(out, lbl_msk)\n","        loss = loss5\n","        #loss = 0.1 * loss0 + 0.1 * loss1 + 0.3 * loss2 + 0.3 * loss3 + 0.2 * loss4 + loss5 * 2\n","\n","        with torch.no_grad():\n","            _probs = 1 - torch.sigmoid(out[:, 0, ...])\n","            dice_sc = 1 - dice_round(_probs, 1 - msks[:, 0, ...])\n","\n","        losses.update(loss.item(), imgs.size(0))\n","        losses1.update(loss5.item(), imgs.size(0))\n","\n","        dices.update(dice_sc, imgs.size(0))\n","\n","        iterator.set_description(\n","            \"epoch: {}; lr {:.7f}; Loss {loss.val:.4f} ({loss.avg:.4f}); cce_loss {loss1.val:.4f} ({loss1.avg:.4f}); Dice {dice.val:.4f} ({dice.avg:.4f})\".format(\n","                current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","\n","        optimizer.zero_grad()\n","        # loss.backward()\n","        with amp.scale_loss(loss, optimizer) as scaled_loss:\n","            scaled_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), 0.999)\n","        optimizer.step()\n","\n","    scheduler.step()\n","\n","    print(\"epoch: {}; lr {:.7f}; Loss {loss.avg:.4f}; CCE_loss {loss1.avg:.4f}; Dice {dice.avg:.4f}\".format(\n","            current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","    return scheduler.get_lr()[-1], losses, losses1, dices\n","\n","def train_epoch(current_epoch, seg_loss, ce_loss, seg_seesaw, model, optimizer, scheduler, train_data_loader):\n","    losses = AverageMeter()\n","    losses1 = AverageMeter()\n","\n","    dices = AverageMeter()\n","\n","    iterator = tqdm(train_data_loader)\n","    model.train()\n","    for i, sample in enumerate(iterator):\n","        if sample[\"img\"] is None or sample[\"msk\"] is None or sample[\"lbl_msk\"] is None:\n","              continue\n","        imgs = sample[\"img\"].cuda(non_blocking=True)\n","        msks = sample[\"msk\"].cuda(non_blocking=True)\n","        lbl_msk = sample[\"lbl_msk\"].cuda(non_blocking=True)\n","\n","        out = model(imgs)\n","\n","        loss5 = ce_loss(out, lbl_msk)\n","        loss = loss5\n","\n","        with torch.no_grad():\n","            _probs = 1 - torch.sigmoid(out[:, 0, ...])\n","            dice_sc = 1 - dice_round(_probs, 1 - msks[:, 0, ...])\n","\n","        losses.update(loss.item(), imgs.size(0))\n","        losses1.update(loss5.item(), imgs.size(0))\n","\n","        dices.update(dice_sc, imgs.size(0))\n","\n","        iterator.set_description(\n","            \"epoch: {}; lr {:.7f}; Loss {loss.val:.4f} ({loss.avg:.4f}); cce_loss {loss1.val:.4f} ({loss1.avg:.4f}); Dice {dice.val:.4f} ({dice.avg:.4f})\".format(\n","                current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.999)\n","        optimizer.step()\n","\n","    scheduler.step()\n","\n","    print(\"epoch: {}; lr {:.7f}; Loss {loss.avg:.4f}; CCE_loss {loss1.avg:.4f}; Dice {dice.avg:.4f}\".format(\n","            current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","    return scheduler.get_lr()[-1], losses, losses1, dices"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AL3hRfYVR6t7","outputId":"e0b84e71-137c-4f2a-980a-aade426f0948","executionInfo":{"status":"ok","timestamp":1706727194118,"user_tz":180,"elapsed":5155157,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9162/9162 [1:40:22<00:00,  1.52it/s]\n","100%|██████████| 906/906 [10:26<00:00,  1.45it/s]\n","100%|██████████| 906/906 [00:00<00:00, 1847369.68it/s]\n","100%|██████████| 9070/9070 [00:00<00:00, 437192.87it/s]\n","100%|██████████| 9070/9070 [00:00<00:00, 450370.40it/s]\n"]}],"source":["# ttt = np.asarray([True, False, True, True])\n","# p1=ttt[0:].max()\n","# p2=ttt[1].max()\n","# ttt2 = np.asarray([True, True, False, False])\n","# p3=ttt2[1].max()\n","# p4=ttt2[2:3].max()\n","t0 = timeit.default_timer()\n","\n","makedirs(models_folder, exist_ok=True)\n","\n","seed = 13\n","#seed=0\n","# vis_dev = sys.argv[2]\n","\n","# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = vis_dev\n","\n","cudnn.benchmark = True\n","\n","batch_size = 8\n","val_batch_size = 6\n","\n","snapshot_name = 'res50_cls_2Ssum_{}_0'.format(seed)\n","\n","file_classes = []\n","for fn in tqdm(all_files):\n","    fl = np.zeros((4,), dtype=bool)\n","    msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","    for c in range(1, 5):\n","        fl[c-1] = c in msk1\n","    file_classes.append(fl)\n","file_classes = np.asarray(file_classes)\n","\n","file_classes2 = []\n","for fn in tqdm(all_files2):\n","    fl = np.zeros((4,), dtype=bool)\n","    msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","    for c in range(1, 5):\n","        fl[c-1] = c in msk1\n","    file_classes2.append(fl)\n","file_classes2 = np.asarray(file_classes2)\n","\n","train_idxs0, val_idxs = train_test_split(np.arange(len(all_files)), test_size=0.01, random_state=seed)\n","\n","val_idxs0 = np.arange(len(all_files2))\n","\n","val_idxs = []\n","for i in tqdm(val_idxs0):\n","    val_idxs.append(i)\n","\n","np.random.seed(seed + 1234)\n","random.seed(seed + 1234)\n","\n","train_idxs = []\n","for i in tqdm(train_idxs0):\n","    train_idxs.append(i)\n","    if file_classes[i, 1:].max():\n","        train_idxs.append(i)\n","    # if file_classes[i, 2].max():\n","    #     train_idxs.append(i)\n","low1 = len(train_idxs)\n","for i in tqdm(train_idxs0):\n","    if file_classes[i, 1:3].max():\n","        train_idxs.append(i)\n","# for i in train_idxs0:\n","#     if file_classes[i, 1].max():\n","#         train_idxs.append(i)\n","high1 = len(train_idxs)\n","\n","train_idxs = np.asarray(train_idxs)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZxguzzcmqDBX","outputId":"cd6438b5-e895-4811-e798-3344ab19e405","executionInfo":{"status":"ok","timestamp":1706727194118,"user_tz":180,"elapsed":3,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["steps_per_epoch 1841 validation_steps 151\n"]}],"source":["steps_per_epoch = int(len(train_idxs) // batch_size)\n","validation_steps = int(len(val_idxs) // val_batch_size)\n","\n","print('steps_per_epoch', steps_per_epoch, 'validation_steps', validation_steps)\n","\n","data_train = TrainData(train_idxs, low1, high1)\n","val_train = ValData(val_idxs)\n","\n","train_data_loader = DataLoader(data_train, batch_size=int(batch_size), num_workers=6, shuffle=True, pin_memory=False, drop_last=True)\n","val_data_loader = DataLoader(val_train, batch_size=int(val_batch_size), num_workers=6, shuffle=False, pin_memory=False)"]},{"cell_type":"markdown","metadata":{"id":"T-rqSugmZwEp"},"source":["## Train"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"Z313YLyMn2XX","executionInfo":{"status":"ok","timestamp":1706727194119,"user_tz":180,"elapsed":2,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import pandas as pd\n","results = pd.DataFrame(None,columns=['epoch', 'lr', 'loss',\n","                                     'CCE_loss', 'dice_train',\n","                                     'layers', 'ops', 'k_sizes',\n","                                     'shape_types','val_score','dice_val',\n","                                     'f1_score','F1_0','F1_1','F1_2',\n","                                     'F1_3'])\n","#results.to_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_3_top_train4.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qFTYPcdBccNc","outputId":"0ad92bbf-b349-4005-bc70-d8a5fd515b3d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","  0%|          | 0/1841 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:454: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 0; lr 0.0002020; Loss 0.0380 (0.0380); cce_loss 0.0380 (0.0380); Dice 0.4057 (0.4057):   0%|          | 0/1841 [00:25<?, ?it/s]<ipython-input-8-b49c52c0b773>:70: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)\n","  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","epoch: 0; lr 0.0002020; Loss 0.0490 (0.0918); cce_loss 0.0490 (0.0918); Dice 0.5480 (0.4034): 100%|██████████| 1841/1841 [1:24:41<00:00,  2.76s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 0; lr 0.0002020; Loss 0.0918; CCE_loss 0.0918; Dice 0.4034\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1841 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:454: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 1; lr 0.0002020; Loss 0.0213 (0.0890); cce_loss 0.0213 (0.0890); Dice 0.2586 (0.4666): 100%|██████████| 1841/1841 [13:15<00:00,  2.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 1; lr 0.0002020; Loss 0.0890; CCE_loss 0.0890; Dice 0.4666\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1841 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:454: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 2; lr 0.0002020; Loss 0.0557 (0.0868); cce_loss 0.0557 (0.0868); Dice 0.6928 (0.5314): 100%|██████████| 1841/1841 [13:18<00:00,  2.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 2; lr 0.0002020; Loss 0.0868; CCE_loss 0.0868; Dice 0.5314\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1841 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:454: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 3; lr 0.0002020; Loss 0.0523 (0.0874); cce_loss 0.0523 (0.0874); Dice 0.4880 (0.5203): 100%|██████████| 1841/1841 [13:16<00:00,  2.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 3; lr 0.0002020; Loss 0.0874; CCE_loss 0.0874; Dice 0.5203\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1841 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:454: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 4; lr 0.0002020; Loss 0.0228 (0.0827); cce_loss 0.0228 (0.0827); Dice 0.2768 (0.5405): 100%|██████████| 1841/1841 [13:19<00:00,  2.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 4; lr 0.0000505; Loss 0.0827; CCE_loss 0.0827; Dice 0.5405\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 151/151 [08:55<00:00,  3.54s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.7978152097754885, Dice: 1.0, F1: 0.711164585393555, F1_0: 0.9460272463838366, F1_1: 0.4984525868667944, F1_2: 0.7148341046176037, F1_3: 0.860290065676239\n","score: [0.7978152097754885, 1.0, 0.711164585393555, 0.9460272463838366, 0.4984525868667944, 0.7148341046176037, 0.860290065676239]\tscore_best: 0.7978152097754885\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1841 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:454: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 5; lr 0.0000505; Loss 0.0495 (0.0771); cce_loss 0.0495 (0.0771); Dice 0.5755 (0.5272): 100%|██████████| 1841/1841 [33:05<00:00,  1.08s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 5; lr 0.0001010; Loss 0.0771; CCE_loss 0.0771; Dice 0.5272\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1841 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:454: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 6; lr 0.0001010; Loss 0.0044 (0.0760); cce_loss 0.0044 (0.0760); Dice 0.2256 (0.5248): 100%|██████████| 1841/1841 [15:12<00:00,  2.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 6; lr 0.0001010; Loss 0.0760; CCE_loss 0.0760; Dice 0.5248\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 151/151 [10:41<00:00,  4.25s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.8329680623491282, Dice: 1.0, F1: 0.7613829462130404, F1_0: 0.9470451315402619, F1_1: 0.583276130374321, F1_2: 0.7535114674153771, F1_3: 0.8649653225213991\n","score: [0.8329680623491282, 1.0, 0.7613829462130404, 0.9470451315402619, 0.583276130374321, 0.7535114674153771, 0.8649653225213991]\tscore_best: 0.8329680623491282\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1841 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:454: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 7; lr 0.0001010; Loss 0.0584 (0.0761); cce_loss 0.0584 (0.0761); Dice 0.6052 (0.5231): 100%|██████████| 1841/1841 [39:35<00:00,  1.29s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 7; lr 0.0001010; Loss 0.0761; CCE_loss 0.0761; Dice 0.5231\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1841 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:454: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 8; lr 0.0001010; Loss 0.0135 (0.0738); cce_loss 0.0135 (0.0738); Dice 0.3775 (0.5287): 100%|██████████| 1841/1841 [14:55<00:00,  2.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 8; lr 0.0001010; Loss 0.0738; CCE_loss 0.0738; Dice 0.5287\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 151/151 [09:42<00:00,  3.86s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.8283407400108025, Dice: 1.0, F1: 0.7547724857297178, F1_0: 0.9477254353096602, F1_1: 0.5754754330787337, F1_2: 0.7474602253112391, F1_3: 0.8555036054876959\n","score: [0.8283407400108025, 1.0, 0.7547724857297178, 0.9477254353096602, 0.5754754330787337, 0.7474602253112391, 0.8555036054876959]\tscore_best: 0.8329680623491282\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1841 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:454: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 9; lr 0.0001010; Loss 0.0319 (0.0717); cce_loss 0.0319 (0.0717); Dice 0.4654 (0.5611):  11%|█         | 206/1841 [02:28<17:43,  1.54it/s]"]}],"source":["import ssl\n","\n","random_combinations = [\n","    (['after_conv10'],['dilatation'], [3], ['square']),\n","]\n","\n","for layers, ops, k_sizes, shape_types in random_combinations:\n","    ssl._create_default_https_context = ssl._create_unverified_context\n","    t0 = timeit.default_timer()\n","\n","    params = model.parameters()\n","\n","    optimizer = AdamW(params, lr=0.000202, weight_decay=1e-6)     #0.002\n","\n","    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[5, 13, 19, 23, 28, 47, 50, 60, 70, 90, 110, 130, 150, 170, 180, 190], gamma=0.5)\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    model = nn.DataParallel(model).cuda()\n","\n","    seg_loss = ComboLoss({'dice': 0.5, 'focal': 2.0}, per_image=False).cuda()\n","    ce_loss = nn.CrossEntropyLoss().cuda()\n","    seg_lossSeesaw = None#SeesawLoss2().cuda()\n","\n","    best_score = 0\n","    torch.cuda.empty_cache()\n","    for epoch in range(10):\n","        train_metrics = train_epoch(epoch, seg_loss, ce_loss, seg_lossSeesaw, model, optimizer, scheduler, train_data_loader)\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        if epoch % 2 == 0 and epoch >= 4:\n","          torch.save(model.state_dict(), f'/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/after_conv10_{ops}_{k_sizes}_{shape_types}.pth')\n","          best_score, d = evaluate_val(val_data_loader, best_score, model, snapshot_name, epoch)\n","        try:\n","          results = pd.read_csv(f'/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_morofologic_{ops}_{k_sizes}_{shape_types}.csv')\n","          results_iter = pd.DataFrame([[epoch, train_metrics[0], train_metrics[1].avg, train_metrics[2].avg, train_metrics[3].avg, layers, ops, k_sizes, shape_types, d[0], d[1], d[2], d[3], d[4], d[5], d[6]]],columns=results.columns)\n","        except:\n","          results_iter = pd.DataFrame([[epoch, train_metrics[0], train_metrics[1].avg, train_metrics[2].avg, train_metrics[3].avg, layers, ops, k_sizes, shape_types, None, None, None, None, None, None, None]],columns=results.columns)\n","        results = pd.concat([results,results_iter])\n","        d = None\n","        results.to_csv(f'/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_morofologic_{ops}_{k_sizes}_{shape_types}.csv', index=False)\n","\n","    try:\n","      #del model\n","      torch.cuda.empty_cache()\n","    except:\n","      None\n","\n","    elapsed = timeit.default_timer() - t0\n","    print('Time: {:.3f} min'.format(elapsed / 60))"]},{"cell_type":"code","source":["best_score, d = evaluate_val(val_data_loader, best_score, model, snapshot_name, epoch)"],"metadata":{"id":"m7uIASjs2BO9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":528,"status":"ok","timestamp":1706719569515,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"jo35FjpNjhio","outputId":"d83a0230-7ee3-4374-9ae7-7b5dacbd948a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Jan 31 16:46:06 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0              50W / 400W |  35421MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["gc.collect()\n","torch.cuda.empty_cache()\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D2ULYI3y6pMz"},"outputs":[],"source":["model = None\n","del model\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zSpL-BZAq7SJ"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WiAQ0sh12alo"},"outputs":[],"source":["x=torch.randn(12, 64, 512, 512)\n","e=Erosion2d(64, 4, 3, soft_max=False)\n","y=e(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706622902497,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"OaieYauVoixX","outputId":"054be29f-ad6f-4e06-e574-c43449f0c663"},"outputs":[{"data":{"text/plain":["torch.Size([12, 64, 512, 512])"]},"execution_count":137,"metadata":{},"output_type":"execute_result"}],"source":["x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706622902497,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"YYmdlJAHoiHf","outputId":"2ffe95aa-07dd-4dbf-9a0f-04dddfc07eca"},"outputs":[{"data":{"text/plain":["torch.Size([4, 64, 3, 3])"]},"execution_count":138,"metadata":{},"output_type":"execute_result"}],"source":["e.weight.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1706622902497,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"_b-XfgqNolZh","outputId":"0a44a1a7-988f-4bd3-af9c-9408a0ace96f"},"outputs":[{"data":{"text/plain":["torch.Size([12, 4, 512, 512])"]},"execution_count":139,"metadata":{},"output_type":"execute_result"}],"source":["y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wEfA88x6onNu"},"outputs":[],"source":["e=Dilation2d(4, 64, 3, soft_max=False)\n","y_2=e(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1706622916759,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"OvfWavfwrww2","outputId":"8f0f4753-e9c3-4ebc-a59b-ba384a7f8464"},"outputs":[{"data":{"text/plain":["torch.Size([12, 64, 512, 512])"]},"execution_count":141,"metadata":{},"output_type":"execute_result"}],"source":["y_2.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Io57nuwory7u"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["xZ0IEwYcRzAl","TlRP-duNXnpK","yUSNtuVeTUwl","pTW3yt84TQMf","FXa807PbTjxn","Ifa7HjK2FVf3","WRj40gRtIhtg"],"machine_shape":"hm","provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyPU5HreI6AYzsUTweg8aFXd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}