{"cells":[{"cell_type":"markdown","metadata":{"id":"xZ0IEwYcRzAl"},"source":["## Requirements"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22224,"status":"ok","timestamp":1695246657562,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"iCeN-zCbXqfV","outputId":"66111552-9944-4191-e934-32fbf5549820"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1695246657562,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"NI4_17gjk3dU","outputId":"a434ae3e-5f93-449e-e5cd-b4349b0ed588"},"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}],"source":["print(1)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"k-JXF71-k4O3","executionInfo":{"status":"ok","timestamp":1695246657562,"user_tz":180,"elapsed":3,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["#import os\n","#os.kill(os.getpid(), 9)"]},{"cell_type":"markdown","metadata":{"id":"TlRP-duNXnpK"},"source":["## Iniciation"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":89938,"status":"ok","timestamp":1695246987109,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"uOyfTgUDztSD","outputId":"2d3b0bd6-21e9-49ef-ba46-3b5c88959523"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 2.0.1+cu118\n","Uninstalling torch-2.0.1+cu118:\n","  Successfully uninstalled torch-2.0.1+cu118\n","Found existing installation: torchvision 0.15.2+cu118\n","Uninstalling torchvision-0.15.2+cu118:\n","  Successfully uninstalled torchvision-0.15.2+cu118\n","Found existing installation: torchaudio 2.0.2+cu118\n","Uninstalling torchaudio-2.0.2+cu118:\n","  Successfully uninstalled torchaudio-2.0.2+cu118\n","Looking in indexes: https://download.pytorch.org/whl/cu118\n","Collecting torch\n","  Using cached https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n","Collecting torchvision\n","  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n","Collecting torchaudio\n","  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp310-cp310-linux_x86_64.whl (4.4 MB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: torch, torchvision, torchaudio\n","Successfully installed torch-2.0.1+cu118 torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["nvfuser","torch"]}}},"metadata":{}}],"source":["!pip uninstall torch torchvision torchaudio -y\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1695246987109,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"dSxd1Sjk6_2a","outputId":"fb7fb7cb-dae7-4f25-affd-7596c96fa421"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1695246987110,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"K0WhAIMi0KX1","outputId":"30f11c6c-a111-49c3-89d3-d45063b60c05"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.1+cu118\n"]}],"source":["import torch\n","print(torch.__version__)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":872,"status":"ok","timestamp":1695246987970,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"ejMJGR8w0nmM","outputId":"a074390d-1863-488a-f4ec-022d6b849183"},"outputs":[{"output_type":"stream","name":"stdout","text":["absl-py==1.4.0\n","aiohttp==3.8.5\n","aiosignal==1.3.1\n","alabaster==0.7.13\n","albumentations==1.3.1\n","altair==4.2.2\n","anyio==3.7.1\n","appdirs==1.4.4\n","argon2-cffi==23.1.0\n","argon2-cffi-bindings==21.2.0\n","array-record==0.4.1\n","arviz==0.15.1\n","astropy==5.3.3\n","astunparse==1.6.3\n","async-timeout==4.0.3\n","attrs==23.1.0\n","audioread==3.0.0\n","autograd==1.6.2\n","Babel==2.12.1\n","backcall==0.2.0\n","beautifulsoup4==4.11.2\n","bleach==6.0.0\n","blinker==1.4\n","blis==0.7.10\n","blosc2==2.0.0\n","bokeh==3.2.2\n","bqplot==0.12.40\n","branca==0.6.0\n","build==1.0.3\n","CacheControl==0.13.1\n","cachetools==5.3.1\n","catalogue==2.0.9\n","certifi==2023.7.22\n","cffi==1.15.1\n","chardet==5.2.0\n","charset-normalizer==3.2.0\n","chex==0.1.7\n","click==8.1.7\n","click-plugins==1.1.1\n","cligj==0.7.2\n","cloudpickle==2.2.1\n","cmake==3.27.4.1\n","cmdstanpy==1.1.0\n","colorcet==3.0.1\n","colorlover==0.3.0\n","colour==0.1.5\n","community==1.0.0b1\n","confection==0.1.2\n","cons==0.4.6\n","contextlib2==21.6.0\n","contourpy==1.1.0\n","convertdate==2.4.0\n","cryptography==41.0.3\n","cufflinks==0.17.3\n","cupy-cuda11x==11.0.0\n","cvxopt==1.3.2\n","cvxpy==1.3.2\n","cycler==0.11.0\n","cymem==2.0.7\n","Cython==3.0.2\n","dask==2023.8.1\n","datascience==0.17.6\n","db-dtypes==1.1.1\n","dbus-python==1.2.18\n","debugpy==1.6.6\n","decorator==4.4.2\n","defusedxml==0.7.1\n","distributed==2023.8.1\n","distro==1.7.0\n","dlib==19.24.2\n","dm-tree==0.1.8\n","docutils==0.18.1\n","dopamine-rl==4.0.6\n","duckdb==0.8.1\n","earthengine-api==0.1.368\n","easydict==1.10\n","ecos==2.0.12\n","editdistance==0.6.2\n","eerepr==0.0.4\n","en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl#sha256=83276fc78a70045627144786b52e1f2728ad5e29e5e43916ec37ea9c26a11212\n","entrypoints==0.4\n","ephem==4.1.4\n","et-xmlfile==1.1.0\n","etils==1.4.1\n","etuples==0.3.9\n","exceptiongroup==1.1.3\n","fastai==2.7.12\n","fastcore==1.5.29\n","fastdownload==0.0.7\n","fastjsonschema==2.18.0\n","fastprogress==1.0.3\n","fastrlock==0.8.2\n","filelock==3.12.2\n","Fiona==1.9.4.post1\n","firebase-admin==5.3.0\n","Flask==2.2.5\n","flatbuffers==23.5.26\n","flax==0.7.2\n","folium==0.14.0\n","fonttools==4.42.1\n","frozendict==2.3.8\n","frozenlist==1.4.0\n","fsspec==2023.6.0\n","future==0.18.3\n","gast==0.4.0\n","gcsfs==2023.6.0\n","GDAL==3.4.3\n","gdown==4.6.6\n","geemap==0.26.0\n","gensim==4.3.2\n","geocoder==1.38.1\n","geographiclib==2.0\n","geopandas==0.13.2\n","geopy==2.3.0\n","gin-config==0.5.0\n","glob2==0.7\n","google==2.0.3\n","google-api-core==2.11.1\n","google-api-python-client==2.84.0\n","google-auth==2.17.3\n","google-auth-httplib2==0.1.0\n","google-auth-oauthlib==1.0.0\n","google-cloud-bigquery==3.10.0\n","google-cloud-bigquery-connection==1.12.1\n","google-cloud-bigquery-storage==2.22.0\n","google-cloud-core==2.3.3\n","google-cloud-datastore==2.15.2\n","google-cloud-firestore==2.11.1\n","google-cloud-functions==1.13.2\n","google-cloud-language==2.9.1\n","google-cloud-storage==2.8.0\n","google-cloud-translate==3.11.3\n","google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz#sha256=9ed579c170362a33cac2a00a4eca57a80f72dbc70b9694b589c0037378ffbc27\n","google-crc32c==1.5.0\n","google-pasta==0.2.0\n","google-resumable-media==2.6.0\n","googleapis-common-protos==1.60.0\n","googledrivedownloader==0.4\n","graphviz==0.20.1\n","greenlet==2.0.2\n","grpc-google-iam-v1==0.12.6\n","grpcio==1.57.0\n","grpcio-status==1.48.2\n","gspread==3.4.2\n","gspread-dataframe==3.3.1\n","gym==0.25.2\n","gym-notices==0.0.8\n","h5netcdf==1.2.0\n","h5py==3.9.0\n","holidays==0.32\n","holoviews==1.17.1\n","html5lib==1.1\n","httpimport==1.3.1\n","httplib2==0.22.0\n","humanize==4.7.0\n","hyperopt==0.2.7\n","idna==3.4\n","imageio==2.31.3\n","imageio-ffmpeg==0.4.8\n","imagesize==1.4.1\n","imbalanced-learn==0.10.1\n","imgaug==0.4.0\n","importlib-metadata==6.8.0\n","importlib-resources==6.0.1\n","imutils==0.5.4\n","inflect==7.0.0\n","iniconfig==2.0.0\n","intel-openmp==2023.2.0\n","ipyevents==2.0.2\n","ipyfilechooser==0.6.0\n","ipykernel==5.5.6\n","ipyleaflet==0.17.3\n","ipython==7.34.0\n","ipython-genutils==0.2.0\n","ipython-sql==0.5.0\n","ipytree==0.2.2\n","ipywidgets==7.7.1\n","itsdangerous==2.1.2\n","jax==0.4.14\n","jaxlib @ https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.14+cuda11.cudnn86-cp310-cp310-manylinux2014_x86_64.whl#sha256=09c439923a785df517e3f470158dd3e46de454a01eca80935eed4ba383b5e90a\n","jeepney==0.7.1\n","jieba==0.42.1\n","Jinja2==3.1.2\n","joblib==1.3.2\n","jsonpickle==3.0.2\n","jsonschema==4.19.0\n","jsonschema-specifications==2023.7.1\n","jupyter-client==6.1.12\n","jupyter-console==6.1.0\n","jupyter-server==1.24.0\n","jupyter_core==5.3.1\n","jupyterlab-pygments==0.2.2\n","jupyterlab-widgets==3.0.8\n","kaggle==1.5.16\n","keras==2.13.1\n","keyring==23.5.0\n","kiwisolver==1.4.5\n","langcodes==3.3.0\n","launchpadlib==1.10.16\n","lazr.restfulclient==0.14.4\n","lazr.uri==1.0.6\n","lazy_loader==0.3\n","libclang==16.0.6\n","librosa==0.10.1\n","lightgbm==4.0.0\n","linkify-it-py==2.0.2\n","lit==16.0.6\n","llvmlite==0.39.1\n","locket==1.0.0\n","logical-unification==0.4.6\n","LunarCalendar==0.0.9\n","lxml==4.9.3\n","Markdown==3.4.4\n","markdown-it-py==3.0.0\n","MarkupSafe==2.1.3\n","matplotlib==3.7.1\n","matplotlib-inline==0.1.6\n","matplotlib-venn==0.11.9\n","mdit-py-plugins==0.4.0\n","mdurl==0.1.2\n","miniKanren==1.0.3\n","missingno==0.5.2\n","mistune==0.8.4\n","mizani==0.9.3\n","mkl==2023.2.0\n","ml-dtypes==0.2.0\n","mlxtend==0.22.0\n","more-itertools==10.1.0\n","moviepy==1.0.3\n","mpmath==1.3.0\n","msgpack==1.0.5\n","multidict==6.0.4\n","multipledispatch==1.0.0\n","multitasking==0.0.11\n","murmurhash==1.0.9\n","music21==9.1.0\n","natsort==8.4.0\n","nbclassic==1.0.0\n","nbclient==0.8.0\n","nbconvert==6.5.4\n","nbformat==5.9.2\n","nest-asyncio==1.5.7\n","networkx==3.1\n","nibabel==4.0.2\n","nltk==3.8.1\n","notebook==6.5.5\n","notebook_shim==0.2.3\n","numba==0.56.4\n","numexpr==2.8.5\n","numpy==1.23.5\n","oauth2client==4.1.3\n","oauthlib==3.2.2\n","opencv-contrib-python==4.8.0.76\n","opencv-python==4.8.0.76\n","opencv-python-headless==4.8.0.76\n","openpyxl==3.1.2\n","opt-einsum==3.3.0\n","optax==0.1.7\n","orbax-checkpoint==0.3.5\n","osqp==0.6.2.post8\n","packaging==23.1\n","pandas==1.5.3\n","pandas-datareader==0.10.0\n","pandas-gbq==0.17.9\n","pandocfilters==1.5.0\n","panel==1.2.2\n","param==1.13.0\n","parso==0.8.3\n","partd==1.4.0\n","pathlib==1.0.1\n","pathy==0.10.2\n","patsy==0.5.3\n","pexpect==4.8.0\n","pickleshare==0.7.5\n","Pillow==9.4.0\n","pip-tools==6.13.0\n","platformdirs==3.10.0\n","plotly==5.15.0\n","plotnine==0.12.3\n","pluggy==1.3.0\n","polars==0.17.3\n","pooch==1.7.0\n","portpicker==1.5.2\n","prefetch-generator==1.0.3\n","preshed==3.0.8\n","prettytable==3.8.0\n","proglog==0.1.10\n","progressbar2==4.2.0\n","prometheus-client==0.17.1\n","promise==2.3\n","prompt-toolkit==3.0.39\n","prophet==1.1.4\n","proto-plus==1.22.3\n","protobuf==3.20.3\n","psutil==5.9.5\n","psycopg2==2.9.7\n","ptyprocess==0.7.0\n","py-cpuinfo==9.0.0\n","py4j==0.10.9.7\n","pyarrow==9.0.0\n","pyasn1==0.5.0\n","pyasn1-modules==0.3.0\n","pycocotools==2.0.7\n","pycparser==2.21\n","pyct==0.5.0\n","pydantic==1.10.12\n","pydata-google-auth==1.8.2\n","pydot==1.4.2\n","pydot-ng==2.0.0\n","pydotplus==2.0.2\n","PyDrive==1.3.1\n","PyDrive2==1.6.3\n","pyerfa==2.0.0.3\n","pygame==2.5.1\n","Pygments==2.16.1\n","PyGObject==3.42.1\n","PyJWT==2.3.0\n","pymc==5.7.2\n","PyMeeus==0.5.12\n","pymystem3==0.2.0\n","PyOpenGL==3.1.7\n","pyOpenSSL==23.2.0\n","pyparsing==3.1.1\n","pyperclip==1.8.2\n","pyproj==3.6.0\n","pyproject_hooks==1.0.0\n","pyshp==2.3.1\n","PySocks==1.7.1\n","pytensor==2.14.2\n","pytest==7.4.1\n","python-apt==0.0.0\n","python-box==7.1.1\n","python-dateutil==2.8.2\n","python-louvain==0.16\n","python-slugify==8.0.1\n","python-utils==3.7.0\n","pytz==2023.3.post1\n","pyviz_comms==3.0.0\n","PyWavelets==1.4.1\n","PyYAML==6.0.1\n","pyzmq==23.2.1\n","qdldl==0.1.7.post0\n","qudida==0.0.4\n","ratelim==0.1.6\n","referencing==0.30.2\n","regex==2023.6.3\n","requests==2.31.0\n","requests-oauthlib==1.3.1\n","requirements-parser==0.5.0\n","rich==13.5.2\n","rpds-py==0.10.2\n","rpy2==3.4.2\n","rsa==4.9\n","scikit-image==0.19.3\n","scikit-learn==1.2.2\n","scipy==1.11.2\n","scooby==0.7.2\n","scs==3.2.3\n","seaborn==0.12.2\n","SecretStorage==3.3.1\n","Send2Trash==1.8.2\n","shapely==2.0.1\n","six==1.16.0\n","sklearn-pandas==2.2.0\n","smart-open==6.4.0\n","sniffio==1.3.0\n","snowballstemmer==2.2.0\n","sortedcontainers==2.4.0\n","soundfile==0.12.1\n","soupsieve==2.5\n","soxr==0.3.6\n","spacy==3.6.1\n","spacy-legacy==3.0.12\n","spacy-loggers==1.0.4\n","Sphinx==5.0.2\n","sphinxcontrib-applehelp==1.0.7\n","sphinxcontrib-devhelp==1.0.5\n","sphinxcontrib-htmlhelp==2.0.4\n","sphinxcontrib-jsmath==1.0.1\n","sphinxcontrib-qthelp==1.0.6\n","sphinxcontrib-serializinghtml==1.1.9\n","SQLAlchemy==2.0.20\n","sqlparse==0.4.4\n","srsly==2.4.7\n","statsmodels==0.14.0\n","sympy==1.12\n","tables==3.8.0\n","tabulate==0.9.0\n","tbb==2021.10.0\n","tblib==2.0.0\n","tenacity==8.2.3\n","tensorboard==2.13.0\n","tensorboard-data-server==0.7.1\n","tensorflow==2.13.0\n","tensorflow-datasets==4.9.2\n","tensorflow-estimator==2.13.0\n","tensorflow-gcs-config==2.13.0\n","tensorflow-hub==0.14.0\n","tensorflow-io-gcs-filesystem==0.33.0\n","tensorflow-metadata==1.14.0\n","tensorflow-probability==0.20.1\n","tensorstore==0.1.41\n","termcolor==2.3.0\n","terminado==0.17.1\n","text-unidecode==1.3\n","textblob==0.17.1\n","tf-slim==1.1.0\n","thinc==8.1.12\n","threadpoolctl==3.2.0\n","tifffile==2023.8.30\n","tinycss2==1.2.1\n","toml==0.10.2\n","tomli==2.0.1\n","toolz==0.12.0\n","torch==2.0.1+cu118\n","torchaudio==2.0.2+cu118\n","torchdata==0.6.1\n","torchsummary==1.5.1\n","torchtext==0.15.2\n","torchvision==0.15.2+cu118\n","tornado==6.3.2\n","tqdm==4.66.1\n","traitlets==5.7.1\n","traittypes==0.2.1\n","triton==2.0.0\n","tweepy==4.13.0\n","typer==0.9.0\n","types-setuptools==68.2.0.0\n","typing_extensions==4.5.0\n","tzlocal==5.0.1\n","uc-micro-py==1.0.2\n","uritemplate==4.1.1\n","urllib3==2.0.4\n","vega-datasets==0.9.0\n","wadllib==1.3.6\n","wasabi==1.1.2\n","wcwidth==0.2.6\n","webcolors==1.13\n","webencodings==0.5.1\n","websocket-client==1.6.2\n","Werkzeug==2.3.7\n","widgetsnbextension==3.6.5\n","wordcloud==1.9.2\n","wrapt==1.15.0\n","xarray==2023.7.0\n","xarray-einstats==0.6.0\n","xgboost==1.7.6\n","xlrd==2.0.1\n","xyzservices==2023.7.0\n","yarl==1.9.2\n","yellowbrick==1.5\n","yfinance==0.2.28\n","zict==3.0.0\n","zipp==3.16.2\n"]}],"source":["!pip freeze"]},{"cell_type":"markdown","metadata":{"id":"yUSNtuVeTUwl"},"source":["## Optimazer"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"HZMtCepVRDaJ","executionInfo":{"status":"ok","timestamp":1695246987970,"user_tz":180,"elapsed":8,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["# Based on https://github.com/pytorch/pytorch/pull/3740\n","import torch\n","import math\n","\n","\n","class AdamW(torch.optim.Optimizer):\n","    \"\"\"Implements AdamW algorithm.\n","\n","    It has been proposed in `Fixing Weight Decay Regularization in Adam`_.\n","\n","    Arguments:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float, optional): learning rate (default: 1e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of gradient and its square (default: (0.9, 0.999))\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability (default: 1e-8)\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","\n","    .. Fixing Weight Decay Regularization in Adam:\n","    https://arxiv.org/abs/1711.05101\n","    \"\"\"\n","\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=0):\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay)\n","        super(AdamW, self).__init__(params, defaults)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n","\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","\n","                # according to the paper, this penalty should come after the bias correction\n","                # if group['weight_decay'] != 0:\n","                #     grad = grad.add(group['weight_decay'], p.data)\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","\n","                denom = exp_avg_sq.sqrt().add_(group['eps'])\n","\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n","\n","                # w = w - wd * lr * w\n","                if group['weight_decay'] != 0:\n","                    p.data.add_(-group['weight_decay'] * group['lr'], p.data)\n","\n","                # w = w - lr * w.grad\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                # w = w - wd * lr * w - lr * w.grad\n","                # See http://www.fast.ai/2018/07/02/adam-weight-decay/\n","\n","        return loss"]},{"cell_type":"markdown","metadata":{"id":"pTW3yt84TQMf"},"source":["## Losses"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"MR5-L7m_SCwH","executionInfo":{"status":"ok","timestamp":1695246987971,"user_tz":180,"elapsed":7,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch.autograd import Variable\n","\n","try:\n","    from itertools import ifilterfalse\n","except ImportError:  # py3k\n","    from itertools import filterfalse\n","\n","eps = 1e-6\n","\n","def dice_round(preds, trues):\n","    preds = preds.float()\n","    return soft_dice_loss(preds, trues)\n","\n","\n","def iou_round(preds, trues):\n","    preds = preds.float()\n","    return jaccard(preds, trues)\n","\n","\n","def soft_dice_loss(outputs, targets, per_image=False):\n","    batch_size = outputs.size()[0]\n","    if not per_image:\n","        batch_size = 1\n","    dice_target = targets.contiguous().view(batch_size, -1).float()\n","    dice_output = outputs.contiguous().view(batch_size, -1)\n","    intersection = torch.sum(dice_output * dice_target, dim=1)\n","    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1) + eps\n","    loss = (1 - (2 * intersection + eps) / union).mean()\n","    return loss\n","\n","\n","def jaccard(outputs, targets, per_image=False):\n","    batch_size = outputs.size()[0]\n","    if not per_image:\n","        batch_size = 1\n","    dice_target = targets.contiguous().view(batch_size, -1).float()\n","    dice_output = outputs.contiguous().view(batch_size, -1)\n","    intersection = torch.sum(dice_output * dice_target, dim=1)\n","    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1) - intersection + eps\n","    losses = 1 - (intersection + eps) / union\n","    return losses.mean()\n","\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True, per_image=False):\n","        super().__init__()\n","        self.size_average = size_average\n","        self.register_buffer('weight', weight)\n","        self.per_image = per_image\n","\n","    def forward(self, input, target):\n","        return soft_dice_loss(input, target, per_image=self.per_image)\n","\n","\n","class JaccardLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True, per_image=False):\n","        super().__init__()\n","        self.size_average = size_average\n","        self.register_buffer('weight', weight)\n","        self.per_image = per_image\n","\n","    def forward(self, input, target):\n","        return jaccard(input, target, per_image=self.per_image)\n","\n","\n","class StableBCELoss(nn.Module):\n","    def __init__(self):\n","        super(StableBCELoss, self).__init__()\n","\n","    def forward(self, input, target):\n","        input = input.float().view(-1)\n","        target = target.float().view(-1)\n","        neg_abs = - input.abs()\n","        # todo check correctness\n","        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n","        return loss.mean()\n","\n","\n","class ComboLoss(nn.Module):\n","    def __init__(self, weights, per_image=False):\n","        super().__init__()\n","        self.weights = weights\n","        self.bce = StableBCELoss()\n","        self.dice = DiceLoss(per_image=False)\n","        self.jaccard = JaccardLoss(per_image=False)\n","        self.lovasz = LovaszLoss(per_image=per_image)\n","        self.lovasz_sigmoid = LovaszLossSigmoid(per_image=per_image)\n","        self.focal = FocalLoss2d()\n","        self.mapping = {'bce': self.bce,\n","                        'dice': self.dice,\n","                        'focal': self.focal,\n","                        'jaccard': self.jaccard,\n","                        'lovasz': self.lovasz,\n","                        'lovasz_sigmoid': self.lovasz_sigmoid}\n","        self.expect_sigmoid = {'dice', 'focal', 'jaccard', 'lovasz_sigmoid'}\n","        self.values = {}\n","\n","    def forward(self, outputs, targets):\n","        loss = 0\n","        weights = self.weights\n","        sigmoid_input = torch.sigmoid(outputs)\n","        for k, v in weights.items():\n","            if not v:\n","                continue\n","            val = self.mapping[k](sigmoid_input if k in self.expect_sigmoid else outputs, targets)\n","            self.values[k] = val\n","            loss += self.weights[k] * val\n","        return loss\n","\n","\n","def lovasz_grad(gt_sorted):\n","    \"\"\"\n","    Computes gradient of the Lovasz extension w.r.t sorted errors\n","    See Alg. 1 in paper\n","    \"\"\"\n","    p = len(gt_sorted)\n","    gts = gt_sorted.sum()\n","    intersection = gts.float() - gt_sorted.float().cumsum(0)\n","    union = gts.float() + (1 - gt_sorted).float().cumsum(0)\n","    jaccard = 1. - intersection / union\n","    if p > 1:  # cover 1-pixel case\n","        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n","    return jaccard\n","\n","\n","def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n","      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class id\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n","                    for log, lab in zip(logits, labels))\n","    else:\n","        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n","    return loss\n","\n","\n","def lovasz_hinge_flat(logits, labels):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n","      labels: [P] Tensor, binary ground truth labels (0 or 1)\n","      ignore: label to ignore\n","    \"\"\"\n","    if len(labels) == 0:\n","        # only void pixels, the gradients should be 0\n","        return logits.sum() * 0.\n","    signs = 2. * labels.float() - 1.\n","    errors = (1. - logits * Variable(signs))\n","    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n","    perm = perm.data\n","    gt_sorted = labels[perm]\n","    grad = lovasz_grad(gt_sorted)\n","    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n","    return loss\n","\n","\n","def flatten_binary_scores(scores, labels, ignore=None):\n","    \"\"\"\n","    Flattens predictions in the batch (binary case)\n","    Remove labels equal to 'ignore'\n","    \"\"\"\n","    scores = scores.view(-1)\n","    labels = labels.view(-1)\n","    if ignore is None:\n","        return scores, labels\n","    valid = (labels != ignore)\n","    vscores = scores[valid]\n","    vlabels = labels[valid]\n","    return vscores, vlabels\n","\n","\n","def lovasz_sigmoid(probas, labels, per_image=False, ignore=None):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n","      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n","      only_present: average only on classes present in ground truth\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class labels\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_sigmoid_flat(*flatten_binary_scores(prob.unsqueeze(0), lab.unsqueeze(0), ignore))\n","                          for prob, lab in zip(probas, labels))\n","    else:\n","        loss = lovasz_sigmoid_flat(*flatten_binary_scores(probas, labels, ignore))\n","    return loss\n","\n","\n","def lovasz_sigmoid_flat(probas, labels):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n","      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n","      only_present: average only on classes present in ground truth\n","    \"\"\"\n","    fg = labels.float()\n","    errors = (Variable(fg) - probas).abs()\n","    errors_sorted, perm = torch.sort(errors, 0, descending=True)\n","    perm = perm.data\n","    fg_sorted = fg[perm]\n","    loss = torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted)))\n","    return loss\n","\n","\n","def mean(l, ignore_nan=False, empty=0):\n","    \"\"\"\n","    nanmean compatible with generators.\n","    \"\"\"\n","    l = iter(l)\n","    if ignore_nan:\n","        l = ifilterfalse(np.isnan, l)\n","    try:\n","        n = 1\n","        acc = next(l)\n","    except StopIteration:\n","        if empty == 'raise':\n","            raise ValueError('Empty mean')\n","        return empty\n","    for n, v in enumerate(l, 2):\n","        acc += v\n","    if n == 1:\n","        return acc\n","    return acc / n\n","\n","\n","class LovaszLoss(nn.Module):\n","    def __init__(self, ignore_index=255, per_image=True):\n","        super().__init__()\n","        self.ignore_index = ignore_index\n","        self.per_image = per_image\n","\n","    def forward(self, outputs, targets):\n","        outputs = outputs.contiguous()\n","        targets = targets.contiguous()\n","        return lovasz_hinge(outputs, targets, per_image=self.per_image, ignore=self.ignore_index)\n","\n","\n","class LovaszLossSigmoid(nn.Module):\n","    def __init__(self, ignore_index=255, per_image=True):\n","        super().__init__()\n","        self.ignore_index = ignore_index\n","        self.per_image = per_image\n","\n","    def forward(self, outputs, targets):\n","        outputs = outputs.contiguous()\n","        targets = targets.contiguous()\n","        return lovasz_sigmoid(outputs, targets, per_image=self.per_image, ignore=self.ignore_index)\n","\n","\n","class FocalLoss2d(nn.Module):\n","    def __init__(self, gamma=2, ignore_index=255):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.ignore_index = ignore_index\n","\n","    def forward(self, outputs, targets):\n","        outputs = outputs.contiguous()\n","        targets = targets.contiguous()\n","        # eps = 1e-8\n","        non_ignored = targets.view(-1) != self.ignore_index\n","        targets = targets.view(-1)[non_ignored].float()\n","        outputs = outputs.contiguous().view(-1)[non_ignored]\n","        outputs = torch.clamp(outputs, eps, 1. - eps)\n","        targets = torch.clamp(targets, eps, 1. - eps)\n","        pt = (1 - targets) * (1 - outputs) + targets * outputs\n","        return (-(1. - pt) ** self.gamma * torch.log(pt)).mean()"]},{"cell_type":"markdown","metadata":{"id":"FXa807PbTjxn"},"source":["## Utils"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"7Fs2XBvfToVf","executionInfo":{"status":"ok","timestamp":1695246988333,"user_tz":180,"elapsed":12,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","#### Augmentations\n","def shift_image(img, shift_pnt):\n","    M = np.float32([[1, 0, shift_pnt[0]], [0, 1, shift_pnt[1]]])\n","    res = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]), borderMode=cv2.BORDER_REFLECT_101)\n","    return res\n","\n","\n","def rotate_image(image, angle, scale, rot_pnt):\n","    rot_mat = cv2.getRotationMatrix2D(rot_pnt, angle, scale)\n","    result = cv2.warpAffine(image, rot_mat, (image.shape[1], image.shape[0]), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101) #INTER_NEAREST\n","    return result\n","\n","\n","def gauss_noise(img, var=30):\n","    row, col, ch = img.shape\n","    mean = var\n","    sigma = var**0.5\n","    gauss = np.random.normal(mean,sigma,(row,col,ch))\n","    gauss = gauss.reshape(row,col,ch)\n","    gauss = (gauss - np.min(gauss)).astype(np.uint8)\n","    return np.clip(img.astype(np.int32) + gauss, 0, 255).astype('uint8')\n","\n","\n","def clahe(img, clipLimit=2.0, tileGridSize=(5,5)):\n","    img_yuv = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n","    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n","    img_yuv[:, :, 0] = clahe.apply(img_yuv[:, :, 0])\n","    img_output = cv2.cvtColor(img_yuv, cv2.COLOR_LAB2RGB)\n","    return img_output\n","\n","\n","def _blend(img1, img2, alpha):\n","    return np.clip(img1 * alpha + (1 - alpha) * img2, 0, 255).astype('uint8')\n","\n","\n","_alpha = np.asarray([0.114, 0.587, 0.299]).reshape((1, 1, 3))\n","def _grayscale(img):\n","    return np.sum(_alpha * img, axis=2, keepdims=True)\n","\n","\n","def saturation(img, alpha):\n","    gs = _grayscale(img)\n","    return _blend(img, gs, alpha)\n","\n","\n","def brightness(img, alpha):\n","    gs = np.zeros_like(img)\n","    return _blend(img, gs, alpha)\n","\n","\n","def contrast(img, alpha):\n","    gs = _grayscale(img)\n","    gs = np.repeat(gs.mean(), 3)\n","    return _blend(img, gs, alpha)\n","\n","\n","def change_hsv(img, h, s, v):\n","    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","    hsv = hsv.astype(int)\n","    hsv[:,:,0] += h\n","    hsv[:,:,0] = np.clip(hsv[:,:,0], 0, 255)\n","    hsv[:,:,1] += s\n","    hsv[:,:,1] = np.clip(hsv[:,:,1], 0, 255)\n","    hsv[:,:,2] += v\n","    hsv[:,:,2] = np.clip(hsv[:,:,2], 0, 255)\n","    hsv = hsv.astype('uint8')\n","    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","    return img\n","\n","def shift_channels(img, b_shift, g_shift, r_shift):\n","    img = img.astype(int)\n","    img[:,:,0] += b_shift\n","    img[:,:,0] = np.clip(img[:,:,0], 0, 255)\n","    img[:,:,1] += g_shift\n","    img[:,:,1] = np.clip(img[:,:,1], 0, 255)\n","    img[:,:,2] += r_shift\n","    img[:,:,2] = np.clip(img[:,:,2], 0, 255)\n","    img = img.astype('uint8')\n","    return img\n","\n","def invert(img):\n","    return 255 - img\n","\n","def channel_shuffle(img):\n","    ch_arr = [0, 1, 2]\n","    np.random.shuffle(ch_arr)\n","    img = img[..., ch_arr]\n","    return img\n","\n","#######\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","\n","def preprocess_inputs(x):\n","    x = np.asarray(x, dtype='float32')\n","    x /= 127\n","    x -= 1\n","    return x\n","\n","\n","def dice(im1, im2, empty_score=1.0):\n","    \"\"\"\n","    Computes the Dice coefficient, a measure of set similarity.\n","    Parameters\n","    ----------\n","    im1 : array-like, bool\n","        Any array of arbitrary size. If not boolean, will be converted.\n","    im2 : array-like, bool\n","        Any other array of identical size. If not boolean, will be converted.\n","    Returns\n","    -------\n","    dice : float\n","        Dice coefficient as a float on range [0,1].\n","        Maximum similarity = 1\n","        No similarity = 0\n","        Both are empty (sum eq to zero) = empty_score\n","\n","    Notes\n","    -----\n","    The order of inputs for `dice` is irrelevant. The result will be\n","    identical if `im1` and `im2` are switched.\n","    \"\"\"\n","    im1 = np.asarray(im1).astype(np.bool)\n","    im2 = np.asarray(im2).astype(np.bool)\n","\n","    if im1.shape != im2.shape:\n","        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n","\n","    im_sum = im1.sum() + im2.sum()\n","    if im_sum == 0:\n","        return empty_score\n","\n","    # Compute Dice coefficient\n","    intersection = np.logical_and(im1, im2)\n","\n","    return 2. * intersection.sum() / im_sum\n","\n","\n","def iou(im1, im2, empty_score=1.0):\n","    im1 = np.asarray(im1).astype(np.bool)\n","    im2 = np.asarray(im2).astype(np.bool)\n","\n","    if im1.shape != im2.shape:\n","        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n","\n","    union = np.logical_or(im1, im2)\n","    im_sum = union.sum()\n","    if im_sum == 0:\n","        return empty_score\n","\n","    # Compute Dice coefficient\n","    intersection = np.logical_and(im1, im2)\n","\n","    return intersection.sum() / im_sum"]},{"cell_type":"markdown","metadata":{"id":"Ifa7HjK2FVf3"},"source":["## modelMscale"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"RoOkAXB2n4nS","executionInfo":{"status":"ok","timestamp":1695246988334,"user_tz":180,"elapsed":11,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import Conv2D, Activation, UpSampling2D, Concatenate, Input\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.initializers import HeNormal\n","from tensorflow.keras import backend as K\n","\n","class ConvRelu(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3):\n","        super(ConvRelu, self).__init__()\n","        self.layer = nn.Sequential(\n","            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1),\n","            nn.ReLU(inplace=True, )\n","        )\n","    #@autocast()\n","    def forward(self, x):\n","        return self.layer(x)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"sDIsasbMn5W8","executionInfo":{"status":"ok","timestamp":1695246988680,"user_tz":180,"elapsed":355,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["\"\"\"\n","ResNet code gently borrowed from\n","https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n","\"\"\"\n","\n","from collections import OrderedDict\n","import math\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils import model_zoo\n","\n","__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n","           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n","\n","pretrained_settings = {\n","    'senet154': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnet50': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnet101': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnet152': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnext50_32x4d': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnext101_32x4d': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","}\n","\n","\n","class SEModule(nn.Module):\n","\n","    def __init__(self, channels, reduction, concat=False):\n","        super(SEModule, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n","                             padding=0)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n","                             padding=0)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        module_input = x\n","        x = self.avg_pool(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.sigmoid(x)\n","        return module_input * x\n","\n","class SCSEModule(nn.Module):\n","    # according to https://arxiv.org/pdf/1808.08127.pdf concat is better\n","    def __init__(self, channels, reduction=16, concat=False):\n","        super(SCSEModule, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n","                             padding=0)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n","                             padding=0)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        self.spatial_se = nn.Sequential(nn.Conv2d(channels, 1, kernel_size=1,\n","                                                  stride=1, padding=0, bias=False),\n","                                        nn.Sigmoid())\n","        self.concat = concat\n","\n","    def forward(self, x):\n","        module_input = x\n","\n","        x = self.avg_pool(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        chn_se = self.sigmoid(x)\n","        chn_se = chn_se * module_input\n","\n","        spa_se = self.spatial_se(module_input)\n","        spa_se = module_input * spa_se\n","        if self.concat:\n","            return torch.cat([chn_se, spa_se], dim=1)\n","        else:\n","            return chn_se + spa_se\n","\n","class Bottleneck(nn.Module):\n","    \"\"\"\n","    Base class for bottlenecks that implements `forward()` method.\n","    \"\"\"\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out = self.se_module(out) + residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class SEBottleneck(Bottleneck):\n","    \"\"\"\n","    Bottleneck for SENet154.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None):\n","        super(SEBottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes * 2)\n","        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n","                               stride=stride, padding=1, groups=groups,\n","                               bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes * 4)\n","        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n","                               bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SCSEBottleneck(Bottleneck):\n","    \"\"\"\n","    Bottleneck for SENet154.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None):\n","        super(SCSEBottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes * 2)\n","        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n","                               stride=stride, padding=1, groups=groups,\n","                               bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes * 4)\n","        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n","                               bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SCSEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SEResNetBottleneck(Bottleneck):\n","    \"\"\"\n","    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n","    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n","    (the latter is used in the torchvision implementation of ResNet).\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None):\n","        super(SEResNetBottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n","                               stride=stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n","                               groups=groups, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SEResNeXtBottleneck(Bottleneck):\n","    \"\"\"\n","    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None, base_width=4):\n","        super(SEResNeXtBottleneck, self).__init__()\n","        width = math.floor(planes * (base_width / 64)) * groups\n","        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n","                               stride=1)\n","        self.bn1 = nn.BatchNorm2d(width)\n","        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n","                               padding=1, groups=groups, bias=False)\n","        self.bn2 = nn.BatchNorm2d(width)\n","        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","\n","class SCSEResNeXtBottleneck(Bottleneck):\n","    \"\"\"\n","    ResNeXt bottleneck type C with a Concurrent Spatial Squeeze-and-Excitation module.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None, base_width=4, final=False):\n","        super(SCSEResNeXtBottleneck, self).__init__()\n","        width = math.floor(planes * (base_width / 64)) * groups\n","        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n","                               stride=1)\n","        self.bn1 = nn.BatchNorm2d(width)\n","        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n","                               padding=1, groups=groups, bias=False)\n","        self.bn2 = nn.BatchNorm2d(width)\n","        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SCSEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SENet(nn.Module):\n","\n","    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n","                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n","                 downsample_padding=1, num_classes=1000):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        block (nn.Module): Bottleneck class.\n","            - For SENet154: SEBottleneck\n","            - For SE-ResNet models: SEResNetBottleneck\n","            - For SE-ResNeXt models:  SEResNeXtBottleneck\n","        layers (list of ints): Number of residual blocks for 4 layers of the\n","            network (layer1...layer4).\n","        groups (int): Number of groups for the 3x3 convolution in each\n","            bottleneck block.\n","            - For SENet154: 64\n","            - For SE-ResNet models: 1\n","            - For SE-ResNeXt models:  32\n","        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n","            - For all models: 16\n","        dropout_p (float or None): Drop probability for the Dropout layer.\n","            If `None` the Dropout layer is not used.\n","            - For SENet154: 0.2\n","            - For SE-ResNet models: None\n","            - For SE-ResNeXt models: None\n","        inplanes (int):  Number of input channels for layer1.\n","            - For SENet154: 128\n","            - For SE-ResNet models: 64\n","            - For SE-ResNeXt models: 64\n","        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n","            a single 7x7 convolution in layer0.\n","            - For SENet154: True\n","            - For SE-ResNet models: False\n","            - For SE-ResNeXt models: False\n","        downsample_kernel_size (int): Kernel size for downsampling convolutions\n","            in layer2, layer3 and layer4.\n","            - For SENet154: 3\n","            - For SE-ResNet models: 1\n","            - For SE-ResNeXt models: 1\n","        downsample_padding (int): Padding for downsampling convolutions in\n","            layer2, layer3 and layer4.\n","            - For SENet154: 1\n","            - For SE-ResNet models: 0\n","            - For SE-ResNeXt models: 0\n","        num_classes (int): Number of outputs in `last_linear` layer.\n","            - For all models: 1000\n","        \"\"\"\n","        super(SENet, self).__init__()\n","        self.inplanes = inplanes\n","        if input_3x3:\n","            layer0_modules = [\n","                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n","                                    bias=False)),\n","                ('bn1', nn.BatchNorm2d(64)),\n","                ('relu1', nn.ReLU(inplace=True)),\n","                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n","                                    bias=False)),\n","                ('bn2', nn.BatchNorm2d(64)),\n","                ('relu2', nn.ReLU(inplace=True)),\n","                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n","                                    bias=False)),\n","                ('bn3', nn.BatchNorm2d(inplanes)),\n","                ('relu3', nn.ReLU(inplace=True)),\n","            ]\n","        else:\n","            layer0_modules = [\n","                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n","                                    padding=3, bias=False)),\n","                ('bn1', nn.BatchNorm2d(inplanes)),\n","                ('relu1', nn.ReLU(inplace=True)),\n","            ]\n","        # To preserve compatibility with Caffe weights `ceil_mode=True`\n","        # is used instead of `padding=1`.\n","        self.pool = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n","        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n","        self.layer1 = self._make_layer(\n","            block,\n","            planes=64,\n","            blocks=layers[0],\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=1,\n","            downsample_padding=0\n","        )\n","        self.layer2 = self._make_layer(\n","            block,\n","            planes=128,\n","            blocks=layers[1],\n","            stride=2,\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=downsample_kernel_size,\n","            downsample_padding=downsample_padding\n","        )\n","        self.layer3 = self._make_layer(\n","            block,\n","            planes=256,\n","            blocks=layers[2],\n","            stride=2,\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=downsample_kernel_size,\n","            downsample_padding=downsample_padding\n","        )\n","        self.layer4 = self._make_layer(\n","            block,\n","            planes=512,\n","            blocks=layers[3],\n","            stride=2,\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=downsample_kernel_size,\n","            downsample_padding=downsample_padding\n","        )\n","        self.avg_pool = nn.AvgPool2d(7, stride=1)\n","        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n","        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n","        self._initialize_weights()\n","\n","    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n","                    downsample_kernel_size=1, downsample_padding=0):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=downsample_kernel_size, stride=stride,\n","                          padding=downsample_padding, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n","                            downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, groups, reduction))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","                m.weight.data = nn.init.kaiming_normal_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def features(self, x):\n","        x = self.layer0(x)\n","        x = self.pool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        return x\n","\n","    def logits(self, x):\n","        x = self.avg_pool(x)\n","        if self.dropout is not None:\n","            x = self.dropout(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.last_linear(x)\n","        return x\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.logits(x)\n","        return x\n","\n","\n","def initialize_pretrained_model(model, num_classes, settings):\n","    assert num_classes == settings['num_classes'], \\\n","        'num_classes should be {}, but is {}'.format(\n","            settings['num_classes'], num_classes)\n","    model.load_state_dict(model_zoo.load_url(settings['url']), strict=False)\n","    model.input_space = settings['input_space']\n","    model.input_size = settings['input_size']\n","    model.input_range = settings['input_range']\n","    model.mean = settings['mean']\n","    model.std = settings['std']\n","\n","\n","def senet154(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n","                  dropout_p=0.2, num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['senet154'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","def scsenet154(num_classes=1000, pretrained='imagenet'):\n","    print(\"scsenet154\")\n","    model = SENet(SCSEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n","                  dropout_p=0.2, num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['senet154'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnet50(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnet50'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnet101(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnet101'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnet152(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnet152'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def scse_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SCSEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"YEmiREpPoCWI","executionInfo":{"status":"ok","timestamp":1695246988681,"user_tz":180,"elapsed":3,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["class SeResNext50_Unet_2Ssum(nn.Module):\n","    def __init__(self, pretrained='imagenet', **kwargs):\n","        super(SeResNext50_Unet_2Ssum, self).__init__()\n","\n","        encoder_filters = [64, 256, 512, 1024, 2048]\n","        decoder_filters = np.asarray([64, 96, 128, 256, 512]) // 2\n","\n","        self.conv6 = ConvRelu(encoder_filters[-1], decoder_filters[-1])\n","        self.conv6_2 = ConvRelu(decoder_filters[-1] + encoder_filters[-2], decoder_filters[-1])\n","        self.conv7 = ConvRelu(decoder_filters[-1], decoder_filters[-2])\n","        self.conv7_2 = ConvRelu(decoder_filters[-2] + encoder_filters[-3] , decoder_filters[-2] )\n","        self.conv8 = ConvRelu(decoder_filters[-2], decoder_filters[-3])\n","        self.conv8_2 = ConvRelu(decoder_filters[-3] + encoder_filters[-4] , decoder_filters[-3])\n","        self.conv9 = ConvRelu(decoder_filters[-3], decoder_filters[-4])\n","        self.conv9_2 = ConvRelu(decoder_filters[-4] + encoder_filters[-5], decoder_filters[-4])\n","\n","        # self.convx9_3 = ConvRelu(encoder_filters[-4], encoder_filters[-4])\n","\n","\n","        self.conv10 = ConvRelu(decoder_filters[-4], decoder_filters[-5])\n","        self.conv10_s = nn.Sequential(ConvRelu(decoder_filters[-5], decoder_filters[-5]),\n","                                      nn.Conv2d(decoder_filters[-5] , 1, 1, stride=1, padding=0),\n","                                      nn.Sigmoid())\n","        # self.convxx = nn.Sequential(ConvRelu(decoder_filters[-5]*2, decoder_filters[-5]*2),\n","        #                             nn.Conv2d(decoder_filters[-5] * 2, decoder_filters[-5], 1, stride=1, padding=0))\n","\n","        self.res = nn.Conv2d(decoder_filters[-5] * 2, 5, 1, stride=1, padding=0)\n","\n","        self._initialize_weights()\n","\n","        encoder = se_resnext50_32x4d(pretrained=pretrained)\n","        #encoder = torchvision.models.resnet50(pretrained=pretrained)\n","        self.conv1 = nn.Sequential(encoder.layer0.conv1, encoder.layer0.bn1, encoder.layer0.relu1) #encoder.layer0.conv1\n","        self.conv2 = nn.Sequential(encoder.pool, encoder.layer1)\n","        self.conv3 = encoder.layer2\n","        self.conv4 = encoder.layer3\n","        self.conv5 = encoder.layer4\n","\n","\n","    def forward1(self, x):\n","        batch_size, C, H, W = x.shape\n","        xx = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n","\n","        enc1 = self.conv1(x)\n","        enc2 = self.conv2(enc1)\n","        enc3 = self.conv3(enc2)\n","        enc4 = self.conv4(enc3)\n","        enc5 = self.conv5(enc4)\n","\n","        encx1 = self.conv1(xx)   # 64 128 128\n","        encx2 = self.conv2(encx1) # 64\n","        encx3 = self.conv3(encx2) # 32\n","        encx4 = self.conv4(encx3) # 16\n","        encx5 = self.conv5(encx4) # 8\n","\n","        dec6 = self.conv6(F.interpolate(enc5, scale_factor=2))\n","        dec6 = self.conv6_2(torch.cat([dec6, enc4 ], 1))\n","        dec7 = self.conv7(F.interpolate(dec6, scale_factor=2))\n","        dec7 = self.conv7_2(torch.cat([dec7, enc3], 1))\n","        dec8 = self.conv8(F.interpolate(dec7, scale_factor=2))\n","        dec8 = self.conv8_2(torch.cat([dec8, enc2], 1))\n","        dec9 = self.conv9(F.interpolate(dec8, scale_factor=2))\n","        dec9 = self.conv9_2(torch.cat([dec9,  enc1], 1))   #256\n","\n","        decx6 = self.conv6(F.interpolate(encx5, scale_factor=2))\n","        decx6 = self.conv6_2(torch.cat([decx6, encx4 ], 1))\n","        decx7 = self.conv7(F.interpolate(decx6, scale_factor=2))\n","        decx7 = self.conv7_2(torch.cat([decx7, encx3], 1))\n","        decx8 = self.conv8(F.interpolate(decx7, scale_factor=2))\n","        decx8 = self.conv8_2(torch.cat([decx8, encx2], 1))\n","        decx9 = self.conv9(F.interpolate(decx8, scale_factor=2))\n","        decx9 = self.conv9_2(torch.cat([decx9,  encx1], 1))   #128\n","        #decx9 = self.convx9_3(F.interpolate(decx9, scale_factor=4))\n","\n","\n","        dec10 = self.conv10(F.interpolate(dec9, scale_factor=2))\n","        alpha = self.conv10_s(dec10)\n","        decx10 = self.conv10(F.interpolate(decx9, scale_factor=4))\n","\n","        dec = alpha * dec10 + (1-alpha)*decx10\n","\n","        return dec\n","\n","    def forward(self, x):\n","        dec10_0 = self.forward1(x[:, :3, :, :])\n","        dec10_1 = self.forward1(x[:, 3:, :, :])\n","\n","        dec10 = torch.cat([dec10_0, dec10_1], 1)\n","\n","        return self.res(dec10)\n","\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n","                m.weight.data = nn.init.kaiming_normal_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"6LDy5nU9FYrM","executionInfo":{"status":"ok","timestamp":1695246988681,"user_tz":180,"elapsed":3,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","def create_square_kernel(size):\n","    return torch.ones((1, 1, size, size))\n","\n","def create_circle_kernel(size):\n","    y, x = np.ogrid[-(size-1)//2:(size-1)//2+1, -(size-1)//2:(size-1)//2+1]\n","    mask = x*x + y*y <= ((size-1)//2)**2\n","    return torch.Tensor(mask.astype(float)).view(1, 1, size, size)\n","\n","class MorphologicalLayer(nn.Module):\n","    def __init__(self, in_channels, kernel_size=3, shape='square', morph_op='erosion'):\n","        super(MorphologicalLayer, self).__init__()\n","        self.kernel_size = kernel_size\n","        self.padding = kernel_size // 2\n","        self.morph_op = morph_op\n","\n","        if shape == 'square':\n","            self.kernel = create_square_kernel(kernel_size)\n","        elif shape == 'circle':\n","            self.kernel = create_circle_kernel(kernel_size)\n","        else:\n","            raise ValueError(\"Invalid shape\")\n","\n","        # Repita o kernel para que ele tenha o mesmo nmero de canais que a entrada\n","        self.kernel = self.kernel.repeat(in_channels, 1, 1, 1)\n","\n","        self.kernel = nn.Parameter(self.kernel, requires_grad=False)\n","\n","    def forward(self, x):\n","        if self.morph_op == 'erosion':\n","            return F.conv2d(-x, self.kernel, padding=self.padding, groups=x.size(1)) * -1\n","        elif self.morph_op == 'dilation':\n","            return F.conv2d(x, self.kernel, padding=self.padding, groups=x.size(1))\n","        elif self.morph_op == 'opening':\n","            return F.conv2d(x, self.kernel, padding=self.padding, groups=x.size(1)) * -1\n","        elif self.morph_op == 'closing':\n","            return F.conv2d(-x, self.kernel, padding=self.padding, groups=x.size(1))\n","        else:\n","            raise ValueError(\"Invalid morphological operation\")"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"qVNa9uDwFOx8","executionInfo":{"status":"ok","timestamp":1695246988681,"user_tz":180,"elapsed":3,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["# Testar operaes acontecendo ao mesmo tempo\n","# Variar tamanho das janelas\n","# Mudar elemento estruturante (quadrado ou circulo)\n","\n","# Testar randon search\n","# Fazer testes curtos (Com 200 ou 100 imagens)"]},{"cell_type":"markdown","metadata":{"id":"tHCAM2vfRK7e"},"source":["## LoadData"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":161570,"status":"ok","timestamp":1695214694742,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"Un2B5Ov0F8f-","outputId":"397780ae-23d0-4b19-b34e-cf70b9d5d2d4"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|| 2/2 [02:26<00:00, 73.22s/it]\n","100%|| 1/1 [00:13<00:00, 13.43s/it]\n"]}],"source":["import os\n","os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n","os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n","os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n","\n","from os import path, makedirs, listdir\n","import sys\n","import numpy as np\n","np.random.seed(1)\n","import random\n","random.seed(1)\n","\n","import torch\n","from torch import nn\n","from torch.backends import cudnn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.optim.lr_scheduler as lr_scheduler\n","\n","#from apex import amp\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import timeit\n","import cv2\n","\n","from imgaug import augmenters as iaa\n","\n","from skimage.morphology import square, dilation\n","\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import accuracy_score\n","\n","import gc\n","\n","cv2.setNumThreads(0)\n","cv2.ocl.setUseOpenCL(False)\n","\n","train_dirs = ['/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/tier3','/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/train']\n","\n","val_dirs = ['/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/test']\n","\n","models_folder = '/content/drive/MyDrive/Modeling Satelities Images Building Damaged/src/train/weights'\n","\n","loc_folder = 'pred_loc_val'\n","\n","input_shape = (512, 512)\n","\n","\n","all_files = []\n","for d in tqdm(train_dirs):\n","    for f in sorted(listdir(path.join(d, 'images'))):\n","        if '_pre_disaster.png' in f:\n","            post_disaster_file = f.replace('_pre_disaster.png', '_post_disaster.png')\n","            if path.exists(path.join(d, 'images', post_disaster_file)):\n","                all_files.append(path.join(d, 'images', f))\n","\n","all_files2 = []\n","for d in tqdm(val_dirs):\n","    for f in sorted(listdir(path.join(d, 'images'))):\n","        if '_pre_disaster.png' in f:\n","            post_disaster_file = f.replace('_pre_disaster.png', '_post_disaster.png')\n","            if path.exists(path.join(d, 'images', post_disaster_file)):\n","                all_files2.append(path.join(d, 'images', f))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_V8N6ykQ2g0"},"outputs":[],"source":["def rand_bbox(size, lam):\n","    W = size[0]\n","    H = size[1]\n","    cut_rat = np.sqrt(1. - lam)\n","    cut_w = int(W * cut_rat)\n","    cut_h = int(H * cut_rat)\n","    # if cut_rat > 0.9:\n","    #   cut_w = np.int(W * cut_rat * 0.9)\n","    #   cut_h = np.int(H * cut_rat * 0.9)\n","\n","    # uniform\n","    cx = np.random.randint(W)\n","    cy = np.random.randint(H)\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n","\n","    return bbx1, bby1, bbx2, bby2\n","\n","class TrainData(Dataset):\n","    def __init__(self, train_idxs, low, high):\n","        super().__init__()\n","        self.train_idxs = train_idxs\n","        self.elastic = iaa.ElasticTransformation(alpha=(0.25, 1.2), sigma=0.2)\n","        self.low =low\n","        self.high = high\n","\n","    def __len__(self):\n","        return len(self.train_idxs)\n","\n","    def __getitem__(self, idx):\n","        _idx = self.train_idxs[idx]\n","\n","        fn = all_files[_idx]\n","\n","        img = cv2.imread(fn, cv2.IMREAD_COLOR)\n","        img2 = cv2.imread(fn.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","        if img is None and img2 is None:\n","          sample = {'img': None, 'msk': None, 'lbl_msk': None, 'fn': None}\n","        elif img is None or img2 is None:\n","          sample = {'img': None, 'msk': None, 'lbl_msk': None, 'fn': None}\n","\n","        msk0 = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        lbl_msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        msk1 = np.zeros_like(lbl_msk1)\n","        msk2 = np.zeros_like(lbl_msk1)\n","        msk3 = np.zeros_like(lbl_msk1)\n","        msk4 = np.zeros_like(lbl_msk1)\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","        msk1[lbl_msk1 == 1] = 255\n","\n","        try:\n","          if random.random() > 0.87:\n","              lam = np.random.beta(2, 1.8)\n","              rand_inx = torch.randint(low=self.low,high=self.high,size=(1,))\n","              ttt = self.train_idxs[rand_inx]\n","              fn_rand = all_files[ttt]\n","              img_random = cv2.imread(fn_rand, cv2.IMREAD_COLOR)\n","              img2_random = cv2.imread(fn_rand.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","              msk0_random = cv2.imread(fn_rand.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","              lbl_msk1_random = cv2.imread(fn_rand.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","              bbx1, bby1, bbx2, bby2 = rand_bbox((1024, 1024), lam)\n","              img[bbx1:bbx2, bby1:bby2, :] = img_random[bbx1:bbx2, bby1:bby2, :]\n","              img2[bbx1:bbx2, bby1:bby2, :] = img2_random[bbx1:bbx2, bby1:bby2, :]\n","              msk0[bbx1:bbx2, bby1:bby2] = msk0_random[bbx1:bbx2, bby1:bby2]\n","              lbl_msk1[bbx1:bbx2, bby1:bby2] = lbl_msk1_random[bbx1:bbx2, bby1:bby2]\n","              # lbl_msk1[lbl_msk1==1]=70\n","              # lbl_msk1[lbl_msk1==2]=130\n","              # lbl_msk1[lbl_msk1==3]=190\n","              # lbl_msk1[lbl_msk1==4]=255\n","              # cv2.imshow('input_image', lbl_msk1)\n","              # cv2.waitKey(5000)\n","        except:\n","          None\n","\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","        msk1[lbl_msk1 == 1] = 255\n","\n","        try:\n","          if random.random() > 0.5:\n","              img = img[::-1, ...]\n","              img2 = img2[::-1, ...]\n","              msk0 = msk0[::-1, ...]\n","              msk1 = msk1[::-1, ...]\n","              msk2 = msk2[::-1, ...]\n","              msk3 = msk3[::-1, ...]\n","              msk4 = msk4[::-1, ...]\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.05:\n","              rot = random.randrange(4)\n","              if rot > 0:\n","                img = np.rot90(img, k=rot)\n","                img2 = np.rot90(img2, k=rot)\n","                msk0 = np.rot90(msk0, k=rot)\n","                msk1 = np.rot90(msk1, k=rot)\n","                msk2 = np.rot90(msk2, k=rot)\n","                msk3 = np.rot90(msk3, k=rot)\n","                msk4 = np.rot90(msk4, k=rot)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.8:\n","              shift_pnt = (random.randint(-320, 320), random.randint(-320, 320))\n","              img = shift_image(img, shift_pnt)\n","              img2 = shift_image(img2, shift_pnt)\n","              msk0 = shift_image(msk0, shift_pnt)\n","              msk1 = shift_image(msk1, shift_pnt)\n","              msk2 = shift_image(msk2, shift_pnt)\n","              msk3 = shift_image(msk3, shift_pnt)\n","              msk4 = shift_image(msk4, shift_pnt)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.2:\n","              rot_pnt =  (img.shape[0] // 2 + random.randint(-320, 320), img.shape[1] // 2 + random.randint(-320, 320))\n","              scale = 0.9 + random.random() * 0.2\n","              angle = random.randint(0, 20) - 10\n","              if (angle != 0) or (scale != 1):\n","                  img = rotate_image(img, angle, scale, rot_pnt)\n","                  img2 = rotate_image(img2, angle, scale, rot_pnt)\n","                  msk0 = rotate_image(msk0, angle, scale, rot_pnt)\n","                  msk1 = rotate_image(msk1, angle, scale, rot_pnt)\n","                  msk2 = rotate_image(msk2, angle, scale, rot_pnt)\n","                  msk3 = rotate_image(msk3, angle, scale, rot_pnt)\n","                  msk4 = rotate_image(msk4, angle, scale, rot_pnt)\n","        except:\n","          None\n","\n","        crop_size = input_shape[0]\n","\n","        try:\n","          if random.random() > 0.1:\n","              crop_size = random.randint(int(input_shape[0] / 1.15), int(input_shape[0] / 0.85))\n","        except:\n","          None\n","\n","        try:\n","          bst_x0 = random.randint(0, img.shape[1] - crop_size)\n","          bst_y0 = random.randint(0, img.shape[0] - crop_size)\n","          bst_sc = -1\n","          try_cnt = random.randint(1, 10)\n","          for i in range(try_cnt):\n","              x0 = random.randint(0, img.shape[1] - crop_size)\n","              y0 = random.randint(0, img.shape[0] - crop_size)\n","              _sc = msk2[y0:y0+crop_size, x0:x0+crop_size].sum() * 5 + msk3[y0:y0+crop_size, x0:x0+crop_size].sum() * 5 + msk4[y0:y0+crop_size, x0:x0+crop_size].sum() * 2 + msk1[y0:y0+crop_size, x0:x0+crop_size].sum()\n","              if _sc > bst_sc:\n","                  bst_sc = _sc\n","                  bst_x0 = x0\n","                  bst_y0 = y0\n","          x0 = bst_x0\n","          y0 = bst_y0\n","          img = img[y0:y0+crop_size, x0:x0+crop_size, :]\n","          img2 = img2[y0:y0+crop_size, x0:x0+crop_size, :]\n","          msk0 = msk0[y0:y0+crop_size, x0:x0+crop_size]\n","          msk1 = msk1[y0:y0+crop_size, x0:x0+crop_size]\n","          msk2 = msk2[y0:y0+crop_size, x0:x0+crop_size]\n","          msk3 = msk3[y0:y0+crop_size, x0:x0+crop_size]\n","          msk4 = msk4[y0:y0+crop_size, x0:x0+crop_size]\n","\n","          if crop_size != input_shape[0]:\n","            img = cv2.resize(img, input_shape, interpolation=cv2.INTER_LINEAR)\n","            img2 = cv2.resize(img2, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk0 = cv2.resize(msk0, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk1 = cv2.resize(msk1, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk2 = cv2.resize(msk2, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk3 = cv2.resize(msk3, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk4 = cv2.resize(msk4, input_shape, interpolation=cv2.INTER_LINEAR)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.96:\n","              img = shift_channels(img, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","          elif random.random() > 0.96:\n","              img2 = shift_channels(img2, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","\n","          if random.random() > 0.96:\n","              img = change_hsv(img, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","          elif random.random() > 0.96:\n","              img2 = change_hsv(img2, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img = clahe(img)\n","              elif random.random() > 0.96:\n","                  img = gauss_noise(img)\n","              elif random.random() > 0.96:\n","                  img = cv2.blur(img, (3, 3))\n","          elif random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img = saturation(img, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img = brightness(img, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img = contrast(img, 0.9 + random.random() * 0.2)\n","\n","          if random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img2 = clahe(img2)\n","              elif random.random() > 0.96:\n","                  img2 = gauss_noise(img2)\n","              elif random.random() > 0.96:\n","                  img2 = cv2.blur(img2, (3, 3))\n","          elif random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img2 = saturation(img2, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img2 = brightness(img2, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img2 = contrast(img2, 0.9 + random.random() * 0.2)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.96:\n","              el_det = self.elastic.to_deterministic()\n","              img = el_det.augment_image(img)\n","\n","          if random.random() > 0.96:\n","              el_det = self.elastic.to_deterministic()\n","              img2 = el_det.augment_image(img2)\n","        except:\n","          None\n","\n","        msk0 = msk0[..., np.newaxis]\n","        msk1 = msk1[..., np.newaxis]\n","        msk2 = msk2[..., np.newaxis]\n","        msk3 = msk3[..., np.newaxis]\n","        msk4 = msk4[..., np.newaxis]\n","\n","        msk = np.concatenate([msk0, msk1, msk2, msk3, msk4], axis=2)\n","        msk = (msk > 127)\n","\n","        msk[..., 0] = True\n","        msk[..., 1] = dilation(msk[..., 1], square(5))\n","        msk[..., 2] = dilation(msk[..., 2], square(5))\n","        msk[..., 3] = dilation(msk[..., 3], square(5))\n","        msk[..., 4] = dilation(msk[..., 4], square(5))\n","        msk[..., 1][msk[..., 2:].max(axis=2)] = False\n","        msk[..., 3][msk[..., 2]] = False\n","        msk[..., 4][msk[..., 2]] = False\n","        msk[..., 4][msk[..., 3]] = False\n","        msk[..., 0][msk[..., 1:].max(axis=2)] = False\n","        msk = msk * 1\n","\n","        lbl_msk = msk.argmax(axis=2)\n","\n","        img = np.concatenate([img, img2], axis=2)\n","        img = preprocess_inputs(img)\n","\n","        img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n","        msk = torch.from_numpy(msk.transpose((2, 0, 1))).long()\n","\n","        sample = {'img': img, 'msk': msk, 'lbl_msk': lbl_msk, 'fn': fn}\n","        return sample\n","\n","\n","class ValData(Dataset):\n","    def __init__(self, image_idxs):\n","        super().__init__()\n","        self.image_idxs = image_idxs\n","\n","    def __len__(self):\n","        return len(self.image_idxs)\n","\n","    def __getitem__(self, idx):\n","        _idx = self.image_idxs[idx]\n","\n","        fn = all_files2[_idx]\n","\n","        img = cv2.imread(fn, cv2.IMREAD_COLOR)\n","        img2 = cv2.imread(fn.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","        msk_loc = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)# > (0.3*255)\n","\n","        msk0 = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        lbl_msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        msk1 = np.zeros_like(lbl_msk1)\n","        msk2 = np.zeros_like(lbl_msk1)\n","        msk3 = np.zeros_like(lbl_msk1)\n","        msk4 = np.zeros_like(lbl_msk1)\n","        msk1[lbl_msk1 == 1] = 255\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","\n","        msk0 = msk0[..., np.newaxis]\n","        msk1 = msk1[..., np.newaxis]\n","        msk2 = msk2[..., np.newaxis]\n","        msk3 = msk3[..., np.newaxis]\n","        msk4 = msk4[..., np.newaxis]\n","\n","        msk = np.concatenate([msk0, msk1, msk2, msk3, msk4], axis=2)\n","        msk = (msk > 127)\n","\n","        msk = msk * 1\n","\n","        lbl_msk = msk[..., 1:].argmax(axis=2)\n","\n","        img = np.concatenate([img, img2], axis=2)\n","        img = preprocess_inputs(img)\n","\n","        img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n","        msk = torch.from_numpy(msk.transpose((2, 0, 1))).long()\n","\n","        sample = {'img': img, 'msk': msk, 'lbl_msk': lbl_msk, 'fn': fn, 'msk_loc': msk_loc}\n","        return sample\n","\n","\n","def validate(net, data_loader):\n","    dices0 = []\n","\n","    tp = np.zeros((5,))\n","    fp = np.zeros((5,))\n","    fn = np.zeros((5,))\n","\n","    _thr = 0.3\n","\n","    with torch.no_grad():\n","        for i, sample in enumerate(tqdm(data_loader)):\n","            msks = sample[\"msk\"].numpy()\n","            lbl_msk = sample[\"lbl_msk\"].numpy()\n","            imgs = sample[\"img\"].cuda(non_blocking=True)\n","            msk_loc = sample[\"msk_loc\"].numpy() * 1\n","            out = model(imgs)\n","\n","            msk_pred = msk_loc\n","            msk_damage_pred = torch.softmax(out, dim=1).cpu().numpy()[:, 1:, ...]\n","\n","            for j in range(msks.shape[0]):\n","                tp[4] += np.logical_and(msks[j, 0] > 0, msk_pred[j] > 0).sum()\n","                fn[4] += np.logical_and(msks[j, 0] < 1, msk_pred[j] > 0).sum()\n","                fp[4] += np.logical_and(msks[j, 0] > 0, msk_pred[j] < 1).sum()\n","\n","\n","                targ = lbl_msk[j][msks[j, 0] > 0]\n","                pred = msk_damage_pred[j].argmax(axis=0)\n","                pred = pred * (msk_pred[j] > _thr)\n","                pred = pred[msks[j, 0] > 0]\n","                for c in range(4):\n","                    tp[c] += np.logical_and(pred == c, targ == c).sum()\n","                    fn[c] += np.logical_and(pred != c, targ == c).sum()\n","                    fp[c] += np.logical_and(pred == c, targ != c).sum()\n","\n","    d0 = 2 * tp[4] / (2 * tp[4] + fp[4] + fn[4])\n","\n","    f1_sc = np.zeros((4,))\n","    for c in range(4):\n","        f1_sc[c] = 2 * tp[c] / (2 * tp[c] + fp[c] + fn[c])\n","\n","    f1 = 4 / np.sum(1.0 / (f1_sc + 1e-6))\n","\n","    sc = 0.3 * d0 + 0.7 * f1\n","    print(\"Val Score: {}, Dice: {}, F1: {}, F1_0: {}, F1_1: {}, F1_2: {}, F1_3: {}\".format(sc, d0, f1, f1_sc[0], f1_sc[1], f1_sc[2], f1_sc[3]))\n","    return [sc, d0, f1, f1_sc[0], f1_sc[1], f1_sc[2], f1_sc[3]]\n","\n","\n","def evaluate_val(data_val, best_score, model, snapshot_name, current_epoch):\n","    model = model.eval()\n","    d = validate(model, data_loader=data_val)\n","    score = d[0]\n","\n","    if score > best_score:\n","        torch.save({\n","            'epoch': current_epoch + 1,\n","            'state_dict': model.state_dict(),\n","            'best_score': d,\n","        }, path.join(models_folder, snapshot_name + '_best'))\n","        best_score = score\n","\n","    print(\"score: {}\\tscore_best: {}\".format(d, best_score))\n","    return d[0],d\n","\n","\n","def train_epoch(current_epoch, seg_loss, ce_loss, seg_seesaw, model, optimizer, scheduler, train_data_loader):\n","    losses = AverageMeter()\n","    losses1 = AverageMeter()\n","\n","    dices = AverageMeter()\n","\n","    iterator = tqdm(train_data_loader)\n","    model.train()\n","    for i, sample in enumerate(iterator):\n","        imgs = sample[\"img\"].cuda(non_blocking=True)\n","        msks = sample[\"msk\"].cuda(non_blocking=True)\n","        lbl_msk = sample[\"lbl_msk\"].cuda(non_blocking=True)\n","\n","        out = model(imgs)\n","\n","        # loss0 = seg_loss(out[:, 0, ...], msks[:, 0, ...])\n","        # loss1 = seg_loss(out[:, 1, ...], msks[:, 1, ...])\n","        # loss2 = seg_loss(out[:, 2, ...], msks[:, 2, ...])\n","        # loss3 = seg_loss(out[:, 3, ...], msks[:, 3, ...])\n","        # loss4 = seg_loss(out[:, 4, ...], msks[:, 4, ...])\n","\n","        loss5 = ce_loss(out, lbl_msk)\n","        #loss5 = seg_seesaw(out, lbl_msk)\n","        loss = loss5\n","        #loss = 0.1 * loss0 + 0.1 * loss1 + 0.3 * loss2 + 0.3 * loss3 + 0.2 * loss4 + loss5 * 2\n","\n","        with torch.no_grad():\n","            _probs = 1 - torch.sigmoid(out[:, 0, ...])\n","            dice_sc = 1 - dice_round(_probs, 1 - msks[:, 0, ...])\n","\n","        losses.update(loss.item(), imgs.size(0))\n","        losses1.update(loss5.item(), imgs.size(0))\n","\n","        dices.update(dice_sc, imgs.size(0))\n","\n","        iterator.set_description(\n","            \"epoch: {}; lr {:.7f}; Loss {loss.val:.4f} ({loss.avg:.4f}); cce_loss {loss1.val:.4f} ({loss1.avg:.4f}); Dice {dice.val:.4f} ({dice.avg:.4f})\".format(\n","                current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","\n","        optimizer.zero_grad()\n","        # loss.backward()\n","        with amp.scale_loss(loss, optimizer) as scaled_loss:\n","            scaled_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), 0.999)\n","        optimizer.step()\n","\n","    scheduler.step()\n","\n","    print(\"epoch: {}; lr {:.7f}; Loss {loss.avg:.4f}; CCE_loss {loss1.avg:.4f}; Dice {dice.avg:.4f}\".format(\n","            current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","    return scheduler.get_lr()[-1], losses, losses1, dices\n","\n","def train_epoch(current_epoch, seg_loss, ce_loss, seg_seesaw, model, optimizer, scheduler, train_data_loader):\n","    losses = AverageMeter()\n","    losses1 = AverageMeter()\n","\n","    dices = AverageMeter()\n","\n","    iterator = tqdm(train_data_loader)\n","    model.train()\n","    for i, sample in enumerate(iterator):\n","        if sample[\"img\"] is None or sample[\"msk\"] is None or sample[\"lbl_msk\"] is None:\n","              continue\n","        imgs = sample[\"img\"].cuda(non_blocking=True)\n","        msks = sample[\"msk\"].cuda(non_blocking=True)\n","        lbl_msk = sample[\"lbl_msk\"].cuda(non_blocking=True)\n","\n","        out = model(imgs)\n","\n","        loss5 = ce_loss(out, lbl_msk)\n","        loss = loss5\n","\n","        with torch.no_grad():\n","            _probs = 1 - torch.sigmoid(out[:, 0, ...])\n","            dice_sc = 1 - dice_round(_probs, 1 - msks[:, 0, ...])\n","\n","        losses.update(loss.item(), imgs.size(0))\n","        losses1.update(loss5.item(), imgs.size(0))\n","\n","        dices.update(dice_sc, imgs.size(0))\n","\n","        iterator.set_description(\n","            \"epoch: {}; lr {:.7f}; Loss {loss.val:.4f} ({loss.avg:.4f}); cce_loss {loss1.val:.4f} ({loss1.avg:.4f}); Dice {dice.val:.4f} ({dice.avg:.4f})\".format(\n","                current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.999)\n","        optimizer.step()\n","\n","    scheduler.step()\n","\n","    print(\"epoch: {}; lr {:.7f}; Loss {loss.avg:.4f}; CCE_loss {loss1.avg:.4f}; Dice {dice.avg:.4f}\".format(\n","            current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","    return scheduler.get_lr()[-1], losses, losses1, dices"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1695214694744,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"HaSdFhpqYnCi","outputId":"99a5a18b-7f2a-4a89-b810-d5bc1fd51b15"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                    images        disaster\n","0     joplin-tornado_00000000_pre_disaster  joplin-tornado\n","1     joplin-tornado_00000001_pre_disaster  joplin-tornado\n","2     joplin-tornado_00000002_pre_disaster  joplin-tornado\n","3     joplin-tornado_00000003_pre_disaster  joplin-tornado\n","4     joplin-tornado_00000004_pre_disaster  joplin-tornado\n","...                                    ...             ...\n","9157      socal-fire_00001396_pre_disaster      socal-fire\n","9158      socal-fire_00001397_pre_disaster      socal-fire\n","9159      socal-fire_00001398_pre_disaster      socal-fire\n","9160      socal-fire_00001399_pre_disaster      socal-fire\n","9161      socal-fire_00001402_pre_disaster      socal-fire\n","\n","[9162 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-46d7c538-c807-4f3b-8bec-3cfad80b2ac7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>images</th>\n","      <th>disaster</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>joplin-tornado_00000000_pre_disaster</td>\n","      <td>joplin-tornado</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>joplin-tornado_00000001_pre_disaster</td>\n","      <td>joplin-tornado</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>joplin-tornado_00000002_pre_disaster</td>\n","      <td>joplin-tornado</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>joplin-tornado_00000003_pre_disaster</td>\n","      <td>joplin-tornado</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>joplin-tornado_00000004_pre_disaster</td>\n","      <td>joplin-tornado</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9157</th>\n","      <td>socal-fire_00001396_pre_disaster</td>\n","      <td>socal-fire</td>\n","    </tr>\n","    <tr>\n","      <th>9158</th>\n","      <td>socal-fire_00001397_pre_disaster</td>\n","      <td>socal-fire</td>\n","    </tr>\n","    <tr>\n","      <th>9159</th>\n","      <td>socal-fire_00001398_pre_disaster</td>\n","      <td>socal-fire</td>\n","    </tr>\n","    <tr>\n","      <th>9160</th>\n","      <td>socal-fire_00001399_pre_disaster</td>\n","      <td>socal-fire</td>\n","    </tr>\n","    <tr>\n","      <th>9161</th>\n","      <td>socal-fire_00001402_pre_disaster</td>\n","      <td>socal-fire</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9162 rows  2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46d7c538-c807-4f3b-8bec-3cfad80b2ac7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-46d7c538-c807-4f3b-8bec-3cfad80b2ac7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-46d7c538-c807-4f3b-8bec-3cfad80b2ac7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-054f6bf5-b174-4672-a413-b0bc5e4e6a41\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-054f6bf5-b174-4672-a413-b0bc5e4e6a41')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-054f6bf5-b174-4672-a413-b0bc5e4e6a41 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":18}],"source":["list_images = pd.DataFrame(None)\n","list_images['images'] = [n.split('/')[-1].split('.')[0] for n in all_files]\n","list_images['disaster'] = [n.split('/')[-1].split('_')[0] for n in all_files]\n","list_images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1695214694744,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"_Hdd9c2GYtYf","outputId":"cdd7ef75-dcae-4fdd-e377-cd26c0a6a153"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                       images           disaster\n","6369  guatemala-volcano_00000010_pre_disaster  guatemala-volcano\n","6366  guatemala-volcano_00000006_pre_disaster  guatemala-volcano\n","6376  guatemala-volcano_00000023_pre_disaster  guatemala-volcano\n","6365  guatemala-volcano_00000002_pre_disaster  guatemala-volcano\n","6377  guatemala-volcano_00000024_pre_disaster  guatemala-volcano\n","...                                       ...                ...\n","5552      woolsey-fire_00000061_post_disaster       woolsey-fire\n","5695      woolsey-fire_00000204_post_disaster       woolsey-fire\n","6210      woolsey-fire_00000725_post_disaster       woolsey-fire\n","5775      woolsey-fire_00000284_post_disaster       woolsey-fire\n","5491      woolsey-fire_00000000_post_disaster       woolsey-fire\n","\n","[380 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-75364aa2-89bb-4edc-bdb2-979dd83a9311\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>images</th>\n","      <th>disaster</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6369</th>\n","      <td>guatemala-volcano_00000010_pre_disaster</td>\n","      <td>guatemala-volcano</td>\n","    </tr>\n","    <tr>\n","      <th>6366</th>\n","      <td>guatemala-volcano_00000006_pre_disaster</td>\n","      <td>guatemala-volcano</td>\n","    </tr>\n","    <tr>\n","      <th>6376</th>\n","      <td>guatemala-volcano_00000023_pre_disaster</td>\n","      <td>guatemala-volcano</td>\n","    </tr>\n","    <tr>\n","      <th>6365</th>\n","      <td>guatemala-volcano_00000002_pre_disaster</td>\n","      <td>guatemala-volcano</td>\n","    </tr>\n","    <tr>\n","      <th>6377</th>\n","      <td>guatemala-volcano_00000024_pre_disaster</td>\n","      <td>guatemala-volcano</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5552</th>\n","      <td>woolsey-fire_00000061_post_disaster</td>\n","      <td>woolsey-fire</td>\n","    </tr>\n","    <tr>\n","      <th>5695</th>\n","      <td>woolsey-fire_00000204_post_disaster</td>\n","      <td>woolsey-fire</td>\n","    </tr>\n","    <tr>\n","      <th>6210</th>\n","      <td>woolsey-fire_00000725_post_disaster</td>\n","      <td>woolsey-fire</td>\n","    </tr>\n","    <tr>\n","      <th>5775</th>\n","      <td>woolsey-fire_00000284_post_disaster</td>\n","      <td>woolsey-fire</td>\n","    </tr>\n","    <tr>\n","      <th>5491</th>\n","      <td>woolsey-fire_00000000_post_disaster</td>\n","      <td>woolsey-fire</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>380 rows  2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75364aa2-89bb-4edc-bdb2-979dd83a9311')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-75364aa2-89bb-4edc-bdb2-979dd83a9311 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-75364aa2-89bb-4edc-bdb2-979dd83a9311');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-68f856df-0ca4-42b9-b43f-cbfe4f23108b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-68f856df-0ca4-42b9-b43f-cbfe4f23108b')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-68f856df-0ca4-42b9-b43f-cbfe4f23108b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":19}],"source":["# Nmero de pares de imagens a serem selecionados de cada desastre\n","num_pairs_per_disaster = 10  # Isso dar 2 imagens (pr e ps) para cada desastre\n","\n","# Lista para armazenar os DataFrames filtrados\n","filtered_dfs = []\n","\n","# Agrupar por desastre e selecionar imagens\n","for name, group in list_images.groupby('disaster'):\n","    # Escolha o mnimo entre o nmero desejado de pares e o nmero disponvel de pares\n","    num_pairs = min(num_pairs_per_disaster, len(group))\n","    if num_pairs > 0:\n","        selected_pairs = group.sample(n=num_pairs)\n","        # Adicionar as imagens ps-desastre correspondentes\n","        post_disaster_images = selected_pairs['images'].str.replace('pre_disaster', 'post_disaster')\n","        selected_pairs = pd.concat([selected_pairs, pd.DataFrame({'images': post_disaster_images, 'disaster': name})])\n","        filtered_dfs.append(selected_pairs)\n","\n","# Concatenar todos os DataFrames filtrados para obter o DataFrame final\n","final_df = pd.concat(filtered_dfs)#.reset_index(drop=True)\n","final_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OiLvf-fPY38A"},"outputs":[],"source":["all_files = [all_files[i] for i in final_df.index]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":922214,"status":"ok","timestamp":1695215616944,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"AL3hRfYVR6t7","outputId":"4c654233-ff27-4c44-9f3c-47e854569941"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|| 380/380 [03:02<00:00,  2.08it/s]\n","100%|| 906/906 [12:19<00:00,  1.23it/s]\n","100%|| 906/906 [00:00<00:00, 1692667.89it/s]\n","100%|| 376/376 [00:00<00:00, 340470.27it/s]\n","100%|| 376/376 [00:00<00:00, 380014.05it/s]\n"]}],"source":["# ttt = np.asarray([True, False, True, True])\n","# p1=ttt[0:].max()\n","# p2=ttt[1].max()\n","# ttt2 = np.asarray([True, True, False, False])\n","# p3=ttt2[1].max()\n","# p4=ttt2[2:3].max()\n","t0 = timeit.default_timer()\n","\n","makedirs(models_folder, exist_ok=True)\n","\n","seed = 13\n","#seed=0\n","# vis_dev = sys.argv[2]\n","\n","# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = vis_dev\n","\n","cudnn.benchmark = True\n","\n","batch_size = 12\n","val_batch_size = 10\n","\n","snapshot_name = 'res50_cls_2Ssum_{}_0'.format(seed)\n","\n","file_classes = []\n","for fn in tqdm(all_files):\n","    fl = np.zeros((4,), dtype=bool)\n","    msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","    for c in range(1, 5):\n","        fl[c-1] = c in msk1\n","    file_classes.append(fl)\n","file_classes = np.asarray(file_classes)\n","\n","file_classes2 = []\n","for fn in tqdm(all_files2):\n","    fl = np.zeros((4,), dtype=bool)\n","    msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","    for c in range(1, 5):\n","        fl[c-1] = c in msk1\n","    file_classes2.append(fl)\n","file_classes2 = np.asarray(file_classes2)\n","\n","train_idxs0, val_idxs = train_test_split(np.arange(len(all_files)), test_size=0.01, random_state=seed)\n","\n","val_idxs0 = np.arange(len(all_files2))\n","\n","val_idxs = []\n","for i in tqdm(val_idxs0):\n","    val_idxs.append(i)\n","\n","np.random.seed(seed + 1234)\n","random.seed(seed + 1234)\n","\n","train_idxs = []\n","for i in tqdm(train_idxs0):\n","    train_idxs.append(i)\n","    if file_classes[i, 1:].max():\n","        train_idxs.append(i)\n","    # if file_classes[i, 2].max():\n","    #     train_idxs.append(i)\n","low1 = len(train_idxs)\n","for i in tqdm(train_idxs0):\n","    if file_classes[i, 1:3].max():\n","        train_idxs.append(i)\n","# for i in train_idxs0:\n","#     if file_classes[i, 1].max():\n","#         train_idxs.append(i)\n","high1 = len(train_idxs)\n","\n","train_idxs = np.asarray(train_idxs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68,"status":"ok","timestamp":1695215616945,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"ZxguzzcmqDBX","outputId":"5d9b62e8-8d6d-4ba1-ce14-82e949d4c420"},"outputs":[{"output_type":"stream","name":"stdout","text":["steps_per_epoch 57 validation_steps 90\n"]}],"source":["steps_per_epoch = int(len(train_idxs) // batch_size)\n","validation_steps = int(len(val_idxs) // val_batch_size)\n","\n","print('steps_per_epoch', steps_per_epoch, 'validation_steps', validation_steps)\n","\n","data_train = TrainData(train_idxs, low1, high1)\n","val_train = ValData(val_idxs)\n","\n","train_data_loader = DataLoader(data_train, batch_size=int(batch_size), num_workers=6, shuffle=True, pin_memory=False, drop_last=True)\n","val_data_loader = DataLoader(val_train, batch_size=int(val_batch_size), num_workers=6, shuffle=False, pin_memory=False)"]},{"cell_type":"markdown","metadata":{"id":"T-rqSugmZwEp"},"source":["## Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIavFyXpaPYX"},"outputs":[],"source":["from itertools import product\n","import random\n","\n","# Camadas candidatas para adicionar operaes morfolgicas.\n","candidate_layers = ['after_conv3', 'after_conv4', 'after_conv5']\n","\n","# Tipos de operaes morfolgicas para experimentar.\n","morph_ops = ['erosion', 'dilation', 'opening', 'closing']\n","\n","# Tamanhos de kernel para experimentar.\n","kernel_sizes = [3, 5, 7]\n","\n","# Tipos de elementos estruturantes.\n","shapes = ['square', 'circle']\n","\n","# Nmero de configuraes aleatrias para testar.\n","num_random_configs = 72\n","\n","# Gere um subconjunto aleatrio de todas as combinaes possveis.\n","random_combinations = []\n","for _ in range(num_random_configs):\n","    num_layers = random.randint(1, len(candidate_layers))  # Nmero de camadas morfolgicas para adicionar\n","    layers = random.sample(candidate_layers, num_layers)  # Escolha aleatria das camadas\n","    ops = random.choices(morph_ops, k=num_layers)  # Escolha aleatria das operaes\n","    k_sizes = random.choices(kernel_sizes, k=num_layers)  # Escolha aleatria dos tamanhos de kernel\n","    shape_types = random.choices(shapes, k=num_layers)  # Escolha aleatria dos tipos de elementos estruturantes\n","    random_combinations.append((layers, ops, k_sizes, shape_types))"]},{"cell_type":"code","source":["random_combinations"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SqalSq26lNLQ","executionInfo":{"status":"ok","timestamp":1695213217637,"user_tz":180,"elapsed":1277,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"04c63e53-c3d1-4f71-bfac-7d724d75bc89"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(['after_conv5'], ['opening'], [3], ['circle']),\n"," (['after_conv4'], ['dilation'], [3], ['circle']),\n"," (['after_conv3', 'after_conv4'],\n","  ['dilation', 'opening'],\n","  [7, 5],\n","  ['circle', 'square']),\n"," (['after_conv4', 'after_conv3', 'after_conv5'],\n","  ['erosion', 'dilation', 'dilation'],\n","  [5, 7, 7],\n","  ['square', 'circle', 'square']),\n"," (['after_conv3', 'after_conv4', 'after_conv5'],\n","  ['dilation', 'opening', 'dilation'],\n","  [3, 5, 5],\n","  ['circle', 'circle', 'square']),\n"," (['after_conv5'], ['dilation'], [7], ['circle']),\n"," (['after_conv3', 'after_conv5', 'after_conv4'],\n","  ['opening', 'closing', 'opening'],\n","  [7, 7, 5],\n","  ['square', 'circle', 'circle']),\n"," (['after_conv3', 'after_conv4'],\n","  ['opening', 'opening'],\n","  [3, 5],\n","  ['square', 'square']),\n"," (['after_conv5'], ['dilation'], [7], ['square']),\n"," (['after_conv4', 'after_conv5'],\n","  ['opening', 'closing'],\n","  [3, 7],\n","  ['circle', 'square']),\n"," (['after_conv4', 'after_conv3'],\n","  ['closing', 'erosion'],\n","  [5, 7],\n","  ['circle', 'square']),\n"," (['after_conv4', 'after_conv3'],\n","  ['dilation', 'closing'],\n","  [3, 5],\n","  ['circle', 'square']),\n"," (['after_conv4', 'after_conv5'],\n","  ['dilation', 'opening'],\n","  [5, 7],\n","  ['square', 'circle']),\n"," (['after_conv3'], ['dilation'], [7], ['square']),\n"," (['after_conv3', 'after_conv5'],\n","  ['dilation', 'closing'],\n","  [7, 3],\n","  ['circle', 'square']),\n"," (['after_conv3', 'after_conv4'],\n","  ['erosion', 'closing'],\n","  [3, 7],\n","  ['square', 'circle']),\n"," (['after_conv5', 'after_conv4'],\n","  ['opening', 'erosion'],\n","  [7, 3],\n","  ['square', 'circle']),\n"," (['after_conv5', 'after_conv3', 'after_conv4'],\n","  ['closing', 'opening', 'opening'],\n","  [5, 5, 7],\n","  ['circle', 'circle', 'circle']),\n"," (['after_conv4'], ['erosion'], [3], ['circle']),\n"," (['after_conv5'], ['opening'], [7], ['circle']),\n"," (['after_conv5', 'after_conv3'],\n","  ['closing', 'closing'],\n","  [7, 5],\n","  ['circle', 'circle']),\n"," (['after_conv4', 'after_conv3'],\n","  ['dilation', 'dilation'],\n","  [5, 7],\n","  ['circle', 'circle']),\n"," (['after_conv3', 'after_conv4', 'after_conv5'],\n","  ['erosion', 'closing', 'dilation'],\n","  [3, 3, 3],\n","  ['square', 'circle', 'square']),\n"," (['after_conv5', 'after_conv4'],\n","  ['closing', 'dilation'],\n","  [5, 7],\n","  ['circle', 'square']),\n"," (['after_conv3'], ['closing'], [7], ['circle']),\n"," (['after_conv5', 'after_conv4', 'after_conv3'],\n","  ['erosion', 'opening', 'opening'],\n","  [7, 5, 3],\n","  ['circle', 'square', 'square']),\n"," (['after_conv3', 'after_conv4'],\n","  ['dilation', 'dilation'],\n","  [3, 5],\n","  ['square', 'square']),\n"," (['after_conv3'], ['dilation'], [3], ['circle']),\n"," (['after_conv5', 'after_conv4', 'after_conv3'],\n","  ['erosion', 'dilation', 'opening'],\n","  [7, 5, 7],\n","  ['square', 'square', 'square']),\n"," (['after_conv5'], ['erosion'], [3], ['square']),\n"," (['after_conv5', 'after_conv3', 'after_conv4'],\n","  ['opening', 'closing', 'erosion'],\n","  [5, 7, 3],\n","  ['square', 'square', 'square']),\n"," (['after_conv3'], ['opening'], [7], ['square']),\n"," (['after_conv3'], ['opening'], [5], ['circle']),\n"," (['after_conv4'], ['closing'], [7], ['circle']),\n"," (['after_conv4', 'after_conv5', 'after_conv3'],\n","  ['erosion', 'closing', 'erosion'],\n","  [7, 7, 7],\n","  ['square', 'square', 'square']),\n"," (['after_conv3'], ['closing'], [5], ['square']),\n"," (['after_conv3', 'after_conv4'],\n","  ['closing', 'dilation'],\n","  [7, 5],\n","  ['square', 'circle']),\n"," (['after_conv5', 'after_conv4'],\n","  ['opening', 'erosion'],\n","  [3, 3],\n","  ['square', 'circle']),\n"," (['after_conv5', 'after_conv3', 'after_conv4'],\n","  ['erosion', 'closing', 'closing'],\n","  [3, 7, 3],\n","  ['square', 'circle', 'square']),\n"," (['after_conv5', 'after_conv3', 'after_conv4'],\n","  ['dilation', 'closing', 'opening'],\n","  [3, 3, 3],\n","  ['circle', 'circle', 'square']),\n"," (['after_conv5'], ['dilation'], [3], ['circle']),\n"," (['after_conv3', 'after_conv5'],\n","  ['closing', 'opening'],\n","  [7, 7],\n","  ['square', 'square']),\n"," (['after_conv5', 'after_conv3', 'after_conv4'],\n","  ['closing', 'opening', 'closing'],\n","  [5, 7, 3],\n","  ['square', 'circle', 'circle']),\n"," (['after_conv3'], ['opening'], [7], ['square']),\n"," (['after_conv3', 'after_conv4', 'after_conv5'],\n","  ['dilation', 'dilation', 'dilation'],\n","  [7, 5, 3],\n","  ['square', 'square', 'square']),\n"," (['after_conv5', 'after_conv3'],\n","  ['erosion', 'erosion'],\n","  [5, 3],\n","  ['square', 'circle']),\n"," (['after_conv5', 'after_conv4'],\n","  ['closing', 'closing'],\n","  [7, 7],\n","  ['square', 'square']),\n"," (['after_conv4', 'after_conv3'],\n","  ['erosion', 'closing'],\n","  [7, 3],\n","  ['circle', 'circle']),\n"," (['after_conv5', 'after_conv3', 'after_conv4'],\n","  ['erosion', 'dilation', 'dilation'],\n","  [7, 3, 7],\n","  ['square', 'circle', 'circle']),\n"," (['after_conv3', 'after_conv5'],\n","  ['opening', 'closing'],\n","  [7, 7],\n","  ['square', 'square']),\n"," (['after_conv3', 'after_conv4', 'after_conv5'],\n","  ['closing', 'opening', 'erosion'],\n","  [7, 3, 5],\n","  ['circle', 'circle', 'circle']),\n"," (['after_conv3', 'after_conv4', 'after_conv5'],\n","  ['closing', 'erosion', 'closing'],\n","  [5, 3, 7],\n","  ['circle', 'square', 'circle']),\n"," (['after_conv4', 'after_conv3', 'after_conv5'],\n","  ['opening', 'dilation', 'closing'],\n","  [5, 5, 5],\n","  ['circle', 'circle', 'square']),\n"," (['after_conv4', 'after_conv5', 'after_conv3'],\n","  ['closing', 'opening', 'dilation'],\n","  [3, 5, 3],\n","  ['square', 'square', 'square']),\n"," (['after_conv5'], ['closing'], [7], ['circle']),\n"," (['after_conv3', 'after_conv4', 'after_conv5'],\n","  ['erosion', 'closing', 'opening'],\n","  [7, 3, 7],\n","  ['circle', 'square', 'square']),\n"," (['after_conv5', 'after_conv3', 'after_conv4'],\n","  ['closing', 'erosion', 'dilation'],\n","  [3, 3, 5],\n","  ['square', 'circle', 'square']),\n"," (['after_conv3'], ['erosion'], [5], ['circle']),\n"," (['after_conv3', 'after_conv5'],\n","  ['erosion', 'closing'],\n","  [3, 7],\n","  ['circle', 'circle']),\n"," (['after_conv5', 'after_conv4'],\n","  ['erosion', 'opening'],\n","  [5, 7],\n","  ['circle', 'circle']),\n"," (['after_conv3', 'after_conv5', 'after_conv4'],\n","  ['closing', 'closing', 'dilation'],\n","  [5, 3, 5],\n","  ['circle', 'circle', 'circle']),\n"," (['after_conv5', 'after_conv4', 'after_conv3'],\n","  ['opening', 'closing', 'opening'],\n","  [7, 7, 5],\n","  ['square', 'square', 'circle']),\n"," (['after_conv5'], ['erosion'], [5], ['square']),\n"," (['after_conv3'], ['closing'], [3], ['circle']),\n"," (['after_conv3', 'after_conv5', 'after_conv4'],\n","  ['erosion', 'opening', 'opening'],\n","  [5, 7, 3],\n","  ['circle', 'square', 'circle']),\n"," (['after_conv5', 'after_conv4'],\n","  ['opening', 'erosion'],\n","  [5, 5],\n","  ['circle', 'circle']),\n"," (['after_conv4', 'after_conv3', 'after_conv5'],\n","  ['dilation', 'erosion', 'erosion'],\n","  [7, 5, 5],\n","  ['circle', 'circle', 'circle']),\n"," (['after_conv5', 'after_conv3', 'after_conv4'],\n","  ['dilation', 'closing', 'opening'],\n","  [3, 7, 5],\n","  ['square', 'circle', 'circle']),\n"," (['after_conv5'], ['opening'], [7], ['circle']),\n"," (['after_conv5'], ['erosion'], [3], ['square']),\n"," (['after_conv3'], ['erosion'], [7], ['square']),\n"," (['after_conv4'], ['erosion'], [5], ['circle'])]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["from itertools import product\n","\n","candidate_layers = ['after_conv4']\n","morph_ops = ['erosion', 'dilation', 'opening', 'closing']\n","kernel_sizes = [3, 5, 7]\n","shapes = ['square', 'circle']\n","\n","# Gerar todas as combinaes possveis\n","all_combinations = list(product(candidate_layers, morph_ops, kernel_sizes, shapes))\n","\n","# Para usar em mltiplas camadas, voc precisar fazer o produto cartesiano 'num_layers' vezes.\n","# Por exemplo, para duas camadas:\n","all_combinations_two_layers = list(product(all_combinations, repeat=2))\n","\n","# E assim por diante para mais camadas."],"metadata":{"id":"BlSKsZODncsn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from itertools import chain\n","\n","# Nmero mximo de camadas\n","max_layers = len(candidate_layers)\n","\n","# Armazenar todas as combinaes aqui\n","all_combinations_variable_layers = []\n","\n","# Loop sobre o nmero possvel de camadas\n","for num_layers in range(1, max_layers + 1):\n","    combinations_for_this_layer = list(product(all_combinations, repeat=num_layers))\n","    all_combinations_variable_layers.append(combinations_for_this_layer)\n","\n","# Achatar a lista\n","all_combinations_variable_layers = list(chain.from_iterable(all_combinations_variable_layers))"],"metadata":{"id":"pnVjJlzBmfdv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(all_combinations_variable_layers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K6Sjkk9pngfx","executionInfo":{"status":"ok","timestamp":1695215616947,"user_tz":180,"elapsed":21,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"a59063f4-06c3-4b07-c9e8-025438ee4da7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["24"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["print(\"Horas de treinamento: \",((len(all_combinations_variable_layers) * 15) / 60))\n","print(\"Custo em crditos: \",((len(all_combinations_variable_layers) * 15) / 60) * 13.5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-dhewxTEmgw7","executionInfo":{"status":"ok","timestamp":1695215616949,"user_tz":180,"elapsed":20,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"85152938-5f34-40f2-abb6-fd355c7db8ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Horas de treinamento:  6.0\n","Custo em crditos:  81.0\n"]}]},{"cell_type":"code","source":["random.shuffle(all_combinations_variable_layers)"],"metadata":{"id":"HhHJvuyGo-9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_combinations_variable_layers[0][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k8RG6wvYmy1N","executionInfo":{"status":"ok","timestamp":1695215616950,"user_tz":180,"elapsed":17,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"d6f61493-1d06-4d9b-fab4-537a59f7c37e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('after_conv4', 'closing', 5, 'square')"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ymFmmlD7cgBF"},"outputs":[],"source":["def get_morph_layer(morph_op, kernel_size, shape, in_channels):\n","    return MorphologicalLayer(in_channels=in_channels, kernel_size=kernel_size, shape=shape, morph_op=morph_op)\n","\n","def get_out_channels_from_conv(model, conv_name):\n","    conv_block = getattr(model, conv_name, None)\n","\n","    if conv_block is None:\n","        print(f\"{conv_name} not found in the model.\")\n","        return None\n","\n","    if not isinstance(conv_block, nn.Sequential):\n","        print(f\"{conv_name} is not an instance of nn.Sequential.\")\n","        return None\n","\n","    for layer in conv_block:\n","        if isinstance(layer, nn.Module) and hasattr(layer, 'conv3'):\n","            return layer.conv3.out_channels\n","\n","    return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z313YLyMn2XX"},"outputs":[],"source":["results = pd.DataFrame(None,columns=['epoch', 'lr', 'loss',\n","                                     'CCE_loss', 'dice_train',\n","                                     'layers', 'ops', 'k_sizes',\n","                                     'shape_types','val_score','dice_val',\n","                                     'f1_score','F1_0','F1_1','F1_2',\n","                                     'F1_3'])\n","#results.to_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_pre_train_grid.csv', index=False)\n","results = pd.read_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_pre_train_grid.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1695229199564,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"7aH5AfLSTxhT","outputId":"1fc13618-62d1-4c52-a5ba-7c9d6d3c2167"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   epoch        lr      loss  CCE_loss                       dice_train  \\\n","0      0  0.000202  0.720655  0.720655  tensor(0.1831, device='cuda:0')   \n","1      1  0.000202  0.330329  0.330329  tensor(0.2568, device='cuda:0')   \n","2      2  0.000202  0.301048  0.301048  tensor(0.3390, device='cuda:0')   \n","3      3  0.000202  0.270442  0.270442  tensor(0.4186, device='cuda:0')   \n","4      4  0.000051  0.237950  0.237950  tensor(0.4617, device='cuda:0')   \n","\n","        layers      ops  k_sizes shape_types val_score dice_val f1_score F1_0  \\\n","0  after_conv4  closing        5      square       NaN      NaN      NaN  NaN   \n","1  after_conv4  closing        5      square       NaN      NaN      NaN  NaN   \n","2  after_conv4  closing        5      square       NaN      NaN      NaN  NaN   \n","3  after_conv4  closing        5      square       NaN      NaN      NaN  NaN   \n","4  after_conv4  closing        5      square       NaN      NaN      NaN  NaN   \n","\n","  F1_1 F1_2 F1_3  \n","0  NaN  NaN  NaN  \n","1  NaN  NaN  NaN  \n","2  NaN  NaN  NaN  \n","3  NaN  NaN  NaN  \n","4  NaN  NaN  NaN  "],"text/html":["\n","  <div id=\"df-89520662-9ed0-44b1-ae29-cf7c5960f395\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch</th>\n","      <th>lr</th>\n","      <th>loss</th>\n","      <th>CCE_loss</th>\n","      <th>dice_train</th>\n","      <th>layers</th>\n","      <th>ops</th>\n","      <th>k_sizes</th>\n","      <th>shape_types</th>\n","      <th>val_score</th>\n","      <th>dice_val</th>\n","      <th>f1_score</th>\n","      <th>F1_0</th>\n","      <th>F1_1</th>\n","      <th>F1_2</th>\n","      <th>F1_3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.000202</td>\n","      <td>0.720655</td>\n","      <td>0.720655</td>\n","      <td>tensor(0.1831, device='cuda:0')</td>\n","      <td>after_conv4</td>\n","      <td>closing</td>\n","      <td>5</td>\n","      <td>square</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.000202</td>\n","      <td>0.330329</td>\n","      <td>0.330329</td>\n","      <td>tensor(0.2568, device='cuda:0')</td>\n","      <td>after_conv4</td>\n","      <td>closing</td>\n","      <td>5</td>\n","      <td>square</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.000202</td>\n","      <td>0.301048</td>\n","      <td>0.301048</td>\n","      <td>tensor(0.3390, device='cuda:0')</td>\n","      <td>after_conv4</td>\n","      <td>closing</td>\n","      <td>5</td>\n","      <td>square</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.000202</td>\n","      <td>0.270442</td>\n","      <td>0.270442</td>\n","      <td>tensor(0.4186, device='cuda:0')</td>\n","      <td>after_conv4</td>\n","      <td>closing</td>\n","      <td>5</td>\n","      <td>square</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.000051</td>\n","      <td>0.237950</td>\n","      <td>0.237950</td>\n","      <td>tensor(0.4617, device='cuda:0')</td>\n","      <td>after_conv4</td>\n","      <td>closing</td>\n","      <td>5</td>\n","      <td>square</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89520662-9ed0-44b1-ae29-cf7c5960f395')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-89520662-9ed0-44b1-ae29-cf7c5960f395 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-89520662-9ed0-44b1-ae29-cf7c5960f395');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-1c75d112-6d20-41aa-add4-5d0bf4792276\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1c75d112-6d20-41aa-add4-5d0bf4792276')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-1c75d112-6d20-41aa-add4-5d0bf4792276 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":64}],"source":["results.head()"]},{"cell_type":"code","source":["all_combinations_variable_layers[17:]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ub_NIEU9hk27","executionInfo":{"status":"ok","timestamp":1695229115024,"user_tz":180,"elapsed":650,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"ca1be702-5943-4460-d22b-090eba50843f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('after_conv4', 'erosion', 3, 'circle'),),\n"," (('after_conv4', 'opening', 3, 'square'),),\n"," (('after_conv4', 'opening', 5, 'square'),),\n"," (('after_conv4', 'opening', 7, 'circle'),),\n"," (('after_conv4', 'dilation', 7, 'square'),),\n"," (('after_conv4', 'erosion', 5, 'square'),),\n"," (('after_conv4', 'erosion', 5, 'circle'),)]"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["all_combinations_variable_layers = all_combinations_variable_layers[17:]"],"metadata":{"id":"tAWtB1Tgh31V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context"],"metadata":{"id":"Y_hROAKhk1-n","executionInfo":{"status":"ok","timestamp":1695247114864,"user_tz":180,"elapsed":7,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["model = SeResNext50_Unet_2Ssum(pretrained='imagenet').cuda()\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HGr1f_jpk4_9","executionInfo":{"status":"ok","timestamp":1695247427622,"user_tz":180,"elapsed":312286,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"8755036a-1d28-447f-e2c4-6640af38998e"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\" to /root/.cache/torch/hub/checkpoints/se_resnext50_32x4d-a260b3a4.pth\n","100%|| 105M/105M [05:10<00:00, 356kB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["SeResNext50_Unet_2Ssum(\n","  (conv6): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv6_2): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(1280, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv7): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv7_2): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(640, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv8): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv8_2): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv9): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(64, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv9_2): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(112, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv10): ConvRelu(\n","    (layer): Sequential(\n","      (0): Conv2d(48, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","    )\n","  )\n","  (conv10_s): Sequential(\n","    (0): ConvRelu(\n","      (layer): Sequential(\n","        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","      )\n","    )\n","    (1): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n","    (2): Sigmoid()\n","  )\n","  (res): Conv2d(64, 5, kernel_size=(1, 1), stride=(1, 1))\n","  (conv1): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (conv2): Sequential(\n","    (0): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n","    (1): Sequential(\n","      (0): SEResNeXtBottleneck(\n","        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (se_module): SEModule(\n","          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (sigmoid): Sigmoid()\n","        )\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): SEResNeXtBottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (se_module): SEModule(\n","          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (sigmoid): Sigmoid()\n","        )\n","      )\n","      (2): SEResNeXtBottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (se_module): SEModule(\n","          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (sigmoid): Sigmoid()\n","        )\n","      )\n","    )\n","  )\n","  (conv3): Sequential(\n","    (0): SEResNeXtBottleneck(\n","      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): SEResNeXtBottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (2): SEResNeXtBottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (3): SEResNeXtBottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","  )\n","  (conv4): Sequential(\n","    (0): SEResNeXtBottleneck(\n","      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): SEResNeXtBottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (2): SEResNeXtBottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (3): SEResNeXtBottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (4): SEResNeXtBottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (5): SEResNeXtBottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","  )\n","  (conv5): Sequential(\n","    (0): SEResNeXtBottleneck(\n","      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): SEResNeXtBottleneck(\n","      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","    (2): SEResNeXtBottleneck(\n","      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (se_module): SEModule(\n","        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","        (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","        (relu): ReLU(inplace=True)\n","        (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","        (sigmoid): Sigmoid()\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qFTYPcdBccNc","outputId":"e032eaf6-7330-44fc-cbfe-82b23b5ac32e","executionInfo":{"status":"ok","timestamp":1695234105869,"user_tz":180,"elapsed":1877517,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 0; lr 0.0002020; Loss 0.3465 (0.5473); cce_loss 0.3465 (0.5473); Dice 0.0687 (0.2153): 100%|| 57/57 [00:38<00:00,  1.48it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 0; lr 0.0002020; Loss 0.5473; CCE_loss 0.5473; Dice 0.2153\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 1; lr 0.0002020; Loss 0.1813 (0.2940); cce_loss 0.1813 (0.2940); Dice 0.4435 (0.3483): 100%|| 57/57 [00:38<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 1; lr 0.0002020; Loss 0.2940; CCE_loss 0.2940; Dice 0.3483\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 2; lr 0.0002020; Loss 0.3114 (0.2457); cce_loss 0.3114 (0.2457); Dice 0.5028 (0.4680): 100%|| 57/57 [00:37<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 2; lr 0.0002020; Loss 0.2457; CCE_loss 0.2457; Dice 0.4680\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 3; lr 0.0002020; Loss 0.2714 (0.2297); cce_loss 0.2714 (0.2297); Dice 0.4683 (0.4915): 100%|| 57/57 [00:38<00:00,  1.46it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 3; lr 0.0002020; Loss 0.2297; CCE_loss 0.2297; Dice 0.4915\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 4; lr 0.0002020; Loss 0.2838 (0.2056); cce_loss 0.2838 (0.2056); Dice 0.5837 (0.4833): 100%|| 57/57 [00:37<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 4; lr 0.0000505; Loss 0.2056; CCE_loss 0.2056; Dice 0.4833\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 5; lr 0.0000505; Loss 0.1004 (0.1844); cce_loss 0.1004 (0.1844); Dice 0.4499 (0.5074): 100%|| 57/57 [00:37<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 5; lr 0.0001010; Loss 0.1844; CCE_loss 0.1844; Dice 0.5074\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 6; lr 0.0001010; Loss 0.0866 (0.1725); cce_loss 0.0866 (0.1725); Dice 0.5798 (0.5151): 100%|| 57/57 [00:37<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 6; lr 0.0001010; Loss 0.1725; CCE_loss 0.1725; Dice 0.5151\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 7; lr 0.0001010; Loss 0.1462 (0.1531); cce_loss 0.1462 (0.1531); Dice 0.5540 (0.5034): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 7; lr 0.0001010; Loss 0.1531; CCE_loss 0.1531; Dice 0.5034\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|| 91/91 [01:44<00:00,  1.15s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Val Score: 0.6177367133255398, Dice: 1.0, F1: 0.4539095904650569, F1_0: 0.9116960780834492, F1_1: 0.2466615652860061, F1_2: 0.420145326028843, F1_3: 0.7805005699508901\n","score: [0.6177367133255398, 1.0, 0.4539095904650569, 0.9116960780834492, 0.2466615652860061, 0.420145326028843, 0.7805005699508901]\tscore_best: 0.6177367133255398\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 8; lr 0.0001010; Loss 0.1581 (0.1486); cce_loss 0.1581 (0.1486); Dice 0.5840 (0.5154): 100%|| 57/57 [00:37<00:00,  1.51it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 8; lr 0.0001010; Loss 0.1486; CCE_loss 0.1486; Dice 0.5154\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|| 91/91 [01:44<00:00,  1.15s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Val Score: 0.625319658873256, Dice: 1.0, F1: 0.46474236981893724, F1_0: 0.9059056316514046, F1_1: 0.29255280479143714, F1_2: 0.36576329779480565, F1_3: 0.7402589268195091\n","score: [0.625319658873256, 1.0, 0.46474236981893724, 0.9059056316514046, 0.29255280479143714, 0.36576329779480565, 0.7402589268195091]\tscore_best: 0.625319658873256\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 9; lr 0.0001010; Loss 0.4123 (0.1513); cce_loss 0.4123 (0.1513); Dice 0.4086 (0.4957): 100%|| 57/57 [00:38<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 9; lr 0.0001010; Loss 0.1513; CCE_loss 0.1513; Dice 0.4957\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|| 91/91 [01:44<00:00,  1.14s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Val Score: 0.6057764267670631, Dice: 1.0, F1: 0.4368234668100901, F1_0: 0.9030106526721275, F1_1: 0.29877821914649955, F1_2: 0.29588889588357753, F1_3: 0.7558435980833177\n","score: [0.6057764267670631, 1.0, 0.4368234668100901, 0.9030106526721275, 0.29877821914649955, 0.29588889588357753, 0.7558435980833177]\tscore_best: 0.625319658873256\n","Time: 11.626 min\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 0; lr 0.0002020; Loss 0.3057 (0.6965); cce_loss 0.3057 (0.6965); Dice 0.1204 (0.1284): 100%|| 57/57 [00:37<00:00,  1.51it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 0; lr 0.0002020; Loss 0.6965; CCE_loss 0.6965; Dice 0.1284\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 1; lr 0.0002020; Loss 0.3182 (0.2725); cce_loss 0.3182 (0.2725); Dice 0.2090 (0.1707): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 1; lr 0.0002020; Loss 0.2725; CCE_loss 0.2725; Dice 0.1707\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 2; lr 0.0002020; Loss 0.3480 (0.2527); cce_loss 0.3480 (0.2527); Dice 0.1987 (0.2448): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 2; lr 0.0002020; Loss 0.2527; CCE_loss 0.2527; Dice 0.2448\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 3; lr 0.0002020; Loss 0.3048 (0.2377); cce_loss 0.3048 (0.2377); Dice 0.2942 (0.2699): 100%|| 57/57 [00:37<00:00,  1.51it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 3; lr 0.0002020; Loss 0.2377; CCE_loss 0.2377; Dice 0.2699\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 4; lr 0.0002020; Loss 0.3410 (0.2124); cce_loss 0.3410 (0.2124); Dice 0.2979 (0.2960): 100%|| 57/57 [00:38<00:00,  1.47it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 4; lr 0.0000505; Loss 0.2124; CCE_loss 0.2124; Dice 0.2960\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 5; lr 0.0000505; Loss 0.1345 (0.2027); cce_loss 0.1345 (0.2027); Dice 0.3523 (0.2874): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 5; lr 0.0001010; Loss 0.2027; CCE_loss 0.2027; Dice 0.2874\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 6; lr 0.0001010; Loss 0.0975 (0.1809); cce_loss 0.0975 (0.1809); Dice 0.2008 (0.2891): 100%|| 57/57 [00:37<00:00,  1.53it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 6; lr 0.0001010; Loss 0.1809; CCE_loss 0.1809; Dice 0.2891\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 7; lr 0.0001010; Loss 0.2341 (0.1899); cce_loss 0.2341 (0.1899); Dice 0.2914 (0.2641): 100%|| 57/57 [00:37<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 7; lr 0.0001010; Loss 0.1899; CCE_loss 0.1899; Dice 0.2641\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|| 91/91 [01:44<00:00,  1.15s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Val Score: 0.3590056492552322, Dice: 1.0, F1: 0.08429378465033174, F1_0: 0.8960584143211354, F1_1: 0.03545274725499857, F1_2: 0.06014175406447303, F1_3: 0.6647971569465274\n","score: [0.3590056492552322, 1.0, 0.08429378465033174, 0.8960584143211354, 0.03545274725499857, 0.06014175406447303, 0.6647971569465274]\tscore_best: 0.3590056492552322\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 8; lr 0.0001010; Loss 0.1418 (0.1814); cce_loss 0.1418 (0.1814); Dice 0.3334 (0.2947): 100%|| 57/57 [00:38<00:00,  1.47it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 8; lr 0.0001010; Loss 0.1814; CCE_loss 0.1814; Dice 0.2947\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|| 91/91 [01:44<00:00,  1.15s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Val Score: 0.49016330858985047, Dice: 1.0, F1: 0.27166186941407217, F1_0: 0.8956655069082152, F1_1: 0.21570968312541347, F1_2: 0.13380939138725567, F1_3: 0.6672888104567722\n","score: [0.49016330858985047, 1.0, 0.27166186941407217, 0.8956655069082152, 0.21570968312541347, 0.13380939138725567, 0.6672888104567722]\tscore_best: 0.49016330858985047\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 9; lr 0.0001010; Loss 0.0983 (0.1699); cce_loss 0.0983 (0.1699); Dice 0.1948 (0.3071): 100%|| 57/57 [00:38<00:00,  1.48it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 9; lr 0.0001010; Loss 0.1699; CCE_loss 0.1699; Dice 0.3071\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|| 91/91 [01:45<00:00,  1.15s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Val Score: 0.576621888420692, Dice: 1.0, F1: 0.3951741263152742, F1_0: 0.8915886921956772, F1_1: 0.19463119689849528, F1_2: 0.39973900607242213, F1_3: 0.7347496495864101\n","score: [0.576621888420692, 1.0, 0.3951741263152742, 0.8915886921956772, 0.19463119689849528, 0.39973900607242213, 0.7347496495864101]\tscore_best: 0.576621888420692\n","Time: 11.651 min\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 0; lr 0.0002020; Loss 0.2041 (0.5707); cce_loss 0.2041 (0.5707); Dice 0.2514 (0.1712): 100%|| 57/57 [00:37<00:00,  1.51it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 0; lr 0.0002020; Loss 0.5707; CCE_loss 0.5707; Dice 0.1712\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 1; lr 0.0002020; Loss 0.2205 (0.3343); cce_loss 0.2205 (0.3343); Dice 0.3522 (0.3087): 100%|| 57/57 [00:38<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 1; lr 0.0002020; Loss 0.3343; CCE_loss 0.3343; Dice 0.3087\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 2; lr 0.0002020; Loss 0.2397 (0.2965); cce_loss 0.2397 (0.2965); Dice 0.3498 (0.4032): 100%|| 57/57 [00:38<00:00,  1.48it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 2; lr 0.0002020; Loss 0.2965; CCE_loss 0.2965; Dice 0.4032\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 3; lr 0.0002020; Loss 0.1690 (0.2611); cce_loss 0.1690 (0.2611); Dice 0.5795 (0.4414): 100%|| 57/57 [00:38<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 3; lr 0.0002020; Loss 0.2611; CCE_loss 0.2611; Dice 0.4414\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 4; lr 0.0002020; Loss 0.1467 (0.2555); cce_loss 0.1467 (0.2555); Dice 0.4619 (0.4490): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 4; lr 0.0000505; Loss 0.2555; CCE_loss 0.2555; Dice 0.4490\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 5; lr 0.0000505; Loss 0.1417 (0.2022); cce_loss 0.1417 (0.2022); Dice 0.5204 (0.5011): 100%|| 57/57 [00:38<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 5; lr 0.0001010; Loss 0.2022; CCE_loss 0.2022; Dice 0.5011\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 6; lr 0.0001010; Loss 0.1636 (0.1989); cce_loss 0.1636 (0.1989); Dice 0.4613 (0.5208): 100%|| 57/57 [00:37<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 6; lr 0.0001010; Loss 0.1989; CCE_loss 0.1989; Dice 0.5208\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 7; lr 0.0001010; Loss 0.2167 (0.1723); cce_loss 0.2167 (0.1723); Dice 0.5583 (0.5269): 100%|| 57/57 [00:38<00:00,  1.47it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 7; lr 0.0001010; Loss 0.1723; CCE_loss 0.1723; Dice 0.5269\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|| 91/91 [01:44<00:00,  1.15s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Val Score: 0.6603480942865136, Dice: 1.0, F1: 0.5147829918378767, F1_0: 0.9132816684135189, F1_1: 0.298201524871833, F1_2: 0.5322054994383152, F1_3: 0.6930392394416408\n","score: [0.6603480942865136, 1.0, 0.5147829918378767, 0.9132816684135189, 0.298201524871833, 0.5322054994383152, 0.6930392394416408]\tscore_best: 0.6603480942865136\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 8; lr 0.0001010; Loss 0.2140 (0.1604); cce_loss 0.2140 (0.1604); Dice 0.5197 (0.5482): 100%|| 57/57 [00:37<00:00,  1.51it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 8; lr 0.0001010; Loss 0.1604; CCE_loss 0.1604; Dice 0.5482\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|| 91/91 [01:44<00:00,  1.15s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Val Score: 0.6725481521735879, Dice: 1.0, F1: 0.5322116459622683, F1_0: 0.9085549668784321, F1_1: 0.2992003616391191, F1_2: 0.5909486653039833, F1_3: 0.7242498713499114\n","score: [0.6725481521735879, 1.0, 0.5322116459622683, 0.9085549668784321, 0.2992003616391191, 0.5909486653039833, 0.7242498713499114]\tscore_best: 0.6725481521735879\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 9; lr 0.0001010; Loss 0.1254 (0.1616); cce_loss 0.1254 (0.1616); Dice 0.4385 (0.5341): 100%|| 57/57 [00:39<00:00,  1.46it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 9; lr 0.0001010; Loss 0.1616; CCE_loss 0.1616; Dice 0.5341\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|| 91/91 [01:45<00:00,  1.16s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Val Score: 0.6344924760501189, Dice: 1.0, F1: 0.47784639435731274, F1_0: 0.9115802356718106, F1_1: 0.25015179466581805, F1_2: 0.5573633446027813, F1_3: 0.6746806275784887\n","score: [0.6344924760501189, 1.0, 0.47784639435731274, 0.9115802356718106, 0.25015179466581805, 0.5573633446027813, 0.6746806275784887]\tscore_best: 0.6725481521735879\n","Time: 11.663 min\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 0; lr 0.0002020; Loss 0.2384 (0.5992); cce_loss 0.2384 (0.5992); Dice 0.2936 (0.2273): 100%|| 57/57 [00:38<00:00,  1.48it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 0; lr 0.0002020; Loss 0.5992; CCE_loss 0.5992; Dice 0.2273\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 1; lr 0.0002020; Loss 0.1543 (0.3344); cce_loss 0.1543 (0.3344); Dice 0.2777 (0.2563): 100%|| 57/57 [00:38<00:00,  1.46it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 1; lr 0.0002020; Loss 0.3344; CCE_loss 0.3344; Dice 0.2563\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 2; lr 0.0002020; Loss 0.2799 (0.2933); cce_loss 0.2799 (0.2933); Dice 0.3679 (0.3220): 100%|| 57/57 [00:37<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 2; lr 0.0002020; Loss 0.2933; CCE_loss 0.2933; Dice 0.3220\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 3; lr 0.0002020; Loss 0.1703 (0.2537); cce_loss 0.1703 (0.2537); Dice 0.3016 (0.3608): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 3; lr 0.0002020; Loss 0.2537; CCE_loss 0.2537; Dice 0.3608\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 4; lr 0.0002020; Loss 0.3603 (0.2327); cce_loss 0.3603 (0.2327); Dice 0.4520 (0.3812): 100%|| 57/57 [00:38<00:00,  1.48it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 4; lr 0.0000505; Loss 0.2327; CCE_loss 0.2327; Dice 0.3812\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 5; lr 0.0000505; Loss 0.2437 (0.2058); cce_loss 0.2437 (0.2058); Dice 0.3987 (0.4012): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 5; lr 0.0001010; Loss 0.2058; CCE_loss 0.2058; Dice 0.4012\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 6; lr 0.0001010; Loss 0.2584 (0.2004); cce_loss 0.2584 (0.2004); Dice 0.4110 (0.3703): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 6; lr 0.0001010; Loss 0.2004; CCE_loss 0.2004; Dice 0.3703\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 7; lr 0.0001010; Loss 0.0971 (0.1887); cce_loss 0.0971 (0.1887); Dice 0.4516 (0.3454): 100%|| 57/57 [00:37<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 7; lr 0.0001010; Loss 0.1887; CCE_loss 0.1887; Dice 0.3454\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|| 91/91 [01:45<00:00,  1.16s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Val Score: 0.40936064390912036, Dice: 1.0, F1: 0.15622949129874342, F1_0: 0.8932345378575278, F1_1: 0.05295950927871452, F1_2: 0.2311070325780438, F1_3: 0.7843984387154793\n","score: [0.40936064390912036, 1.0, 0.15622949129874342, 0.8932345378575278, 0.05295950927871452, 0.2311070325780438, 0.7843984387154793]\tscore_best: 0.40936064390912036\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 8; lr 0.0001010; Loss 0.3260 (0.1753); cce_loss 0.3260 (0.1753); Dice 0.2829 (0.3342): 100%|| 57/57 [00:38<00:00,  1.47it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 8; lr 0.0001010; Loss 0.1753; CCE_loss 0.1753; Dice 0.3342\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|| 91/91 [01:45<00:00,  1.16s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Val Score: 0.6389776568736285, Dice: 1.0, F1: 0.48425379553375497, F1_0: 0.8960598636024787, F1_1: 0.24197306048638106, F1_2: 0.5737556752257812, F1_3: 0.7882929451583959\n","score: [0.6389776568736285, 1.0, 0.48425379553375497, 0.8960598636024787, 0.24197306048638106, 0.5737556752257812, 0.7882929451583959]\tscore_best: 0.6389776568736285\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 9; lr 0.0001010; Loss 0.2332 (0.1798); cce_loss 0.2332 (0.1798); Dice 0.4278 (0.3411): 100%|| 57/57 [00:38<00:00,  1.48it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 9; lr 0.0001010; Loss 0.1798; CCE_loss 0.1798; Dice 0.3411\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|| 91/91 [01:45<00:00,  1.16s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Val Score: 0.6386620401139256, Dice: 1.0, F1: 0.4838029144484651, F1_0: 0.885893413508352, F1_1: 0.2620175244788266, F1_2: 0.5245419875393778, F1_3: 0.7061731461366497\n","score: [0.6386620401139256, 1.0, 0.4838029144484651, 0.885893413508352, 0.2620175244788266, 0.5245419875393778, 0.7061731461366497]\tscore_best: 0.6389776568736285\n","Time: 11.718 min\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 0; lr 0.0002020; Loss 0.3555 (0.5470); cce_loss 0.3555 (0.5470); Dice 0.0490 (0.0711): 100%|| 57/57 [00:39<00:00,  1.46it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 0; lr 0.0002020; Loss 0.5470; CCE_loss 0.5470; Dice 0.0711\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 1; lr 0.0002020; Loss 0.2810 (0.3082); cce_loss 0.2810 (0.3082); Dice 0.2267 (0.1341): 100%|| 57/57 [00:37<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 1; lr 0.0002020; Loss 0.3082; CCE_loss 0.3082; Dice 0.1341\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 2; lr 0.0002020; Loss 0.1564 (0.2691); cce_loss 0.1564 (0.2691); Dice 0.1909 (0.2519): 100%|| 57/57 [00:37<00:00,  1.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 2; lr 0.0002020; Loss 0.2691; CCE_loss 0.2691; Dice 0.2519\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 3; lr 0.0002020; Loss 0.1751 (0.2346); cce_loss 0.1751 (0.2346); Dice 0.2173 (0.2800): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 3; lr 0.0002020; Loss 0.2346; CCE_loss 0.2346; Dice 0.2800\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 4; lr 0.0002020; Loss 0.0967 (0.2317); cce_loss 0.0967 (0.2317); Dice 0.3921 (0.3012): 100%|| 57/57 [00:38<00:00,  1.47it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["epoch: 4; lr 0.0000505; Loss 0.2317; CCE_loss 0.2317; Dice 0.3012\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 5; lr 0.0000505; Loss 0.1500 (0.2155); cce_loss 0.1500 (0.2155); Dice 0.2449 (0.3371): 100%|| 57/57 [00:37<00:00,  1.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 5; lr 0.0001010; Loss 0.2155; CCE_loss 0.2155; Dice 0.3371\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 6; lr 0.0001010; Loss 0.2215 (0.2077); cce_loss 0.2215 (0.2077); Dice 0.2731 (0.3198): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 6; lr 0.0001010; Loss 0.2077; CCE_loss 0.2077; Dice 0.3198\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 7; lr 0.0001010; Loss 0.1819 (0.1943); cce_loss 0.1819 (0.1943); Dice 0.1516 (0.2866): 100%|| 57/57 [00:38<00:00,  1.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 7; lr 0.0001010; Loss 0.1943; CCE_loss 0.1943; Dice 0.2866\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 91/91 [01:45<00:00,  1.16s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.30797922814591827, Dice: 1.0, F1: 0.011398897351311808, F1_0: 0.8950889239300195, F1_1: 0.0029418700826423683, F1_2: 0.11561701592369121, F1_3: 0.7460305007100815\n","score: [0.30797922814591827, 1.0, 0.011398897351311808, 0.8950889239300195, 0.0029418700826423683, 0.11561701592369121, 0.7460305007100815]\tscore_best: 0.30797922814591827\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 8; lr 0.0001010; Loss 0.1211 (0.1801); cce_loss 0.1211 (0.1801); Dice 0.2932 (0.2972): 100%|| 57/57 [00:38<00:00,  1.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 8; lr 0.0001010; Loss 0.1801; CCE_loss 0.1801; Dice 0.2972\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 91/91 [01:44<00:00,  1.15s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.5481907994922826, Dice: 1.0, F1: 0.3545582849889751, F1_0: 0.8840473440527361, F1_1: 0.20752919993705524, F1_2: 0.2631501634428167, F1_3: 0.6528214131885494\n","score: [0.5481907994922826, 1.0, 0.3545582849889751, 0.8840473440527361, 0.20752919993705524, 0.2631501634428167, 0.6528214131885494]\tscore_best: 0.5481907994922826\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 9; lr 0.0001010; Loss 0.1916 (0.1819); cce_loss 0.1916 (0.1819); Dice 0.2561 (0.3101): 100%|| 57/57 [00:38<00:00,  1.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 9; lr 0.0001010; Loss 0.1819; CCE_loss 0.1819; Dice 0.3101\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 91/91 [01:45<00:00,  1.16s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.5923370447817655, Dice: 1.0, F1: 0.41762434968823653, F1_0: 0.867502289969995, F1_1: 0.21618876606702414, F1_2: 0.39494043555714387, F1_3: 0.7888507147817655\n","score: [0.5923370447817655, 1.0, 0.41762434968823653, 0.867502289969995, 0.21618876606702414, 0.39494043555714387, 0.7888507147817655]\tscore_best: 0.5923370447817655\n","Time: 11.709 min\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 0; lr 0.0002020; Loss 0.2059 (0.5114); cce_loss 0.2059 (0.5114); Dice 0.1837 (0.2195): 100%|| 57/57 [00:38<00:00,  1.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 0; lr 0.0002020; Loss 0.5114; CCE_loss 0.5114; Dice 0.2195\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 1; lr 0.0002020; Loss 0.2587 (0.3440); cce_loss 0.2587 (0.3440); Dice 0.4113 (0.3143): 100%|| 57/57 [00:37<00:00,  1.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 1; lr 0.0002020; Loss 0.3440; CCE_loss 0.3440; Dice 0.3143\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 2; lr 0.0002020; Loss 0.2690 (0.2808); cce_loss 0.2690 (0.2808); Dice 0.3657 (0.4506): 100%|| 57/57 [00:37<00:00,  1.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 2; lr 0.0002020; Loss 0.2808; CCE_loss 0.2808; Dice 0.4506\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 3; lr 0.0002020; Loss 0.4206 (0.2559); cce_loss 0.4206 (0.2559); Dice 0.4394 (0.5186): 100%|| 57/57 [00:38<00:00,  1.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 3; lr 0.0002020; Loss 0.2559; CCE_loss 0.2559; Dice 0.5186\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 4; lr 0.0002020; Loss 0.0763 (0.2435); cce_loss 0.0763 (0.2435); Dice 0.5614 (0.5675): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 4; lr 0.0000505; Loss 0.2435; CCE_loss 0.2435; Dice 0.5675\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 5; lr 0.0000505; Loss 0.2359 (0.2114); cce_loss 0.2359 (0.2114); Dice 0.6364 (0.5819): 100%|| 57/57 [00:38<00:00,  1.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 5; lr 0.0001010; Loss 0.2114; CCE_loss 0.2114; Dice 0.5819\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 6; lr 0.0001010; Loss 0.1021 (0.1870); cce_loss 0.1021 (0.1870); Dice 0.6044 (0.5741): 100%|| 57/57 [00:37<00:00,  1.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 6; lr 0.0001010; Loss 0.1870; CCE_loss 0.1870; Dice 0.5741\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 7; lr 0.0001010; Loss 0.2530 (0.1959); cce_loss 0.2530 (0.1959); Dice 0.6694 (0.5680): 100%|| 57/57 [00:39<00:00,  1.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 7; lr 0.0001010; Loss 0.1959; CCE_loss 0.1959; Dice 0.5680\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 91/91 [01:44<00:00,  1.15s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.634513298877674, Dice: 1.0, F1: 0.47787614125382016, F1_0: 0.8965088029771157, F1_1: 0.28632256359467784, F1_2: 0.43040227587297625, F1_3: 0.6949364801332474\n","score: [0.634513298877674, 1.0, 0.47787614125382016, 0.8965088029771157, 0.28632256359467784, 0.43040227587297625, 0.6949364801332474]\tscore_best: 0.634513298877674\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 8; lr 0.0001010; Loss 0.3365 (0.1864); cce_loss 0.3365 (0.1864); Dice 0.6009 (0.5719): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 8; lr 0.0001010; Loss 0.1864; CCE_loss 0.1864; Dice 0.5719\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 91/91 [01:44<00:00,  1.15s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.6362432453954183, Dice: 1.0, F1: 0.48034749342202626, F1_0: 0.9136875326765285, F1_1: 0.2597493408323, F1_2: 0.4852864570526087, F1_3: 0.7562255602301456\n","score: [0.6362432453954183, 1.0, 0.48034749342202626, 0.9136875326765285, 0.2597493408323, 0.4852864570526087, 0.7562255602301456]\tscore_best: 0.6362432453954183\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 9; lr 0.0001010; Loss 0.1302 (0.1805); cce_loss 0.1302 (0.1805); Dice 0.5002 (0.5759): 100%|| 57/57 [00:37<00:00,  1.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 9; lr 0.0001010; Loss 0.1805; CCE_loss 0.1805; Dice 0.5759\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 91/91 [01:44<00:00,  1.15s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.5520115060809317, Dice: 1.0, F1: 0.36001643725847393, F1_0: 0.9128010215280118, F1_1: 0.17216913473431725, F1_2: 0.34128640844193736, F1_3: 0.7832156055639533\n","score: [0.5520115060809317, 1.0, 0.36001643725847393, 0.9128010215280118, 0.17216913473431725, 0.34128640844193736, 0.7832156055639533]\tscore_best: 0.6362432453954183\n","Time: 11.641 min\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 0; lr 0.0002020; Loss 0.6079 (0.6096); cce_loss 0.6079 (0.6096); Dice 0.1750 (0.2103): 100%|| 57/57 [00:38<00:00,  1.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 0; lr 0.0002020; Loss 0.6096; CCE_loss 0.6096; Dice 0.2103\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 1; lr 0.0002020; Loss 0.3009 (0.3141); cce_loss 0.3009 (0.3141); Dice 0.3927 (0.3494): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 1; lr 0.0002020; Loss 0.3141; CCE_loss 0.3141; Dice 0.3494\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 2; lr 0.0002020; Loss 0.1786 (0.2611); cce_loss 0.1786 (0.2611); Dice 0.4605 (0.4840): 100%|| 57/57 [00:37<00:00,  1.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 2; lr 0.0002020; Loss 0.2611; CCE_loss 0.2611; Dice 0.4840\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 3; lr 0.0002020; Loss 0.1315 (0.2604); cce_loss 0.1315 (0.2604); Dice 0.5414 (0.5496): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 3; lr 0.0002020; Loss 0.2604; CCE_loss 0.2604; Dice 0.5496\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 4; lr 0.0002020; Loss 0.3521 (0.2205); cce_loss 0.3521 (0.2205); Dice 0.5633 (0.6276): 100%|| 57/57 [00:38<00:00,  1.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 4; lr 0.0000505; Loss 0.2205; CCE_loss 0.2205; Dice 0.6276\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 5; lr 0.0000505; Loss 0.2728 (0.1993); cce_loss 0.2728 (0.1993); Dice 0.5691 (0.6594): 100%|| 57/57 [00:38<00:00,  1.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 5; lr 0.0001010; Loss 0.1993; CCE_loss 0.1993; Dice 0.6594\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 6; lr 0.0001010; Loss 0.1582 (0.1936); cce_loss 0.1582 (0.1936); Dice 0.6895 (0.6625): 100%|| 57/57 [00:38<00:00,  1.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 6; lr 0.0001010; Loss 0.1936; CCE_loss 0.1936; Dice 0.6625\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 7; lr 0.0001010; Loss 0.1835 (0.1804); cce_loss 0.1835 (0.1804); Dice 0.7370 (0.6756): 100%|| 57/57 [00:38<00:00,  1.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 7; lr 0.0001010; Loss 0.1804; CCE_loss 0.1804; Dice 0.6756\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 91/91 [01:44<00:00,  1.15s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.5651649652035068, Dice: 1.0, F1: 0.37880709314786687, F1_0: 0.9072038778800997, F1_1: 0.2888232191066926, F1_2: 0.21139587203536964, F1_3: 0.790872301466402\n","score: [0.5651649652035068, 1.0, 0.37880709314786687, 0.9072038778800997, 0.2888232191066926, 0.21139587203536964, 0.790872301466402]\tscore_best: 0.5651649652035068\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 8; lr 0.0001010; Loss 0.2231 (0.1857); cce_loss 0.2231 (0.1857); Dice 0.7077 (0.6825): 100%|| 57/57 [00:38<00:00,  1.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 8; lr 0.0001010; Loss 0.1857; CCE_loss 0.1857; Dice 0.6825\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 91/91 [01:45<00:00,  1.15s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.5301223149548548, Dice: 1.0, F1: 0.32874616422122116, F1_0: 0.9109621323890302, F1_1: 0.1290119652656927, F1_2: 0.5060045187199701, F1_3: 0.7449994304219254\n","score: [0.5301223149548548, 1.0, 0.32874616422122116, 0.9109621323890302, 0.1290119652656927, 0.5060045187199701, 0.7449994304219254]\tscore_best: 0.5651649652035068\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:437: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 9; lr 0.0001010; Loss 0.1380 (0.1568); cce_loss 0.1380 (0.1568); Dice 0.7240 (0.6991): 100%|| 57/57 [00:37<00:00,  1.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 9; lr 0.0001010; Loss 0.1568; CCE_loss 0.1568; Dice 0.6991\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 91/91 [01:45<00:00,  1.16s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.6113506488504299, Dice: 1.0, F1: 0.4447866412148998, F1_0: 0.9043008885758426, F1_1: 0.23621039566081992, F1_2: 0.43589707957649054, F1_3: 0.7354877090091937\n","score: [0.6113506488504299, 1.0, 0.4447866412148998, 0.9043008885758426, 0.23621039566081992, 0.43589707957649054, 0.7354877090091937]\tscore_best: 0.6113506488504299\n","Time: 11.654 min\n"]}],"source":["import ssl\n","\n","#for layers, ops, k_sizes, shape_types in random_combinations:\n","for combination in all_combinations_variable_layers:\n","    for layer, morph_op, k_size, shape in combination:\n","      # Crie um novo modelo aqui. Voc pode usar algo como:\n","      ssl._create_default_https_context = ssl._create_unverified_context\n","      t0 = timeit.default_timer()\n","      model = SeResNext50_Unet_2Ssum().cuda()\n","\n","      #for layer, morph_op, k_size, shape in zip(layers, ops, k_sizes, shape_types):\n","      if layer == 'after_conv3':\n","          in_channels = get_out_channels_from_conv(model,'conv3')\n","          morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","          model.conv3 = nn.Sequential(model.conv3, morph_layer)\n","      elif layer == 'after_conv4':\n","          in_channels = get_out_channels_from_conv(model,'conv4')\n","          morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","          model.conv4 = nn.Sequential(model.conv4, morph_layer)\n","      elif layer == 'after_conv5':\n","          in_channels = get_out_channels_from_conv(model,'conv5')\n","          morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","          model.conv5 = nn.Sequential(model.conv5, morph_layer)\n","\n","      params = model.parameters()\n","\n","      optimizer = AdamW(params, lr=0.000202, weight_decay=1e-6)     #0.002\n","\n","      scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[5, 13, 19, 23, 28, 47, 50, 60, 70, 90, 110, 130, 150, 170, 180, 190], gamma=0.5)\n","\n","      gc.collect()\n","      torch.cuda.empty_cache()\n","\n","      model = nn.DataParallel(model).cuda()\n","\n","      seg_loss = ComboLoss({'dice': 0.5, 'focal': 2.0}, per_image=False).cuda()\n","      ce_loss = nn.CrossEntropyLoss().cuda()\n","      seg_lossSeesaw = None#SeesawLoss2().cuda()\n","\n","      best_score = 0\n","      torch.cuda.empty_cache()\n","      for epoch in range(10):\n","          train_metrics = train_epoch(epoch, seg_loss, ce_loss, seg_lossSeesaw, model, optimizer, scheduler, train_data_loader)\n","          torch.cuda.empty_cache()\n","          if epoch > 6:\n","            best_score, d = evaluate_val(val_data_loader, best_score, model, snapshot_name, epoch)\n","          results = pd.read_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_pre_train_grid.csv')\n","          try:\n","            results_iter = pd.DataFrame([[epoch, train_metrics[0], train_metrics[1].avg, train_metrics[2].avg, train_metrics[3].avg, layer, morph_op, k_size, shape, d[0], d[1], d[2], d[3], d[4], d[5], d[6]]],columns=results.columns)\n","          except:\n","            results_iter = pd.DataFrame([[epoch, train_metrics[0], train_metrics[1].avg, train_metrics[2].avg, train_metrics[3].avg, layer, morph_op, k_size, shape, None, None, None, None, None, None, None]],columns=results.columns)\n","          results = pd.concat([results,results_iter])\n","          d = None\n","          results.to_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_pre_train_grid.csv', index=False)\n","      #torch.save(model.state_dict(), f'/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/{layer}_{morph_op}.pth')\n","\n","      try:\n","        del model\n","        torch.cuda.empty_cache()\n","      except:\n","        None\n","\n","      elapsed = timeit.default_timer() - t0\n","      print('Time: {:.3f} min'.format(elapsed / 60))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88dlU2o2sWNu","colab":{"base_uri":"https://localhost:8080/","height":263},"executionInfo":{"status":"ok","timestamp":1695234106396,"user_tz":180,"elapsed":16,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"1b34d2fc-0ebe-47fc-9573-6d5847ac4b31"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-66-e3a5c711612d>:1: FutureWarning: Dropping invalid columns in DataFrameGroupBy.max is deprecated. In a future version, a TypeError will be raised. Before calling .max, select only columns which should be valid for the function.\n","  results.groupby([\"layers\",\"ops\",\"k_sizes\",\"shape_types\"]).max().reset_index().sort_values([\"f1_score\"],ascending=False).head(5)\n"]},{"output_type":"execute_result","data":{"text/plain":["         layers       ops  k_sizes shape_types  epoch        lr      loss  \\\n","13  after_conv4   erosion        3      square      9  0.000202  0.622032   \n","21  after_conv4   opening        5      square      9  0.000202  0.570719   \n","0   after_conv4   closing        3      circle      9  0.000202  0.473010   \n","4   after_conv4   closing        7      circle      9  0.000202  0.638313   \n","7   after_conv4  dilation        3      square      9  0.000202  0.566724   \n","\n","    CCE_loss  val_score  dice_val  f1_score      F1_0      F1_1      F1_2  \\\n","13  0.622032   0.684388       1.0  0.549125  0.918499  0.332954  0.512308   \n","21  0.570719   0.672548       1.0  0.532212  0.913282  0.299200  0.590949   \n","0   0.473010   0.661385       1.0  0.516264  0.907814  0.325566  0.531645   \n","4   0.638313   0.652965       1.0  0.504235  0.919322  0.311477  0.523703   \n","7   0.566724   0.644959       1.0  0.492799  0.913866  0.290280  0.524787   \n","\n","        F1_3  \n","13  0.810643  \n","21  0.724250  \n","0   0.806691  \n","4   0.764633  \n","7   0.772179  "],"text/html":["\n","  <div id=\"df-d3d3e3c8-1ce2-473e-a843-18cfbf7d409d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>layers</th>\n","      <th>ops</th>\n","      <th>k_sizes</th>\n","      <th>shape_types</th>\n","      <th>epoch</th>\n","      <th>lr</th>\n","      <th>loss</th>\n","      <th>CCE_loss</th>\n","      <th>val_score</th>\n","      <th>dice_val</th>\n","      <th>f1_score</th>\n","      <th>F1_0</th>\n","      <th>F1_1</th>\n","      <th>F1_2</th>\n","      <th>F1_3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>13</th>\n","      <td>after_conv4</td>\n","      <td>erosion</td>\n","      <td>3</td>\n","      <td>square</td>\n","      <td>9</td>\n","      <td>0.000202</td>\n","      <td>0.622032</td>\n","      <td>0.622032</td>\n","      <td>0.684388</td>\n","      <td>1.0</td>\n","      <td>0.549125</td>\n","      <td>0.918499</td>\n","      <td>0.332954</td>\n","      <td>0.512308</td>\n","      <td>0.810643</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>after_conv4</td>\n","      <td>opening</td>\n","      <td>5</td>\n","      <td>square</td>\n","      <td>9</td>\n","      <td>0.000202</td>\n","      <td>0.570719</td>\n","      <td>0.570719</td>\n","      <td>0.672548</td>\n","      <td>1.0</td>\n","      <td>0.532212</td>\n","      <td>0.913282</td>\n","      <td>0.299200</td>\n","      <td>0.590949</td>\n","      <td>0.724250</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>after_conv4</td>\n","      <td>closing</td>\n","      <td>3</td>\n","      <td>circle</td>\n","      <td>9</td>\n","      <td>0.000202</td>\n","      <td>0.473010</td>\n","      <td>0.473010</td>\n","      <td>0.661385</td>\n","      <td>1.0</td>\n","      <td>0.516264</td>\n","      <td>0.907814</td>\n","      <td>0.325566</td>\n","      <td>0.531645</td>\n","      <td>0.806691</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>after_conv4</td>\n","      <td>closing</td>\n","      <td>7</td>\n","      <td>circle</td>\n","      <td>9</td>\n","      <td>0.000202</td>\n","      <td>0.638313</td>\n","      <td>0.638313</td>\n","      <td>0.652965</td>\n","      <td>1.0</td>\n","      <td>0.504235</td>\n","      <td>0.919322</td>\n","      <td>0.311477</td>\n","      <td>0.523703</td>\n","      <td>0.764633</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>after_conv4</td>\n","      <td>dilation</td>\n","      <td>3</td>\n","      <td>square</td>\n","      <td>9</td>\n","      <td>0.000202</td>\n","      <td>0.566724</td>\n","      <td>0.566724</td>\n","      <td>0.644959</td>\n","      <td>1.0</td>\n","      <td>0.492799</td>\n","      <td>0.913866</td>\n","      <td>0.290280</td>\n","      <td>0.524787</td>\n","      <td>0.772179</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3d3e3c8-1ce2-473e-a843-18cfbf7d409d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d3d3e3c8-1ce2-473e-a843-18cfbf7d409d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d3d3e3c8-1ce2-473e-a843-18cfbf7d409d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-1ad6b29b-5125-4466-a519-1f2e97798a9f\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1ad6b29b-5125-4466-a519-1f2e97798a9f')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-1ad6b29b-5125-4466-a519-1f2e97798a9f button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":66}],"source":["results.groupby([\"layers\",\"ops\",\"k_sizes\",\"shape_types\"]).max().reset_index().sort_values([\"f1_score\"],ascending=False).head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GNFOX5WcrlLP"},"outputs":[],"source":["del model\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHGryCXssKzG"},"outputs":[],"source":["#torch.cuda.empty_cache()\n","#del model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRg_hj2MH2e7"},"outputs":[],"source":["#model.load_state_dict(torch.load('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/modelBDANet_test_train_3.pth'))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["TlRP-duNXnpK","yUSNtuVeTUwl","pTW3yt84TQMf","FXa807PbTjxn","Ifa7HjK2FVf3","tHCAM2vfRK7e"],"machine_shape":"hm","provenance":[],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyP83xoBVUbefUCVKJNUf7sY"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}