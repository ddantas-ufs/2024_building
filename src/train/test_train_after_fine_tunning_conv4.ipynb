{"cells":[{"cell_type":"markdown","metadata":{"id":"xZ0IEwYcRzAl"},"source":["## Requirements"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29672,"status":"ok","timestamp":1696711652660,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"iCeN-zCbXqfV","outputId":"66fc303e-7b70-4cd3-fcf1-b6d14b1f3c8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1696711652660,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"NI4_17gjk3dU","outputId":"86e55510-69f6-4b3f-9ed4-24bcc3c6b6dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}],"source":["print(1)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"k-JXF71-k4O3","executionInfo":{"status":"ok","timestamp":1696711652660,"user_tz":180,"elapsed":3,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["#import os\n","#os.kill(os.getpid(), 9)"]},{"cell_type":"markdown","metadata":{"id":"TlRP-duNXnpK"},"source":["## Iniciation"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112332,"status":"ok","timestamp":1696711764989,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"uOyfTgUDztSD","outputId":"b67a0d44-67dd-4721-bd9b-682db7dfb7fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 2.0.1+cu118\n","Uninstalling torch-2.0.1+cu118:\n","  Successfully uninstalled torch-2.0.1+cu118\n","Found existing installation: torchvision 0.15.2+cu118\n","Uninstalling torchvision-0.15.2+cu118:\n","  Successfully uninstalled torchvision-0.15.2+cu118\n","Found existing installation: torchaudio 2.0.2+cu118\n","Uninstalling torchaudio-2.0.2+cu118:\n","  Successfully uninstalled torchaudio-2.0.2+cu118\n","Looking in indexes: https://download.pytorch.org/whl/cu118\n","Collecting torch\n","  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp310-cp310-linux_x86_64.whl (2325.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m852.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision\n","  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp310-cp310-linux_x86_64.whl (6.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchaudio\n","  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp310-cp310-linux_x86_64.whl (3.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting triton==2.1.0 (from torch)\n","  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: triton, torch, torchvision, torchaudio\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.0.0\n","    Uninstalling triton-2.0.0:\n","      Successfully uninstalled triton-2.0.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.0+cu118 which is incompatible.\n","torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.1.0+cu118 which is incompatible.\n","torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.1.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-2.1.0+cu118 torchaudio-2.1.0+cu118 torchvision-0.16.0+cu118 triton-2.1.0\n"]}],"source":["!pip uninstall torch torchvision torchaudio -y\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1696711764989,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"dSxd1Sjk6_2a","outputId":"8496d113-443e-434c-ae51-7e40ed19f25f"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2289,"status":"ok","timestamp":1696711767273,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"K0WhAIMi0KX1","outputId":"369d4edb-12f9-47e4-dc25-1783ce7279d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.1.0+cu118\n"]}],"source":["import torch\n","print(torch.__version__)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":631,"status":"ok","timestamp":1696711767900,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"ejMJGR8w0nmM","outputId":"51dc75c8-190d-48b4-925c-de04aad8f42f"},"outputs":[{"output_type":"stream","name":"stdout","text":["absl-py==1.4.0\n","aiohttp==3.8.5\n","aiosignal==1.3.1\n","alabaster==0.7.13\n","albumentations==1.3.1\n","altair==4.2.2\n","anyio==3.7.1\n","appdirs==1.4.4\n","argon2-cffi==23.1.0\n","argon2-cffi-bindings==21.2.0\n","array-record==0.4.1\n","arviz==0.15.1\n","astropy==5.3.4\n","astunparse==1.6.3\n","async-timeout==4.0.3\n","attrs==23.1.0\n","audioread==3.0.1\n","autograd==1.6.2\n","Babel==2.13.0\n","backcall==0.2.0\n","beautifulsoup4==4.11.2\n","bleach==6.0.0\n","blinker==1.4\n","blis==0.7.11\n","blosc2==2.0.0\n","bokeh==3.2.2\n","bqplot==0.12.40\n","branca==0.6.0\n","build==1.0.3\n","CacheControl==0.13.1\n","cachetools==5.3.1\n","catalogue==2.0.10\n","certifi==2023.7.22\n","cffi==1.16.0\n","chardet==5.2.0\n","charset-normalizer==3.3.0\n","chex==0.1.7\n","click==8.1.7\n","click-plugins==1.1.1\n","cligj==0.7.2\n","cloudpickle==2.2.1\n","cmake==3.27.6\n","cmdstanpy==1.2.0\n","colorcet==3.0.1\n","colorlover==0.3.0\n","colour==0.1.5\n","community==1.0.0b1\n","confection==0.1.3\n","cons==0.4.6\n","contextlib2==21.6.0\n","contourpy==1.1.1\n","convertdate==2.4.0\n","cryptography==41.0.4\n","cufflinks==0.17.3\n","cupy-cuda11x==11.0.0\n","cvxopt==1.3.2\n","cvxpy==1.3.2\n","cycler==0.12.0\n","cymem==2.0.8\n","Cython==3.0.2\n","dask==2023.8.1\n","datascience==0.17.6\n","db-dtypes==1.1.1\n","dbus-python==1.2.18\n","debugpy==1.6.6\n","decorator==4.4.2\n","defusedxml==0.7.1\n","distributed==2023.8.1\n","distro==1.7.0\n","dlib==19.24.2\n","dm-tree==0.1.8\n","docutils==0.18.1\n","dopamine-rl==4.0.6\n","duckdb==0.8.1\n","earthengine-api==0.1.371\n","easydict==1.10\n","ecos==2.0.12\n","editdistance==0.6.2\n","eerepr==0.0.4\n","en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl#sha256=83276fc78a70045627144786b52e1f2728ad5e29e5e43916ec37ea9c26a11212\n","entrypoints==0.4\n","ephem==4.1.4\n","et-xmlfile==1.1.0\n","etils==1.5.0\n","etuples==0.3.9\n","exceptiongroup==1.1.3\n","fastai==2.7.12\n","fastcore==1.5.29\n","fastdownload==0.0.7\n","fastjsonschema==2.18.1\n","fastprogress==1.0.3\n","fastrlock==0.8.2\n","filelock==3.12.4\n","Fiona==1.9.4.post1\n","firebase-admin==5.3.0\n","Flask==2.2.5\n","flatbuffers==23.5.26\n","flax==0.7.4\n","folium==0.14.0\n","fonttools==4.43.0\n","frozendict==2.3.8\n","frozenlist==1.4.0\n","fsspec==2023.6.0\n","future==0.18.3\n","gast==0.4.0\n","gcsfs==2023.6.0\n","GDAL==3.4.3\n","gdown==4.6.6\n","geemap==0.28.1\n","gensim==4.3.2\n","geocoder==1.38.1\n","geographiclib==2.0\n","geopandas==0.13.2\n","geopy==2.3.0\n","gin-config==0.5.0\n","glob2==0.7\n","google==2.0.3\n","google-api-core==2.11.1\n","google-api-python-client==2.84.0\n","google-auth==2.17.3\n","google-auth-httplib2==0.1.1\n","google-auth-oauthlib==1.0.0\n","google-cloud-bigquery==3.10.0\n","google-cloud-bigquery-connection==1.12.1\n","google-cloud-bigquery-storage==2.22.0\n","google-cloud-core==2.3.3\n","google-cloud-datastore==2.15.2\n","google-cloud-firestore==2.11.1\n","google-cloud-functions==1.13.3\n","google-cloud-language==2.9.1\n","google-cloud-storage==2.8.0\n","google-cloud-translate==3.11.3\n","google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz#sha256=5ca4d64db1f8368e3b9822e4025f8cf90b99524b58b544be156c3df5e4adc738\n","google-crc32c==1.5.0\n","google-pasta==0.2.0\n","google-resumable-media==2.6.0\n","googleapis-common-protos==1.60.0\n","googledrivedownloader==0.4\n","graphviz==0.20.1\n","greenlet==3.0.0\n","grpc-google-iam-v1==0.12.6\n","grpcio==1.59.0\n","grpcio-status==1.48.2\n","gspread==3.4.2\n","gspread-dataframe==3.3.1\n","gym==0.25.2\n","gym-notices==0.0.8\n","h5netcdf==1.2.0\n","h5py==3.9.0\n","holidays==0.34\n","holoviews==1.17.1\n","html5lib==1.1\n","httpimport==1.3.1\n","httplib2==0.22.0\n","humanize==4.7.0\n","hyperopt==0.2.7\n","idna==3.4\n","imageio==2.31.5\n","imageio-ffmpeg==0.4.9\n","imagesize==1.4.1\n","imbalanced-learn==0.10.1\n","imgaug==0.4.0\n","importlib-metadata==6.8.0\n","importlib-resources==6.1.0\n","imutils==0.5.4\n","inflect==7.0.0\n","iniconfig==2.0.0\n","intel-openmp==2023.2.0\n","ipyevents==2.0.2\n","ipyfilechooser==0.6.0\n","ipykernel==5.5.6\n","ipyleaflet==0.17.4\n","ipython==7.34.0\n","ipython-genutils==0.2.0\n","ipython-sql==0.5.0\n","ipytree==0.2.2\n","ipywidgets==7.7.1\n","itsdangerous==2.1.2\n","jax==0.4.16\n","jaxlib @ https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.16+cuda11.cudnn86-cp310-cp310-manylinux2014_x86_64.whl#sha256=78b3a9acfda4bfaae8a1dc112995d56454020f5c02dba4d24c40c906332efd4a\n","jeepney==0.7.1\n","jieba==0.42.1\n","Jinja2==3.1.2\n","joblib==1.3.2\n","jsonpickle==3.0.2\n","jsonschema==4.19.1\n","jsonschema-specifications==2023.7.1\n","jupyter-client==6.1.12\n","jupyter-console==6.1.0\n","jupyter-server==1.24.0\n","jupyter_core==5.3.2\n","jupyterlab-pygments==0.2.2\n","jupyterlab-widgets==3.0.9\n","kaggle==1.5.16\n","keras==2.13.1\n","keyring==23.5.0\n","kiwisolver==1.4.5\n","langcodes==3.3.0\n","launchpadlib==1.10.16\n","lazr.restfulclient==0.14.4\n","lazr.uri==1.0.6\n","lazy_loader==0.3\n","libclang==16.0.6\n","librosa==0.10.1\n","lightgbm==4.0.0\n","linkify-it-py==2.0.2\n","lit==17.0.2\n","llvmlite==0.39.1\n","locket==1.0.0\n","logical-unification==0.4.6\n","LunarCalendar==0.0.9\n","lxml==4.9.3\n","Markdown==3.4.4\n","markdown-it-py==3.0.0\n","MarkupSafe==2.1.3\n","matplotlib==3.7.1\n","matplotlib-inline==0.1.6\n","matplotlib-venn==0.11.9\n","mdit-py-plugins==0.4.0\n","mdurl==0.1.2\n","miniKanren==1.0.3\n","missingno==0.5.2\n","mistune==0.8.4\n","mizani==0.9.3\n","mkl==2023.2.0\n","ml-dtypes==0.3.1\n","mlxtend==0.22.0\n","more-itertools==10.1.0\n","moviepy==1.0.3\n","mpmath==1.3.0\n","msgpack==1.0.7\n","multidict==6.0.4\n","multipledispatch==1.0.0\n","multitasking==0.0.11\n","murmurhash==1.0.10\n","music21==9.1.0\n","natsort==8.4.0\n","nbclassic==1.0.0\n","nbclient==0.8.0\n","nbconvert==6.5.4\n","nbformat==5.9.2\n","nest-asyncio==1.5.8\n","networkx==3.1\n","nibabel==4.0.2\n","nltk==3.8.1\n","notebook==6.5.5\n","notebook_shim==0.2.3\n","numba==0.56.4\n","numexpr==2.8.7\n","numpy==1.23.5\n","oauth2client==4.1.3\n","oauthlib==3.2.2\n","opencv-contrib-python==4.8.0.76\n","opencv-python==4.8.0.76\n","opencv-python-headless==4.8.1.78\n","openpyxl==3.1.2\n","opt-einsum==3.3.0\n","optax==0.1.7\n","orbax-checkpoint==0.4.1\n","osqp==0.6.2.post8\n","packaging==23.2\n","pandas==1.5.3\n","pandas-datareader==0.10.0\n","pandas-gbq==0.17.9\n","pandocfilters==1.5.0\n","panel==1.2.3\n","param==1.13.0\n","parso==0.8.3\n","partd==1.4.1\n","pathlib==1.0.1\n","pathy==0.10.2\n","patsy==0.5.3\n","peewee==3.16.3\n","pexpect==4.8.0\n","pickleshare==0.7.5\n","Pillow==9.4.0\n","pip-tools==6.13.0\n","platformdirs==3.11.0\n","plotly==5.15.0\n","plotnine==0.12.3\n","pluggy==1.3.0\n","polars==0.17.3\n","pooch==1.7.0\n","portpicker==1.5.2\n","prefetch-generator==1.0.3\n","preshed==3.0.9\n","prettytable==3.9.0\n","proglog==0.1.10\n","progressbar2==4.2.0\n","prometheus-client==0.17.1\n","promise==2.3\n","prompt-toolkit==3.0.39\n","prophet==1.1.4\n","proto-plus==1.22.3\n","protobuf==3.20.3\n","psutil==5.9.5\n","psycopg2==2.9.9\n","ptyprocess==0.7.0\n","py-cpuinfo==9.0.0\n","py4j==0.10.9.7\n","pyarrow==9.0.0\n","pyasn1==0.5.0\n","pyasn1-modules==0.3.0\n","pycocotools==2.0.7\n","pycparser==2.21\n","pyct==0.5.0\n","pydantic==1.10.13\n","pydata-google-auth==1.8.2\n","pydot==1.4.2\n","pydot-ng==2.0.0\n","pydotplus==2.0.2\n","PyDrive==1.3.1\n","PyDrive2==1.6.3\n","pyerfa==2.0.0.3\n","pygame==2.5.2\n","Pygments==2.16.1\n","PyGObject==3.42.1\n","PyJWT==2.3.0\n","pymc==5.7.2\n","PyMeeus==0.5.12\n","pymystem3==0.2.0\n","PyOpenGL==3.1.7\n","pyOpenSSL==23.2.0\n","pyparsing==3.1.1\n","pyperclip==1.8.2\n","pyproj==3.6.1\n","pyproject_hooks==1.0.0\n","pyshp==2.3.1\n","PySocks==1.7.1\n","pytensor==2.14.2\n","pytest==7.4.2\n","python-apt==0.0.0\n","python-box==7.1.1\n","python-dateutil==2.8.2\n","python-louvain==0.16\n","python-slugify==8.0.1\n","python-utils==3.8.1\n","pytz==2023.3.post1\n","pyviz_comms==3.0.0\n","PyWavelets==1.4.1\n","PyYAML==6.0.1\n","pyzmq==23.2.1\n","qdldl==0.1.7.post0\n","qudida==0.0.4\n","ratelim==0.1.6\n","referencing==0.30.2\n","regex==2023.6.3\n","requests==2.31.0\n","requests-oauthlib==1.3.1\n","requirements-parser==0.5.0\n","rich==13.6.0\n","rpds-py==0.10.3\n","rpy2==3.4.2\n","rsa==4.9\n","scikit-image==0.19.3\n","scikit-learn==1.2.2\n","scipy==1.11.3\n","scooby==0.7.4\n","scs==3.2.3\n","seaborn==0.12.2\n","SecretStorage==3.3.1\n","Send2Trash==1.8.2\n","shapely==2.0.1\n","six==1.16.0\n","sklearn-pandas==2.2.0\n","smart-open==6.4.0\n","sniffio==1.3.0\n","snowballstemmer==2.2.0\n","sortedcontainers==2.4.0\n","soundfile==0.12.1\n","soupsieve==2.5\n","soxr==0.3.6\n","spacy==3.6.1\n","spacy-legacy==3.0.12\n","spacy-loggers==1.0.5\n","Sphinx==5.0.2\n","sphinxcontrib-applehelp==1.0.7\n","sphinxcontrib-devhelp==1.0.5\n","sphinxcontrib-htmlhelp==2.0.4\n","sphinxcontrib-jsmath==1.0.1\n","sphinxcontrib-qthelp==1.0.6\n","sphinxcontrib-serializinghtml==1.1.9\n","SQLAlchemy==2.0.21\n","sqlparse==0.4.4\n","srsly==2.4.8\n","stanio==0.3.0\n","statsmodels==0.14.0\n","sympy==1.12\n","tables==3.8.0\n","tabulate==0.9.0\n","tbb==2021.10.0\n","tblib==2.0.0\n","tenacity==8.2.3\n","tensorboard==2.13.0\n","tensorboard-data-server==0.7.1\n","tensorflow==2.13.0\n","tensorflow-datasets==4.9.3\n","tensorflow-estimator==2.13.0\n","tensorflow-gcs-config==2.13.0\n","tensorflow-hub==0.14.0\n","tensorflow-io-gcs-filesystem==0.34.0\n","tensorflow-metadata==1.14.0\n","tensorflow-probability==0.20.1\n","tensorstore==0.1.45\n","termcolor==2.3.0\n","terminado==0.17.1\n","text-unidecode==1.3\n","textblob==0.17.1\n","tf-slim==1.1.0\n","thinc==8.1.12\n","threadpoolctl==3.2.0\n","tifffile==2023.9.26\n","tinycss2==1.2.1\n","toml==0.10.2\n","tomli==2.0.1\n","toolz==0.12.0\n","torch==2.1.0+cu118\n","torchaudio==2.1.0+cu118\n","torchdata==0.6.1\n","torchsummary==1.5.1\n","torchtext==0.15.2\n","torchvision==0.16.0+cu118\n","tornado==6.3.2\n","tqdm==4.66.1\n","traitlets==5.7.1\n","traittypes==0.2.1\n","triton==2.1.0\n","tweepy==4.13.0\n","typer==0.9.0\n","types-setuptools==68.2.0.0\n","typing_extensions==4.5.0\n","tzlocal==5.1\n","uc-micro-py==1.0.2\n","uritemplate==4.1.1\n","urllib3==2.0.6\n","vega-datasets==0.9.0\n","wadllib==1.3.6\n","wasabi==1.1.2\n","wcwidth==0.2.8\n","webcolors==1.13\n","webencodings==0.5.1\n","websocket-client==1.6.3\n","Werkzeug==3.0.0\n","widgetsnbextension==3.6.6\n","wordcloud==1.9.2\n","wrapt==1.15.0\n","xarray==2023.7.0\n","xarray-einstats==0.6.0\n","xgboost==2.0.0\n","xlrd==2.0.1\n","xyzservices==2023.7.0\n","yarl==1.9.2\n","yellowbrick==1.5\n","yfinance==0.2.30\n","zict==3.0.0\n","zipp==3.17.0\n"]}],"source":["!pip freeze"]},{"cell_type":"markdown","metadata":{"id":"yUSNtuVeTUwl"},"source":["## Optimazer"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"HZMtCepVRDaJ","executionInfo":{"status":"ok","timestamp":1696711767900,"user_tz":180,"elapsed":4,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["# Based on https://github.com/pytorch/pytorch/pull/3740\n","import torch\n","import math\n","\n","\n","class AdamW(torch.optim.Optimizer):\n","    \"\"\"Implements AdamW algorithm.\n","\n","    It has been proposed in `Fixing Weight Decay Regularization in Adam`_.\n","\n","    Arguments:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float, optional): learning rate (default: 1e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of gradient and its square (default: (0.9, 0.999))\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability (default: 1e-8)\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","\n","    .. Fixing Weight Decay Regularization in Adam:\n","    https://arxiv.org/abs/1711.05101\n","    \"\"\"\n","\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=0):\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay)\n","        super(AdamW, self).__init__(params, defaults)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n","\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","\n","                # according to the paper, this penalty should come after the bias correction\n","                # if group['weight_decay'] != 0:\n","                #     grad = grad.add(group['weight_decay'], p.data)\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","\n","                denom = exp_avg_sq.sqrt().add_(group['eps'])\n","\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n","\n","                # w = w - wd * lr * w\n","                if group['weight_decay'] != 0:\n","                    p.data.add_(-group['weight_decay'] * group['lr'], p.data)\n","\n","                # w = w - lr * w.grad\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                # w = w - wd * lr * w - lr * w.grad\n","                # See http://www.fast.ai/2018/07/02/adam-weight-decay/\n","\n","        return loss"]},{"cell_type":"markdown","metadata":{"id":"pTW3yt84TQMf"},"source":["## Losses"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"MR5-L7m_SCwH","executionInfo":{"status":"ok","timestamp":1696711767902,"user_tz":180,"elapsed":6,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch.autograd import Variable\n","\n","try:\n","    from itertools import ifilterfalse\n","except ImportError:  # py3k\n","    from itertools import filterfalse\n","\n","eps = 1e-6\n","\n","def dice_round(preds, trues):\n","    preds = preds.float()\n","    return soft_dice_loss(preds, trues)\n","\n","\n","def iou_round(preds, trues):\n","    preds = preds.float()\n","    return jaccard(preds, trues)\n","\n","\n","def soft_dice_loss(outputs, targets, per_image=False):\n","    batch_size = outputs.size()[0]\n","    if not per_image:\n","        batch_size = 1\n","    dice_target = targets.contiguous().view(batch_size, -1).float()\n","    dice_output = outputs.contiguous().view(batch_size, -1)\n","    intersection = torch.sum(dice_output * dice_target, dim=1)\n","    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1) + eps\n","    loss = (1 - (2 * intersection + eps) / union).mean()\n","    return loss\n","\n","\n","def jaccard(outputs, targets, per_image=False):\n","    batch_size = outputs.size()[0]\n","    if not per_image:\n","        batch_size = 1\n","    dice_target = targets.contiguous().view(batch_size, -1).float()\n","    dice_output = outputs.contiguous().view(batch_size, -1)\n","    intersection = torch.sum(dice_output * dice_target, dim=1)\n","    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1) - intersection + eps\n","    losses = 1 - (intersection + eps) / union\n","    return losses.mean()\n","\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True, per_image=False):\n","        super().__init__()\n","        self.size_average = size_average\n","        self.register_buffer('weight', weight)\n","        self.per_image = per_image\n","\n","    def forward(self, input, target):\n","        return soft_dice_loss(input, target, per_image=self.per_image)\n","\n","\n","class JaccardLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True, per_image=False):\n","        super().__init__()\n","        self.size_average = size_average\n","        self.register_buffer('weight', weight)\n","        self.per_image = per_image\n","\n","    def forward(self, input, target):\n","        return jaccard(input, target, per_image=self.per_image)\n","\n","\n","class StableBCELoss(nn.Module):\n","    def __init__(self):\n","        super(StableBCELoss, self).__init__()\n","\n","    def forward(self, input, target):\n","        input = input.float().view(-1)\n","        target = target.float().view(-1)\n","        neg_abs = - input.abs()\n","        # todo check correctness\n","        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n","        return loss.mean()\n","\n","\n","class ComboLoss(nn.Module):\n","    def __init__(self, weights, per_image=False):\n","        super().__init__()\n","        self.weights = weights\n","        self.bce = StableBCELoss()\n","        self.dice = DiceLoss(per_image=False)\n","        self.jaccard = JaccardLoss(per_image=False)\n","        self.lovasz = LovaszLoss(per_image=per_image)\n","        self.lovasz_sigmoid = LovaszLossSigmoid(per_image=per_image)\n","        self.focal = FocalLoss2d()\n","        self.mapping = {'bce': self.bce,\n","                        'dice': self.dice,\n","                        'focal': self.focal,\n","                        'jaccard': self.jaccard,\n","                        'lovasz': self.lovasz,\n","                        'lovasz_sigmoid': self.lovasz_sigmoid}\n","        self.expect_sigmoid = {'dice', 'focal', 'jaccard', 'lovasz_sigmoid'}\n","        self.values = {}\n","\n","    def forward(self, outputs, targets):\n","        loss = 0\n","        weights = self.weights\n","        sigmoid_input = torch.sigmoid(outputs)\n","        for k, v in weights.items():\n","            if not v:\n","                continue\n","            val = self.mapping[k](sigmoid_input if k in self.expect_sigmoid else outputs, targets)\n","            self.values[k] = val\n","            loss += self.weights[k] * val\n","        return loss\n","\n","\n","def lovasz_grad(gt_sorted):\n","    \"\"\"\n","    Computes gradient of the Lovasz extension w.r.t sorted errors\n","    See Alg. 1 in paper\n","    \"\"\"\n","    p = len(gt_sorted)\n","    gts = gt_sorted.sum()\n","    intersection = gts.float() - gt_sorted.float().cumsum(0)\n","    union = gts.float() + (1 - gt_sorted).float().cumsum(0)\n","    jaccard = 1. - intersection / union\n","    if p > 1:  # cover 1-pixel case\n","        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n","    return jaccard\n","\n","\n","def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n","      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class id\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n","                    for log, lab in zip(logits, labels))\n","    else:\n","        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n","    return loss\n","\n","\n","def lovasz_hinge_flat(logits, labels):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n","      labels: [P] Tensor, binary ground truth labels (0 or 1)\n","      ignore: label to ignore\n","    \"\"\"\n","    if len(labels) == 0:\n","        # only void pixels, the gradients should be 0\n","        return logits.sum() * 0.\n","    signs = 2. * labels.float() - 1.\n","    errors = (1. - logits * Variable(signs))\n","    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n","    perm = perm.data\n","    gt_sorted = labels[perm]\n","    grad = lovasz_grad(gt_sorted)\n","    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n","    return loss\n","\n","\n","def flatten_binary_scores(scores, labels, ignore=None):\n","    \"\"\"\n","    Flattens predictions in the batch (binary case)\n","    Remove labels equal to 'ignore'\n","    \"\"\"\n","    scores = scores.view(-1)\n","    labels = labels.view(-1)\n","    if ignore is None:\n","        return scores, labels\n","    valid = (labels != ignore)\n","    vscores = scores[valid]\n","    vlabels = labels[valid]\n","    return vscores, vlabels\n","\n","\n","def lovasz_sigmoid(probas, labels, per_image=False, ignore=None):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n","      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n","      only_present: average only on classes present in ground truth\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class labels\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_sigmoid_flat(*flatten_binary_scores(prob.unsqueeze(0), lab.unsqueeze(0), ignore))\n","                          for prob, lab in zip(probas, labels))\n","    else:\n","        loss = lovasz_sigmoid_flat(*flatten_binary_scores(probas, labels, ignore))\n","    return loss\n","\n","\n","def lovasz_sigmoid_flat(probas, labels):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n","      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n","      only_present: average only on classes present in ground truth\n","    \"\"\"\n","    fg = labels.float()\n","    errors = (Variable(fg) - probas).abs()\n","    errors_sorted, perm = torch.sort(errors, 0, descending=True)\n","    perm = perm.data\n","    fg_sorted = fg[perm]\n","    loss = torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted)))\n","    return loss\n","\n","\n","def mean(l, ignore_nan=False, empty=0):\n","    \"\"\"\n","    nanmean compatible with generators.\n","    \"\"\"\n","    l = iter(l)\n","    if ignore_nan:\n","        l = ifilterfalse(np.isnan, l)\n","    try:\n","        n = 1\n","        acc = next(l)\n","    except StopIteration:\n","        if empty == 'raise':\n","            raise ValueError('Empty mean')\n","        return empty\n","    for n, v in enumerate(l, 2):\n","        acc += v\n","    if n == 1:\n","        return acc\n","    return acc / n\n","\n","\n","class LovaszLoss(nn.Module):\n","    def __init__(self, ignore_index=255, per_image=True):\n","        super().__init__()\n","        self.ignore_index = ignore_index\n","        self.per_image = per_image\n","\n","    def forward(self, outputs, targets):\n","        outputs = outputs.contiguous()\n","        targets = targets.contiguous()\n","        return lovasz_hinge(outputs, targets, per_image=self.per_image, ignore=self.ignore_index)\n","\n","\n","class LovaszLossSigmoid(nn.Module):\n","    def __init__(self, ignore_index=255, per_image=True):\n","        super().__init__()\n","        self.ignore_index = ignore_index\n","        self.per_image = per_image\n","\n","    def forward(self, outputs, targets):\n","        outputs = outputs.contiguous()\n","        targets = targets.contiguous()\n","        return lovasz_sigmoid(outputs, targets, per_image=self.per_image, ignore=self.ignore_index)\n","\n","\n","class FocalLoss2d(nn.Module):\n","    def __init__(self, gamma=2, ignore_index=255):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.ignore_index = ignore_index\n","\n","    def forward(self, outputs, targets):\n","        outputs = outputs.contiguous()\n","        targets = targets.contiguous()\n","        # eps = 1e-8\n","        non_ignored = targets.view(-1) != self.ignore_index\n","        targets = targets.view(-1)[non_ignored].float()\n","        outputs = outputs.contiguous().view(-1)[non_ignored]\n","        outputs = torch.clamp(outputs, eps, 1. - eps)\n","        targets = torch.clamp(targets, eps, 1. - eps)\n","        pt = (1 - targets) * (1 - outputs) + targets * outputs\n","        return (-(1. - pt) ** self.gamma * torch.log(pt)).mean()"]},{"cell_type":"markdown","metadata":{"id":"FXa807PbTjxn"},"source":["## Utils"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"7Fs2XBvfToVf","executionInfo":{"status":"ok","timestamp":1696711768389,"user_tz":180,"elapsed":492,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","#### Augmentations\n","def shift_image(img, shift_pnt):\n","    M = np.float32([[1, 0, shift_pnt[0]], [0, 1, shift_pnt[1]]])\n","    res = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]), borderMode=cv2.BORDER_REFLECT_101)\n","    return res\n","\n","\n","def rotate_image(image, angle, scale, rot_pnt):\n","    rot_mat = cv2.getRotationMatrix2D(rot_pnt, angle, scale)\n","    result = cv2.warpAffine(image, rot_mat, (image.shape[1], image.shape[0]), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101) #INTER_NEAREST\n","    return result\n","\n","\n","def gauss_noise(img, var=30):\n","    row, col, ch = img.shape\n","    mean = var\n","    sigma = var**0.5\n","    gauss = np.random.normal(mean,sigma,(row,col,ch))\n","    gauss = gauss.reshape(row,col,ch)\n","    gauss = (gauss - np.min(gauss)).astype(np.uint8)\n","    return np.clip(img.astype(np.int32) + gauss, 0, 255).astype('uint8')\n","\n","\n","def clahe(img, clipLimit=2.0, tileGridSize=(5,5)):\n","    img_yuv = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n","    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n","    img_yuv[:, :, 0] = clahe.apply(img_yuv[:, :, 0])\n","    img_output = cv2.cvtColor(img_yuv, cv2.COLOR_LAB2RGB)\n","    return img_output\n","\n","\n","def _blend(img1, img2, alpha):\n","    return np.clip(img1 * alpha + (1 - alpha) * img2, 0, 255).astype('uint8')\n","\n","\n","_alpha = np.asarray([0.114, 0.587, 0.299]).reshape((1, 1, 3))\n","def _grayscale(img):\n","    return np.sum(_alpha * img, axis=2, keepdims=True)\n","\n","\n","def saturation(img, alpha):\n","    gs = _grayscale(img)\n","    return _blend(img, gs, alpha)\n","\n","\n","def brightness(img, alpha):\n","    gs = np.zeros_like(img)\n","    return _blend(img, gs, alpha)\n","\n","\n","def contrast(img, alpha):\n","    gs = _grayscale(img)\n","    gs = np.repeat(gs.mean(), 3)\n","    return _blend(img, gs, alpha)\n","\n","\n","def change_hsv(img, h, s, v):\n","    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","    hsv = hsv.astype(int)\n","    hsv[:,:,0] += h\n","    hsv[:,:,0] = np.clip(hsv[:,:,0], 0, 255)\n","    hsv[:,:,1] += s\n","    hsv[:,:,1] = np.clip(hsv[:,:,1], 0, 255)\n","    hsv[:,:,2] += v\n","    hsv[:,:,2] = np.clip(hsv[:,:,2], 0, 255)\n","    hsv = hsv.astype('uint8')\n","    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","    return img\n","\n","def shift_channels(img, b_shift, g_shift, r_shift):\n","    img = img.astype(int)\n","    img[:,:,0] += b_shift\n","    img[:,:,0] = np.clip(img[:,:,0], 0, 255)\n","    img[:,:,1] += g_shift\n","    img[:,:,1] = np.clip(img[:,:,1], 0, 255)\n","    img[:,:,2] += r_shift\n","    img[:,:,2] = np.clip(img[:,:,2], 0, 255)\n","    img = img.astype('uint8')\n","    return img\n","\n","def invert(img):\n","    return 255 - img\n","\n","def channel_shuffle(img):\n","    ch_arr = [0, 1, 2]\n","    np.random.shuffle(ch_arr)\n","    img = img[..., ch_arr]\n","    return img\n","\n","#######\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","\n","def preprocess_inputs(x):\n","    x = np.asarray(x, dtype='float32')\n","    x /= 127\n","    x -= 1\n","    return x\n","\n","\n","def dice(im1, im2, empty_score=1.0):\n","    \"\"\"\n","    Computes the Dice coefficient, a measure of set similarity.\n","    Parameters\n","    ----------\n","    im1 : array-like, bool\n","        Any array of arbitrary size. If not boolean, will be converted.\n","    im2 : array-like, bool\n","        Any other array of identical size. If not boolean, will be converted.\n","    Returns\n","    -------\n","    dice : float\n","        Dice coefficient as a float on range [0,1].\n","        Maximum similarity = 1\n","        No similarity = 0\n","        Both are empty (sum eq to zero) = empty_score\n","\n","    Notes\n","    -----\n","    The order of inputs for `dice` is irrelevant. The result will be\n","    identical if `im1` and `im2` are switched.\n","    \"\"\"\n","    im1 = np.asarray(im1).astype(np.bool)\n","    im2 = np.asarray(im2).astype(np.bool)\n","\n","    if im1.shape != im2.shape:\n","        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n","\n","    im_sum = im1.sum() + im2.sum()\n","    if im_sum == 0:\n","        return empty_score\n","\n","    # Compute Dice coefficient\n","    intersection = np.logical_and(im1, im2)\n","\n","    return 2. * intersection.sum() / im_sum\n","\n","\n","def iou(im1, im2, empty_score=1.0):\n","    im1 = np.asarray(im1).astype(np.bool)\n","    im2 = np.asarray(im2).astype(np.bool)\n","\n","    if im1.shape != im2.shape:\n","        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n","\n","    union = np.logical_or(im1, im2)\n","    im_sum = union.sum()\n","    if im_sum == 0:\n","        return empty_score\n","\n","    # Compute Dice coefficient\n","    intersection = np.logical_and(im1, im2)\n","\n","    return intersection.sum() / im_sum"]},{"cell_type":"markdown","metadata":{"id":"Ifa7HjK2FVf3"},"source":["## modelMscale"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"RoOkAXB2n4nS","executionInfo":{"status":"ok","timestamp":1696711771288,"user_tz":180,"elapsed":2907,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import Conv2D, Activation, UpSampling2D, Concatenate, Input\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.initializers import HeNormal\n","from tensorflow.keras import backend as K\n","\n","class ConvRelu(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3):\n","        super(ConvRelu, self).__init__()\n","        self.layer = nn.Sequential(\n","            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1),\n","            nn.ReLU(inplace=True, )\n","        )\n","    #@autocast()\n","    def forward(self, x):\n","        return self.layer(x)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"sDIsasbMn5W8","executionInfo":{"status":"ok","timestamp":1696711771289,"user_tz":180,"elapsed":10,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["\"\"\"\n","ResNet code gently borrowed from\n","https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n","\"\"\"\n","\n","from collections import OrderedDict\n","import math\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils import model_zoo\n","\n","__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n","           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n","\n","pretrained_settings = {\n","    'senet154': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnet50': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnet101': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnet152': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnext50_32x4d': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnext101_32x4d': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","}\n","\n","\n","class SEModule(nn.Module):\n","\n","    def __init__(self, channels, reduction, concat=False):\n","        super(SEModule, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n","                             padding=0)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n","                             padding=0)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        module_input = x\n","        x = self.avg_pool(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.sigmoid(x)\n","        return module_input * x\n","\n","class SCSEModule(nn.Module):\n","    # according to https://arxiv.org/pdf/1808.08127.pdf concat is better\n","    def __init__(self, channels, reduction=16, concat=False):\n","        super(SCSEModule, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n","                             padding=0)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n","                             padding=0)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        self.spatial_se = nn.Sequential(nn.Conv2d(channels, 1, kernel_size=1,\n","                                                  stride=1, padding=0, bias=False),\n","                                        nn.Sigmoid())\n","        self.concat = concat\n","\n","    def forward(self, x):\n","        module_input = x\n","\n","        x = self.avg_pool(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        chn_se = self.sigmoid(x)\n","        chn_se = chn_se * module_input\n","\n","        spa_se = self.spatial_se(module_input)\n","        spa_se = module_input * spa_se\n","        if self.concat:\n","            return torch.cat([chn_se, spa_se], dim=1)\n","        else:\n","            return chn_se + spa_se\n","\n","class Bottleneck(nn.Module):\n","    \"\"\"\n","    Base class for bottlenecks that implements `forward()` method.\n","    \"\"\"\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out = self.se_module(out) + residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class SEBottleneck(Bottleneck):\n","    \"\"\"\n","    Bottleneck for SENet154.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None):\n","        super(SEBottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes * 2)\n","        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n","                               stride=stride, padding=1, groups=groups,\n","                               bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes * 4)\n","        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n","                               bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SCSEBottleneck(Bottleneck):\n","    \"\"\"\n","    Bottleneck for SENet154.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None):\n","        super(SCSEBottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes * 2)\n","        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n","                               stride=stride, padding=1, groups=groups,\n","                               bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes * 4)\n","        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n","                               bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SCSEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SEResNetBottleneck(Bottleneck):\n","    \"\"\"\n","    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n","    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n","    (the latter is used in the torchvision implementation of ResNet).\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None):\n","        super(SEResNetBottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n","                               stride=stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n","                               groups=groups, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SEResNeXtBottleneck(Bottleneck):\n","    \"\"\"\n","    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None, base_width=4):\n","        super(SEResNeXtBottleneck, self).__init__()\n","        width = math.floor(planes * (base_width / 64)) * groups\n","        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n","                               stride=1)\n","        self.bn1 = nn.BatchNorm2d(width)\n","        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n","                               padding=1, groups=groups, bias=False)\n","        self.bn2 = nn.BatchNorm2d(width)\n","        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","\n","class SCSEResNeXtBottleneck(Bottleneck):\n","    \"\"\"\n","    ResNeXt bottleneck type C with a Concurrent Spatial Squeeze-and-Excitation module.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None, base_width=4, final=False):\n","        super(SCSEResNeXtBottleneck, self).__init__()\n","        width = math.floor(planes * (base_width / 64)) * groups\n","        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n","                               stride=1)\n","        self.bn1 = nn.BatchNorm2d(width)\n","        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n","                               padding=1, groups=groups, bias=False)\n","        self.bn2 = nn.BatchNorm2d(width)\n","        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SCSEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SENet(nn.Module):\n","\n","    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n","                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n","                 downsample_padding=1, num_classes=1000):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        block (nn.Module): Bottleneck class.\n","            - For SENet154: SEBottleneck\n","            - For SE-ResNet models: SEResNetBottleneck\n","            - For SE-ResNeXt models:  SEResNeXtBottleneck\n","        layers (list of ints): Number of residual blocks for 4 layers of the\n","            network (layer1...layer4).\n","        groups (int): Number of groups for the 3x3 convolution in each\n","            bottleneck block.\n","            - For SENet154: 64\n","            - For SE-ResNet models: 1\n","            - For SE-ResNeXt models:  32\n","        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n","            - For all models: 16\n","        dropout_p (float or None): Drop probability for the Dropout layer.\n","            If `None` the Dropout layer is not used.\n","            - For SENet154: 0.2\n","            - For SE-ResNet models: None\n","            - For SE-ResNeXt models: None\n","        inplanes (int):  Number of input channels for layer1.\n","            - For SENet154: 128\n","            - For SE-ResNet models: 64\n","            - For SE-ResNeXt models: 64\n","        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n","            a single 7x7 convolution in layer0.\n","            - For SENet154: True\n","            - For SE-ResNet models: False\n","            - For SE-ResNeXt models: False\n","        downsample_kernel_size (int): Kernel size for downsampling convolutions\n","            in layer2, layer3 and layer4.\n","            - For SENet154: 3\n","            - For SE-ResNet models: 1\n","            - For SE-ResNeXt models: 1\n","        downsample_padding (int): Padding for downsampling convolutions in\n","            layer2, layer3 and layer4.\n","            - For SENet154: 1\n","            - For SE-ResNet models: 0\n","            - For SE-ResNeXt models: 0\n","        num_classes (int): Number of outputs in `last_linear` layer.\n","            - For all models: 1000\n","        \"\"\"\n","        super(SENet, self).__init__()\n","        self.inplanes = inplanes\n","        if input_3x3:\n","            layer0_modules = [\n","                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n","                                    bias=False)),\n","                ('bn1', nn.BatchNorm2d(64)),\n","                ('relu1', nn.ReLU(inplace=True)),\n","                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n","                                    bias=False)),\n","                ('bn2', nn.BatchNorm2d(64)),\n","                ('relu2', nn.ReLU(inplace=True)),\n","                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n","                                    bias=False)),\n","                ('bn3', nn.BatchNorm2d(inplanes)),\n","                ('relu3', nn.ReLU(inplace=True)),\n","            ]\n","        else:\n","            layer0_modules = [\n","                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n","                                    padding=3, bias=False)),\n","                ('bn1', nn.BatchNorm2d(inplanes)),\n","                ('relu1', nn.ReLU(inplace=True)),\n","            ]\n","        # To preserve compatibility with Caffe weights `ceil_mode=True`\n","        # is used instead of `padding=1`.\n","        self.pool = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n","        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n","        self.layer1 = self._make_layer(\n","            block,\n","            planes=64,\n","            blocks=layers[0],\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=1,\n","            downsample_padding=0\n","        )\n","        self.layer2 = self._make_layer(\n","            block,\n","            planes=128,\n","            blocks=layers[1],\n","            stride=2,\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=downsample_kernel_size,\n","            downsample_padding=downsample_padding\n","        )\n","        self.layer3 = self._make_layer(\n","            block,\n","            planes=256,\n","            blocks=layers[2],\n","            stride=2,\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=downsample_kernel_size,\n","            downsample_padding=downsample_padding\n","        )\n","        self.layer4 = self._make_layer(\n","            block,\n","            planes=512,\n","            blocks=layers[3],\n","            stride=2,\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=downsample_kernel_size,\n","            downsample_padding=downsample_padding\n","        )\n","        self.avg_pool = nn.AvgPool2d(7, stride=1)\n","        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n","        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n","        self._initialize_weights()\n","\n","    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n","                    downsample_kernel_size=1, downsample_padding=0):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=downsample_kernel_size, stride=stride,\n","                          padding=downsample_padding, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n","                            downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, groups, reduction))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","                m.weight.data = nn.init.kaiming_normal_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def features(self, x):\n","        x = self.layer0(x)\n","        x = self.pool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        return x\n","\n","    def logits(self, x):\n","        x = self.avg_pool(x)\n","        if self.dropout is not None:\n","            x = self.dropout(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.last_linear(x)\n","        return x\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.logits(x)\n","        return x\n","\n","\n","def initialize_pretrained_model(model, num_classes, settings):\n","    assert num_classes == settings['num_classes'], \\\n","        'num_classes should be {}, but is {}'.format(\n","            settings['num_classes'], num_classes)\n","    model.load_state_dict(model_zoo.load_url(settings['url']), strict=False)\n","    model.input_space = settings['input_space']\n","    model.input_size = settings['input_size']\n","    model.input_range = settings['input_range']\n","    model.mean = settings['mean']\n","    model.std = settings['std']\n","\n","\n","def senet154(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n","                  dropout_p=0.2, num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['senet154'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","def scsenet154(num_classes=1000, pretrained='imagenet'):\n","    print(\"scsenet154\")\n","    model = SENet(SCSEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n","                  dropout_p=0.2, num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['senet154'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnet50(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnet50'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnet101(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnet101'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnet152(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnet152'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def scse_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SCSEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"YEmiREpPoCWI","executionInfo":{"status":"ok","timestamp":1696711771796,"user_tz":180,"elapsed":515,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["class SeResNext50_Unet_2Ssum(nn.Module):\n","    def __init__(self, pretrained='imagenet', **kwargs):\n","        super(SeResNext50_Unet_2Ssum, self).__init__()\n","\n","        encoder_filters = [64, 256, 512, 1024, 2048]\n","        decoder_filters = np.asarray([64, 96, 128, 256, 512]) // 2\n","\n","        self.conv6 = ConvRelu(encoder_filters[-1], decoder_filters[-1])\n","        self.conv6_2 = ConvRelu(decoder_filters[-1] + encoder_filters[-2], decoder_filters[-1])\n","        self.conv7 = ConvRelu(decoder_filters[-1], decoder_filters[-2])\n","        self.conv7_2 = ConvRelu(decoder_filters[-2] + encoder_filters[-3] , decoder_filters[-2] )\n","        self.conv8 = ConvRelu(decoder_filters[-2], decoder_filters[-3])\n","        self.conv8_2 = ConvRelu(decoder_filters[-3] + encoder_filters[-4] , decoder_filters[-3])\n","        self.conv9 = ConvRelu(decoder_filters[-3], decoder_filters[-4])\n","        self.conv9_2 = ConvRelu(decoder_filters[-4] + encoder_filters[-5], decoder_filters[-4])\n","\n","        # self.convx9_3 = ConvRelu(encoder_filters[-4], encoder_filters[-4])\n","\n","\n","        self.conv10 = ConvRelu(decoder_filters[-4], decoder_filters[-5])\n","        self.conv10_s = nn.Sequential(ConvRelu(decoder_filters[-5], decoder_filters[-5]),\n","                                      nn.Conv2d(decoder_filters[-5] , 1, 1, stride=1, padding=0),\n","                                      nn.Sigmoid())\n","        # self.convxx = nn.Sequential(ConvRelu(decoder_filters[-5]*2, decoder_filters[-5]*2),\n","        #                             nn.Conv2d(decoder_filters[-5] * 2, decoder_filters[-5], 1, stride=1, padding=0))\n","\n","        self.res = nn.Conv2d(decoder_filters[-5] * 2, 5, 1, stride=1, padding=0)\n","\n","        self._initialize_weights()\n","\n","        encoder = se_resnext50_32x4d(pretrained=pretrained)\n","        #encoder = torchvision.models.resnet50(pretrained=pretrained)\n","        self.conv1 = nn.Sequential(encoder.layer0.conv1, encoder.layer0.bn1, encoder.layer0.relu1) #encoder.layer0.conv1\n","        self.conv2 = nn.Sequential(encoder.pool, encoder.layer1)\n","        self.conv3 = encoder.layer2\n","        self.conv4 = encoder.layer3\n","        self.conv5 = encoder.layer4\n","\n","\n","    def forward1(self, x):\n","        batch_size, C, H, W = x.shape\n","        xx = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n","\n","        enc1 = self.conv1(x)\n","        enc2 = self.conv2(enc1)\n","        enc3 = self.conv3(enc2)\n","        enc4 = self.conv4(enc3)\n","        enc5 = self.conv5(enc4)\n","\n","        encx1 = self.conv1(xx)   # 64 128 128\n","        encx2 = self.conv2(encx1) # 64\n","        encx3 = self.conv3(encx2) # 32\n","        encx4 = self.conv4(encx3) # 16\n","        encx5 = self.conv5(encx4) # 8\n","\n","        dec6 = self.conv6(F.interpolate(enc5, scale_factor=2))\n","        dec6 = self.conv6_2(torch.cat([dec6, enc4 ], 1))\n","        dec7 = self.conv7(F.interpolate(dec6, scale_factor=2))\n","        dec7 = self.conv7_2(torch.cat([dec7, enc3], 1))\n","        dec8 = self.conv8(F.interpolate(dec7, scale_factor=2))\n","        dec8 = self.conv8_2(torch.cat([dec8, enc2], 1))\n","        dec9 = self.conv9(F.interpolate(dec8, scale_factor=2))\n","        dec9 = self.conv9_2(torch.cat([dec9,  enc1], 1))   #256\n","\n","        decx6 = self.conv6(F.interpolate(encx5, scale_factor=2))\n","        decx6 = self.conv6_2(torch.cat([decx6, encx4 ], 1))\n","        decx7 = self.conv7(F.interpolate(decx6, scale_factor=2))\n","        decx7 = self.conv7_2(torch.cat([decx7, encx3], 1))\n","        decx8 = self.conv8(F.interpolate(decx7, scale_factor=2))\n","        decx8 = self.conv8_2(torch.cat([decx8, encx2], 1))\n","        decx9 = self.conv9(F.interpolate(decx8, scale_factor=2))\n","        decx9 = self.conv9_2(torch.cat([decx9,  encx1], 1))   #128\n","        #decx9 = self.convx9_3(F.interpolate(decx9, scale_factor=4))\n","\n","\n","        dec10 = self.conv10(F.interpolate(dec9, scale_factor=2))\n","        alpha = self.conv10_s(dec10)\n","        decx10 = self.conv10(F.interpolate(decx9, scale_factor=4))\n","\n","        dec = alpha * dec10 + (1-alpha)*decx10\n","\n","        return dec\n","\n","    def forward(self, x):\n","        dec10_0 = self.forward1(x[:, :3, :, :])\n","        dec10_1 = self.forward1(x[:, 3:, :, :])\n","\n","        dec10 = torch.cat([dec10_0, dec10_1], 1)\n","\n","        return self.res(dec10)\n","\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n","                m.weight.data = nn.init.kaiming_normal_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"6LDy5nU9FYrM","executionInfo":{"status":"ok","timestamp":1696711771796,"user_tz":180,"elapsed":4,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","def create_square_kernel(size):\n","    return torch.ones((1, 1, size, size))\n","\n","def create_circle_kernel(size):\n","    y, x = np.ogrid[-(size-1)//2:(size-1)//2+1, -(size-1)//2:(size-1)//2+1]\n","    mask = x*x + y*y <= ((size-1)//2)**2\n","    return torch.Tensor(mask.astype(float)).view(1, 1, size, size)\n","\n","class MorphologicalLayer(nn.Module):\n","    def __init__(self, in_channels, kernel_size=3, shape='square', morph_op='erosion'):\n","        super(MorphologicalLayer, self).__init__()\n","        self.kernel_size = kernel_size\n","        self.padding = kernel_size // 2\n","        self.morph_op = morph_op\n","\n","        if shape == 'square':\n","            self.kernel = create_square_kernel(kernel_size)\n","        elif shape == 'circle':\n","            self.kernel = create_circle_kernel(kernel_size)\n","        else:\n","            raise ValueError(\"Invalid shape\")\n","\n","        # Repita o kernel para que ele tenha o mesmo número de canais que a entrada\n","        self.kernel = self.kernel.repeat(in_channels, 1, 1, 1)\n","\n","        self.kernel = nn.Parameter(self.kernel, requires_grad=True)\n","\n","    def forward(self, x):\n","        if self.morph_op == 'erosion':\n","            return F.conv2d(-x, self.kernel, padding=self.padding, groups=x.size(1)) * -1\n","        elif self.morph_op == 'dilation':\n","            return F.conv2d(x, self.kernel, padding=self.padding, groups=x.size(1))\n","        elif self.morph_op == 'opening':\n","            return F.conv2d(x, self.kernel, padding=self.padding, groups=x.size(1)) * -1\n","        elif self.morph_op == 'closing':\n","            return F.conv2d(-x, self.kernel, padding=self.padding, groups=x.size(1))\n","        else:\n","            raise ValueError(\"Invalid morphological operation\")"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"qVNa9uDwFOx8","executionInfo":{"status":"ok","timestamp":1696711771796,"user_tz":180,"elapsed":3,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["# Testar operações acontecendo ao mesmo tempo\n","# Variar tamanho das janelas\n","# Mudar elemento estruturante (quadrado ou circulo)\n","\n","# Testar randon search\n","# Fazer testes curtos (Com 200 ou 100 imagens)"]},{"cell_type":"markdown","metadata":{"id":"tHCAM2vfRK7e"},"source":["## LoadData"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155636,"status":"ok","timestamp":1696711927429,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"Un2B5Ov0F8f-","outputId":"ca8705a5-ca5d-423a-b360-d5cf5c313d3e"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [02:22<00:00, 71.38s/it]\n","100%|██████████| 1/1 [00:11<00:00, 11.35s/it]\n"]}],"source":["import os\n","os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n","os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n","os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n","\n","from os import path, makedirs, listdir\n","import sys\n","import numpy as np\n","np.random.seed(1)\n","import random\n","random.seed(1)\n","\n","import torch\n","from torch import nn\n","from torch.backends import cudnn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.optim.lr_scheduler as lr_scheduler\n","\n","#from apex import amp\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import timeit\n","import cv2\n","\n","from imgaug import augmenters as iaa\n","\n","from skimage.morphology import square, dilation\n","\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import accuracy_score\n","\n","import gc\n","\n","cv2.setNumThreads(0)\n","cv2.ocl.setUseOpenCL(False)\n","\n","train_dirs = ['/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/tier3','/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/train']\n","\n","val_dirs = ['/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/test']\n","\n","models_folder = '/content/drive/MyDrive/Modeling Satelities Images Building Damaged/src/train/weights'\n","\n","loc_folder = 'pred_loc_val'\n","\n","input_shape = (512, 512)\n","\n","\n","all_files = []\n","for d in tqdm(train_dirs):\n","    for f in sorted(listdir(path.join(d, 'images'))):\n","        if '_pre_disaster.png' in f:\n","            post_disaster_file = f.replace('_pre_disaster.png', '_post_disaster.png')\n","            if path.exists(path.join(d, 'images', post_disaster_file)):\n","                all_files.append(path.join(d, 'images', f))\n","\n","all_files2 = []\n","for d in tqdm(val_dirs):\n","    for f in sorted(listdir(path.join(d, 'images'))):\n","        if '_pre_disaster.png' in f:\n","            post_disaster_file = f.replace('_pre_disaster.png', '_post_disaster.png')\n","            if path.exists(path.join(d, 'images', post_disaster_file)):\n","                all_files2.append(path.join(d, 'images', f))"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"3_V8N6ykQ2g0","executionInfo":{"status":"ok","timestamp":1696711927429,"user_tz":180,"elapsed":12,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["def rand_bbox(size, lam):\n","    W = size[0]\n","    H = size[1]\n","    cut_rat = np.sqrt(1. - lam)\n","    cut_w = int(W * cut_rat)\n","    cut_h = int(H * cut_rat)\n","    # if cut_rat > 0.9:\n","    #   cut_w = np.int(W * cut_rat * 0.9)\n","    #   cut_h = np.int(H * cut_rat * 0.9)\n","\n","    # uniform\n","    cx = np.random.randint(W)\n","    cy = np.random.randint(H)\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n","\n","    return bbx1, bby1, bbx2, bby2\n","\n","class TrainData(Dataset):\n","    def __init__(self, train_idxs, low, high):\n","        super().__init__()\n","        self.train_idxs = train_idxs\n","        self.elastic = iaa.ElasticTransformation(alpha=(0.25, 1.2), sigma=0.2)\n","        self.low =low\n","        self.high = high\n","\n","    def __len__(self):\n","        return len(self.train_idxs)\n","\n","    def __getitem__(self, idx):\n","        _idx = self.train_idxs[idx]\n","\n","        fn = all_files[_idx]\n","\n","        img = cv2.imread(fn, cv2.IMREAD_COLOR)\n","        img2 = cv2.imread(fn.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","        if img is None and img2 is None:\n","          sample = {'img': None, 'msk': None, 'lbl_msk': None, 'fn': None}\n","        elif img is None or img2 is None:\n","          sample = {'img': None, 'msk': None, 'lbl_msk': None, 'fn': None}\n","\n","        msk0 = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        lbl_msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        msk1 = np.zeros_like(lbl_msk1)\n","        msk2 = np.zeros_like(lbl_msk1)\n","        msk3 = np.zeros_like(lbl_msk1)\n","        msk4 = np.zeros_like(lbl_msk1)\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","        msk1[lbl_msk1 == 1] = 255\n","\n","        try:\n","          if random.random() > 0.87:\n","              lam = np.random.beta(2, 1.8)\n","              rand_inx = torch.randint(low=self.low,high=self.high,size=(1,))\n","              ttt = self.train_idxs[rand_inx]\n","              fn_rand = all_files[ttt]\n","              img_random = cv2.imread(fn_rand, cv2.IMREAD_COLOR)\n","              img2_random = cv2.imread(fn_rand.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","              msk0_random = cv2.imread(fn_rand.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","              lbl_msk1_random = cv2.imread(fn_rand.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","              bbx1, bby1, bbx2, bby2 = rand_bbox((1024, 1024), lam)\n","              img[bbx1:bbx2, bby1:bby2, :] = img_random[bbx1:bbx2, bby1:bby2, :]\n","              img2[bbx1:bbx2, bby1:bby2, :] = img2_random[bbx1:bbx2, bby1:bby2, :]\n","              msk0[bbx1:bbx2, bby1:bby2] = msk0_random[bbx1:bbx2, bby1:bby2]\n","              lbl_msk1[bbx1:bbx2, bby1:bby2] = lbl_msk1_random[bbx1:bbx2, bby1:bby2]\n","              # lbl_msk1[lbl_msk1==1]=70\n","              # lbl_msk1[lbl_msk1==2]=130\n","              # lbl_msk1[lbl_msk1==3]=190\n","              # lbl_msk1[lbl_msk1==4]=255\n","              # cv2.imshow('input_image', lbl_msk1)\n","              # cv2.waitKey(5000)\n","        except:\n","          None\n","\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","        msk1[lbl_msk1 == 1] = 255\n","\n","        try:\n","          if random.random() > 0.5:\n","              img = img[::-1, ...]\n","              img2 = img2[::-1, ...]\n","              msk0 = msk0[::-1, ...]\n","              msk1 = msk1[::-1, ...]\n","              msk2 = msk2[::-1, ...]\n","              msk3 = msk3[::-1, ...]\n","              msk4 = msk4[::-1, ...]\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.05:\n","              rot = random.randrange(4)\n","              if rot > 0:\n","                img = np.rot90(img, k=rot)\n","                img2 = np.rot90(img2, k=rot)\n","                msk0 = np.rot90(msk0, k=rot)\n","                msk1 = np.rot90(msk1, k=rot)\n","                msk2 = np.rot90(msk2, k=rot)\n","                msk3 = np.rot90(msk3, k=rot)\n","                msk4 = np.rot90(msk4, k=rot)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.8:\n","              shift_pnt = (random.randint(-320, 320), random.randint(-320, 320))\n","              img = shift_image(img, shift_pnt)\n","              img2 = shift_image(img2, shift_pnt)\n","              msk0 = shift_image(msk0, shift_pnt)\n","              msk1 = shift_image(msk1, shift_pnt)\n","              msk2 = shift_image(msk2, shift_pnt)\n","              msk3 = shift_image(msk3, shift_pnt)\n","              msk4 = shift_image(msk4, shift_pnt)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.2:\n","              rot_pnt =  (img.shape[0] // 2 + random.randint(-320, 320), img.shape[1] // 2 + random.randint(-320, 320))\n","              scale = 0.9 + random.random() * 0.2\n","              angle = random.randint(0, 20) - 10\n","              if (angle != 0) or (scale != 1):\n","                  img = rotate_image(img, angle, scale, rot_pnt)\n","                  img2 = rotate_image(img2, angle, scale, rot_pnt)\n","                  msk0 = rotate_image(msk0, angle, scale, rot_pnt)\n","                  msk1 = rotate_image(msk1, angle, scale, rot_pnt)\n","                  msk2 = rotate_image(msk2, angle, scale, rot_pnt)\n","                  msk3 = rotate_image(msk3, angle, scale, rot_pnt)\n","                  msk4 = rotate_image(msk4, angle, scale, rot_pnt)\n","        except:\n","          None\n","\n","        crop_size = input_shape[0]\n","\n","        try:\n","          if random.random() > 0.1:\n","              crop_size = random.randint(int(input_shape[0] / 1.15), int(input_shape[0] / 0.85))\n","        except:\n","          None\n","\n","        try:\n","          bst_x0 = random.randint(0, img.shape[1] - crop_size)\n","          bst_y0 = random.randint(0, img.shape[0] - crop_size)\n","          bst_sc = -1\n","          try_cnt = random.randint(1, 10)\n","          for i in range(try_cnt):\n","              x0 = random.randint(0, img.shape[1] - crop_size)\n","              y0 = random.randint(0, img.shape[0] - crop_size)\n","              _sc = msk2[y0:y0+crop_size, x0:x0+crop_size].sum() * 5 + msk3[y0:y0+crop_size, x0:x0+crop_size].sum() * 5 + msk4[y0:y0+crop_size, x0:x0+crop_size].sum() * 2 + msk1[y0:y0+crop_size, x0:x0+crop_size].sum()\n","              if _sc > bst_sc:\n","                  bst_sc = _sc\n","                  bst_x0 = x0\n","                  bst_y0 = y0\n","          x0 = bst_x0\n","          y0 = bst_y0\n","          img = img[y0:y0+crop_size, x0:x0+crop_size, :]\n","          img2 = img2[y0:y0+crop_size, x0:x0+crop_size, :]\n","          msk0 = msk0[y0:y0+crop_size, x0:x0+crop_size]\n","          msk1 = msk1[y0:y0+crop_size, x0:x0+crop_size]\n","          msk2 = msk2[y0:y0+crop_size, x0:x0+crop_size]\n","          msk3 = msk3[y0:y0+crop_size, x0:x0+crop_size]\n","          msk4 = msk4[y0:y0+crop_size, x0:x0+crop_size]\n","\n","          if crop_size != input_shape[0]:\n","            img = cv2.resize(img, input_shape, interpolation=cv2.INTER_LINEAR)\n","            img2 = cv2.resize(img2, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk0 = cv2.resize(msk0, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk1 = cv2.resize(msk1, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk2 = cv2.resize(msk2, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk3 = cv2.resize(msk3, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk4 = cv2.resize(msk4, input_shape, interpolation=cv2.INTER_LINEAR)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.96:\n","              img = shift_channels(img, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","          elif random.random() > 0.96:\n","              img2 = shift_channels(img2, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","\n","          if random.random() > 0.96:\n","              img = change_hsv(img, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","          elif random.random() > 0.96:\n","              img2 = change_hsv(img2, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img = clahe(img)\n","              elif random.random() > 0.96:\n","                  img = gauss_noise(img)\n","              elif random.random() > 0.96:\n","                  img = cv2.blur(img, (3, 3))\n","          elif random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img = saturation(img, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img = brightness(img, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img = contrast(img, 0.9 + random.random() * 0.2)\n","\n","          if random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img2 = clahe(img2)\n","              elif random.random() > 0.96:\n","                  img2 = gauss_noise(img2)\n","              elif random.random() > 0.96:\n","                  img2 = cv2.blur(img2, (3, 3))\n","          elif random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img2 = saturation(img2, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img2 = brightness(img2, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img2 = contrast(img2, 0.9 + random.random() * 0.2)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.96:\n","              el_det = self.elastic.to_deterministic()\n","              img = el_det.augment_image(img)\n","\n","          if random.random() > 0.96:\n","              el_det = self.elastic.to_deterministic()\n","              img2 = el_det.augment_image(img2)\n","        except:\n","          None\n","\n","        msk0 = msk0[..., np.newaxis]\n","        msk1 = msk1[..., np.newaxis]\n","        msk2 = msk2[..., np.newaxis]\n","        msk3 = msk3[..., np.newaxis]\n","        msk4 = msk4[..., np.newaxis]\n","\n","        msk = np.concatenate([msk0, msk1, msk2, msk3, msk4], axis=2)\n","        msk = (msk > 127)\n","\n","        msk[..., 0] = True\n","        msk[..., 1] = dilation(msk[..., 1], square(5))\n","        msk[..., 2] = dilation(msk[..., 2], square(5))\n","        msk[..., 3] = dilation(msk[..., 3], square(5))\n","        msk[..., 4] = dilation(msk[..., 4], square(5))\n","        msk[..., 1][msk[..., 2:].max(axis=2)] = False\n","        msk[..., 3][msk[..., 2]] = False\n","        msk[..., 4][msk[..., 2]] = False\n","        msk[..., 4][msk[..., 3]] = False\n","        msk[..., 0][msk[..., 1:].max(axis=2)] = False\n","        msk = msk * 1\n","\n","        lbl_msk = msk.argmax(axis=2)\n","\n","        img = np.concatenate([img, img2], axis=2)\n","        img = preprocess_inputs(img)\n","\n","        img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n","        msk = torch.from_numpy(msk.transpose((2, 0, 1))).long()\n","\n","        sample = {'img': img, 'msk': msk, 'lbl_msk': lbl_msk, 'fn': fn}\n","        return sample\n","\n","\n","class ValData(Dataset):\n","    def __init__(self, image_idxs):\n","        super().__init__()\n","        self.image_idxs = image_idxs\n","\n","    def __len__(self):\n","        return len(self.image_idxs)\n","\n","    def __getitem__(self, idx):\n","        _idx = self.image_idxs[idx]\n","\n","        fn = all_files2[_idx]\n","\n","        img = cv2.imread(fn, cv2.IMREAD_COLOR)\n","        img2 = cv2.imread(fn.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","        msk_loc = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)# > (0.3*255)\n","\n","        msk0 = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        lbl_msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        msk1 = np.zeros_like(lbl_msk1)\n","        msk2 = np.zeros_like(lbl_msk1)\n","        msk3 = np.zeros_like(lbl_msk1)\n","        msk4 = np.zeros_like(lbl_msk1)\n","        msk1[lbl_msk1 == 1] = 255\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","\n","        msk0 = msk0[..., np.newaxis]\n","        msk1 = msk1[..., np.newaxis]\n","        msk2 = msk2[..., np.newaxis]\n","        msk3 = msk3[..., np.newaxis]\n","        msk4 = msk4[..., np.newaxis]\n","\n","        msk = np.concatenate([msk0, msk1, msk2, msk3, msk4], axis=2)\n","        msk = (msk > 127)\n","\n","        msk = msk * 1\n","\n","        lbl_msk = msk[..., 1:].argmax(axis=2)\n","\n","        img = np.concatenate([img, img2], axis=2)\n","        img = preprocess_inputs(img)\n","\n","        img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n","        msk = torch.from_numpy(msk.transpose((2, 0, 1))).long()\n","\n","        sample = {'img': img, 'msk': msk, 'lbl_msk': lbl_msk, 'fn': fn, 'msk_loc': msk_loc}\n","        return sample\n","\n","\n","def validate(net, data_loader):\n","    dices0 = []\n","\n","    tp = np.zeros((5,))\n","    fp = np.zeros((5,))\n","    fn = np.zeros((5,))\n","\n","    _thr = 0.3\n","\n","    with torch.no_grad():\n","        for i, sample in enumerate(tqdm(data_loader)):\n","            msks = sample[\"msk\"].numpy()\n","            lbl_msk = sample[\"lbl_msk\"].numpy()\n","            imgs = sample[\"img\"].cuda(non_blocking=True)\n","            msk_loc = sample[\"msk_loc\"].numpy() * 1\n","            out = model(imgs)\n","\n","            msk_pred = msk_loc\n","            msk_damage_pred = torch.softmax(out, dim=1).cpu().numpy()[:, 1:, ...]\n","\n","            for j in range(msks.shape[0]):\n","                tp[4] += np.logical_and(msks[j, 0] > 0, msk_pred[j] > 0).sum()\n","                fn[4] += np.logical_and(msks[j, 0] < 1, msk_pred[j] > 0).sum()\n","                fp[4] += np.logical_and(msks[j, 0] > 0, msk_pred[j] < 1).sum()\n","\n","\n","                targ = lbl_msk[j][msks[j, 0] > 0]\n","                pred = msk_damage_pred[j].argmax(axis=0)\n","                pred = pred * (msk_pred[j] > _thr)\n","                pred = pred[msks[j, 0] > 0]\n","                for c in range(4):\n","                    tp[c] += np.logical_and(pred == c, targ == c).sum()\n","                    fn[c] += np.logical_and(pred != c, targ == c).sum()\n","                    fp[c] += np.logical_and(pred == c, targ != c).sum()\n","\n","    d0 = 2 * tp[4] / (2 * tp[4] + fp[4] + fn[4])\n","\n","    f1_sc = np.zeros((4,))\n","    for c in range(4):\n","        f1_sc[c] = 2 * tp[c] / (2 * tp[c] + fp[c] + fn[c])\n","\n","    f1 = 4 / np.sum(1.0 / (f1_sc + 1e-6))\n","\n","    sc = 0.3 * d0 + 0.7 * f1\n","    print(\"Val Score: {}, Dice: {}, F1: {}, F1_0: {}, F1_1: {}, F1_2: {}, F1_3: {}\".format(sc, d0, f1, f1_sc[0], f1_sc[1], f1_sc[2], f1_sc[3]))\n","    return [sc, d0, f1, f1_sc[0], f1_sc[1], f1_sc[2], f1_sc[3]]\n","\n","\n","def evaluate_val(data_val, best_score, model, snapshot_name, current_epoch):\n","    model = model.eval()\n","    d = validate(model, data_loader=data_val)\n","    score = d[0]\n","\n","    if score > best_score:\n","        torch.save({\n","            'epoch': current_epoch + 1,\n","            'state_dict': model.state_dict(),\n","            'best_score': d,\n","        }, path.join(models_folder, snapshot_name + '_best'))\n","        best_score = score\n","\n","    print(\"score: {}\\tscore_best: {}\".format(d, best_score))\n","    return d[0],d\n","\n","\n","def train_epoch(current_epoch, seg_loss, ce_loss, seg_seesaw, model, optimizer, scheduler, train_data_loader):\n","    losses = AverageMeter()\n","    losses1 = AverageMeter()\n","\n","    dices = AverageMeter()\n","\n","    iterator = tqdm(train_data_loader)\n","    model.train()\n","    for i, sample in enumerate(iterator):\n","        imgs = sample[\"img\"].cuda(non_blocking=True)\n","        msks = sample[\"msk\"].cuda(non_blocking=True)\n","        lbl_msk = sample[\"lbl_msk\"].cuda(non_blocking=True)\n","\n","        out = model(imgs)\n","\n","        # loss0 = seg_loss(out[:, 0, ...], msks[:, 0, ...])\n","        # loss1 = seg_loss(out[:, 1, ...], msks[:, 1, ...])\n","        # loss2 = seg_loss(out[:, 2, ...], msks[:, 2, ...])\n","        # loss3 = seg_loss(out[:, 3, ...], msks[:, 3, ...])\n","        # loss4 = seg_loss(out[:, 4, ...], msks[:, 4, ...])\n","\n","        loss5 = ce_loss(out, lbl_msk)\n","        #loss5 = seg_seesaw(out, lbl_msk)\n","        loss = loss5\n","        #loss = 0.1 * loss0 + 0.1 * loss1 + 0.3 * loss2 + 0.3 * loss3 + 0.2 * loss4 + loss5 * 2\n","\n","        with torch.no_grad():\n","            _probs = 1 - torch.sigmoid(out[:, 0, ...])\n","            dice_sc = 1 - dice_round(_probs, 1 - msks[:, 0, ...])\n","\n","        losses.update(loss.item(), imgs.size(0))\n","        losses1.update(loss5.item(), imgs.size(0))\n","\n","        dices.update(dice_sc, imgs.size(0))\n","\n","        iterator.set_description(\n","            \"epoch: {}; lr {:.7f}; Loss {loss.val:.4f} ({loss.avg:.4f}); cce_loss {loss1.val:.4f} ({loss1.avg:.4f}); Dice {dice.val:.4f} ({dice.avg:.4f})\".format(\n","                current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","\n","        optimizer.zero_grad()\n","        # loss.backward()\n","        with amp.scale_loss(loss, optimizer) as scaled_loss:\n","            scaled_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), 0.999)\n","        optimizer.step()\n","\n","    scheduler.step()\n","\n","    print(\"epoch: {}; lr {:.7f}; Loss {loss.avg:.4f}; CCE_loss {loss1.avg:.4f}; Dice {dice.avg:.4f}\".format(\n","            current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","    return scheduler.get_lr()[-1], losses, losses1, dices\n","\n","def train_epoch(current_epoch, seg_loss, ce_loss, seg_seesaw, model, optimizer, scheduler, train_data_loader):\n","    losses = AverageMeter()\n","    losses1 = AverageMeter()\n","\n","    dices = AverageMeter()\n","\n","    iterator = tqdm(train_data_loader)\n","    model.train()\n","    for i, sample in enumerate(iterator):\n","        if sample[\"img\"] is None or sample[\"msk\"] is None or sample[\"lbl_msk\"] is None:\n","              continue\n","        imgs = sample[\"img\"].cuda(non_blocking=True)\n","        msks = sample[\"msk\"].cuda(non_blocking=True)\n","        lbl_msk = sample[\"lbl_msk\"].cuda(non_blocking=True)\n","\n","        out = model(imgs)\n","\n","        loss5 = ce_loss(out, lbl_msk)\n","        loss = loss5\n","\n","        with torch.no_grad():\n","            _probs = 1 - torch.sigmoid(out[:, 0, ...])\n","            dice_sc = 1 - dice_round(_probs, 1 - msks[:, 0, ...])\n","\n","        losses.update(loss.item(), imgs.size(0))\n","        losses1.update(loss5.item(), imgs.size(0))\n","\n","        dices.update(dice_sc, imgs.size(0))\n","\n","        iterator.set_description(\n","            \"epoch: {}; lr {:.7f}; Loss {loss.val:.4f} ({loss.avg:.4f}); cce_loss {loss1.val:.4f} ({loss1.avg:.4f}); Dice {dice.val:.4f} ({dice.avg:.4f})\".format(\n","                current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.999)\n","        optimizer.step()\n","\n","    scheduler.step()\n","\n","    print(\"epoch: {}; lr {:.7f}; Loss {loss.avg:.4f}; CCE_loss {loss1.avg:.4f}; Dice {dice.avg:.4f}\".format(\n","            current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","    return scheduler.get_lr()[-1], losses, losses1, dices"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AL3hRfYVR6t7","executionInfo":{"status":"ok","timestamp":1696718255688,"user_tz":180,"elapsed":6328270,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"9490e6c2-2978-44f7-9b1e-e97e75d9da5f"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9162/9162 [1:33:56<00:00,  1.63it/s]\n","100%|██████████| 906/906 [11:30<00:00,  1.31it/s]\n","100%|██████████| 906/906 [00:00<00:00, 1700241.35it/s]\n","100%|██████████| 9070/9070 [00:00<00:00, 445935.80it/s]\n","100%|██████████| 9070/9070 [00:00<00:00, 417208.66it/s]\n"]}],"source":["# ttt = np.asarray([True, False, True, True])\n","# p1=ttt[0:].max()\n","# p2=ttt[1].max()\n","# ttt2 = np.asarray([True, True, False, False])\n","# p3=ttt2[1].max()\n","# p4=ttt2[2:3].max()\n","t0 = timeit.default_timer()\n","\n","makedirs(models_folder, exist_ok=True)\n","\n","seed = 13\n","#seed=0\n","# vis_dev = sys.argv[2]\n","\n","# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = vis_dev\n","\n","cudnn.benchmark = True\n","\n","batch_size = 12\n","val_batch_size = 10\n","\n","snapshot_name = 'res50_cls_2Ssum_{}_0'.format(seed)\n","\n","file_classes = []\n","for fn in tqdm(all_files):\n","    fl = np.zeros((4,), dtype=bool)\n","    msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","    for c in range(1, 5):\n","        fl[c-1] = c in msk1\n","    file_classes.append(fl)\n","file_classes = np.asarray(file_classes)\n","\n","file_classes2 = []\n","for fn in tqdm(all_files2):\n","    fl = np.zeros((4,), dtype=bool)\n","    msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","    for c in range(1, 5):\n","        fl[c-1] = c in msk1\n","    file_classes2.append(fl)\n","file_classes2 = np.asarray(file_classes2)\n","\n","train_idxs0, val_idxs = train_test_split(np.arange(len(all_files)), test_size=0.01, random_state=seed)\n","\n","val_idxs0 = np.arange(len(all_files2))\n","\n","val_idxs = []\n","for i in tqdm(val_idxs0):\n","    val_idxs.append(i)\n","\n","np.random.seed(seed + 1234)\n","random.seed(seed + 1234)\n","\n","train_idxs = []\n","for i in tqdm(train_idxs0):\n","    train_idxs.append(i)\n","    if file_classes[i, 1:].max():\n","        train_idxs.append(i)\n","    # if file_classes[i, 2].max():\n","    #     train_idxs.append(i)\n","low1 = len(train_idxs)\n","for i in tqdm(train_idxs0):\n","    if file_classes[i, 1:3].max():\n","        train_idxs.append(i)\n","# for i in train_idxs0:\n","#     if file_classes[i, 1].max():\n","#         train_idxs.append(i)\n","high1 = len(train_idxs)\n","\n","train_idxs = np.asarray(train_idxs)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZxguzzcmqDBX","executionInfo":{"status":"ok","timestamp":1696718255688,"user_tz":180,"elapsed":38,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"0c96ebde-3720-45f0-b9fa-18b3b9de2a48"},"outputs":[{"output_type":"stream","name":"stdout","text":["steps_per_epoch 1227 validation_steps 90\n"]}],"source":["steps_per_epoch = int(len(train_idxs) // batch_size)\n","validation_steps = int(len(val_idxs) // val_batch_size)\n","\n","print('steps_per_epoch', steps_per_epoch, 'validation_steps', validation_steps)\n","\n","data_train = TrainData(train_idxs, low1, high1)\n","val_train = ValData(val_idxs)\n","\n","train_data_loader = DataLoader(data_train, batch_size=int(batch_size), num_workers=6, shuffle=True, pin_memory=False, drop_last=True)\n","val_data_loader = DataLoader(val_train, batch_size=int(val_batch_size), num_workers=6, shuffle=False, pin_memory=False)"]},{"cell_type":"markdown","metadata":{"id":"T-rqSugmZwEp"},"source":["## Train"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"UIavFyXpaPYX","executionInfo":{"status":"ok","timestamp":1696718255688,"user_tz":180,"elapsed":17,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["from itertools import product\n","import random\n","\n","# Camadas candidatas para adicionar operações morfológicas.\n","candidate_layers = ['after_conv3', 'after_conv4', 'after_conv5']\n","\n","# Tipos de operações morfológicas para experimentar.\n","morph_ops = ['erosion', 'dilation', 'opening', 'closing']\n","\n","# Tamanhos de kernel para experimentar.\n","kernel_sizes = [3, 5, 7]\n","\n","# Tipos de elementos estruturantes.\n","shapes = ['square', 'circle']\n","\n","# Número de configurações aleatórias para testar.\n","num_random_configs = 72"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"rhk9LcAhi_s4","executionInfo":{"status":"ok","timestamp":1696718255688,"user_tz":180,"elapsed":17,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["# Gere um subconjunto aleatório de todas as combinações possíveis.\n","random_combinations = []\n","for _ in range(num_random_configs):\n","    num_layers = random.randint(1, len(candidate_layers))  # Número de camadas morfológicas para adicionar\n","    layers = random.sample(candidate_layers, num_layers)  # Escolha aleatória das camadas\n","    ops = random.choices(morph_ops, k=num_layers)  # Escolha aleatória das operações\n","    k_sizes = random.choices(kernel_sizes, k=num_layers)  # Escolha aleatória dos tamanhos de kernel\n","    shape_types = random.choices(shapes, k=num_layers)  # Escolha aleatória dos tipos de elementos estruturantes\n","    random_combinations.append((layers, ops, k_sizes, shape_types))\n","\n","def get_morph_layer(morph_op, kernel_size, shape, in_channels):\n","    return MorphologicalLayer(in_channels=in_channels, kernel_size=kernel_size, shape=shape, morph_op=morph_op)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1696718255688,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"RY6l-CxCLijd","outputId":"5eb062b9-ea57-4d3d-a244-811381c79b1e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['after_conv3'], ['closing'], [7], ['circle'])"]},"metadata":{},"execution_count":22}],"source":["random_combinations[0]"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"VRs0hOngMBe-","executionInfo":{"status":"ok","timestamp":1696718255688,"user_tz":180,"elapsed":14,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["random_combinations = [\n","(['after_conv4'],\n"," ['erosion'],\n"," [3],\n"," ['square']),\n","]"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1696718255688,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"8UkZiy8vMwjE","outputId":"320bb6ba-3473-4d65-930b-de4eb48acb23"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(['after_conv4'], ['erosion'], [3], ['square'])]"]},"metadata":{},"execution_count":24}],"source":["random_combinations"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"ymFmmlD7cgBF","executionInfo":{"status":"ok","timestamp":1696718255689,"user_tz":180,"elapsed":14,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["def get_out_channels_from_conv(model, conv_name):\n","    conv_block = getattr(model, conv_name, None)\n","\n","    if conv_block is None:\n","        print(f\"{conv_name} not found in the model.\")\n","        return None\n","\n","    if not isinstance(conv_block, nn.Sequential):\n","        print(f\"{conv_name} is not an instance of nn.Sequential.\")\n","        return None\n","\n","    for layer in conv_block:\n","        if isinstance(layer, nn.Module) and hasattr(layer, 'conv3'):\n","            return layer.conv3.out_channels\n","\n","    return None"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lz2cyVFPjKyq","executionInfo":{"status":"ok","timestamp":1696718255689,"user_tz":180,"elapsed":13,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"ece2d801-7cca-469f-bfc7-80eb561276d3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['after_conv4'], ['erosion'], [3], ['square'])"]},"metadata":{},"execution_count":26}],"source":["random_combinations[0]"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"Z313YLyMn2XX","executionInfo":{"status":"ok","timestamp":1696718255689,"user_tz":180,"elapsed":12,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["results = pd.DataFrame(None,columns=['epoch', 'lr', 'loss',\n","                                     'CCE_loss', 'dice_train',\n","                                     'layers', 'ops', 'k_sizes',\n","                                     'shape_types','val_score','dice_val',\n","                                     'f1_score','F1_0','F1_1','F1_2',\n","                                     'F1_3'])\n","#results.to_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_3_top_train2.csv', index=False)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"7aH5AfLSTxhT","executionInfo":{"status":"ok","timestamp":1696718255689,"user_tz":180,"elapsed":12,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"688a827c-2045-4ab4-9ce5-9440b0235871"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Empty DataFrame\n","Columns: [epoch, lr, loss, CCE_loss, dice_train, layers, ops, k_sizes, shape_types, val_score, dice_val, f1_score, F1_0, F1_1, F1_2, F1_3]\n","Index: []"],"text/html":["\n","  <div id=\"df-db6f476f-4249-48b7-a3ed-ba08c0e459f5\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch</th>\n","      <th>lr</th>\n","      <th>loss</th>\n","      <th>CCE_loss</th>\n","      <th>dice_train</th>\n","      <th>layers</th>\n","      <th>ops</th>\n","      <th>k_sizes</th>\n","      <th>shape_types</th>\n","      <th>val_score</th>\n","      <th>dice_val</th>\n","      <th>f1_score</th>\n","      <th>F1_0</th>\n","      <th>F1_1</th>\n","      <th>F1_2</th>\n","      <th>F1_3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db6f476f-4249-48b7-a3ed-ba08c0e459f5')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-db6f476f-4249-48b7-a3ed-ba08c0e459f5 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-db6f476f-4249-48b7-a3ed-ba08c0e459f5');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":28}],"source":["results"]},{"cell_type":"code","source":["file_name = f\"/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/after_conv4_['erosion']_[3]_['square']2.pth\""],"metadata":{"id":"LufJah033xAb","executionInfo":{"status":"ok","timestamp":1696720747276,"user_tz":180,"elapsed":2,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["len('module.module.module.module.module.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"84WY8QfY4TAv","executionInfo":{"status":"ok","timestamp":1696718255689,"user_tz":180,"elapsed":11,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"3347ea53-4be8-432a-c898-dd919d018ef6"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["35"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["# Carregando o estado do modelo\n","state_dict = torch.load(file_name)\n","# Criando um novo OrderedDict que não contém a palavra 'module.'\n","new_state_dict = OrderedDict()\n","for k, v in state_dict.items():\n","    name = k[35:]#.replace('module.', '')  # remove 'module.'\n","    new_state_dict[name] = v"],"metadata":{"id":"w8PPzXsx2M3G","executionInfo":{"status":"ok","timestamp":1696720786779,"user_tz":180,"elapsed":8305,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["print(len(new_state_dict.keys()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lg0onDLn2OAx","executionInfo":{"status":"ok","timestamp":1696720786779,"user_tz":180,"elapsed":15,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"33c06fef-f4a5-4da7-c3c3-04f5dfa1ba96"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["407\n"]}]},{"cell_type":"code","source":["new_state_dict.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3TByjU4Y3n4w","executionInfo":{"status":"ok","timestamp":1696720786780,"user_tz":180,"elapsed":14,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"0976347a-0364-4098-97b3-4942e46c873c"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['conv6.layer.0.weight', 'conv6.layer.0.bias', 'conv6_2.layer.0.weight', 'conv6_2.layer.0.bias', 'conv7.layer.0.weight', 'conv7.layer.0.bias', 'conv7_2.layer.0.weight', 'conv7_2.layer.0.bias', 'conv8.layer.0.weight', 'conv8.layer.0.bias', 'conv8_2.layer.0.weight', 'conv8_2.layer.0.bias', 'conv9.layer.0.weight', 'conv9.layer.0.bias', 'conv9_2.layer.0.weight', 'conv9_2.layer.0.bias', 'conv10.layer.0.weight', 'conv10.layer.0.bias', 'conv10_s.0.layer.0.weight', 'conv10_s.0.layer.0.bias', 'conv10_s.1.weight', 'conv10_s.1.bias', 'res.weight', 'res.bias', 'conv1.0.weight', 'conv1.1.weight', 'conv1.1.bias', 'conv1.1.running_mean', 'conv1.1.running_var', 'conv1.1.num_batches_tracked', 'conv2.1.0.conv1.weight', 'conv2.1.0.bn1.weight', 'conv2.1.0.bn1.bias', 'conv2.1.0.bn1.running_mean', 'conv2.1.0.bn1.running_var', 'conv2.1.0.bn1.num_batches_tracked', 'conv2.1.0.conv2.weight', 'conv2.1.0.bn2.weight', 'conv2.1.0.bn2.bias', 'conv2.1.0.bn2.running_mean', 'conv2.1.0.bn2.running_var', 'conv2.1.0.bn2.num_batches_tracked', 'conv2.1.0.conv3.weight', 'conv2.1.0.bn3.weight', 'conv2.1.0.bn3.bias', 'conv2.1.0.bn3.running_mean', 'conv2.1.0.bn3.running_var', 'conv2.1.0.bn3.num_batches_tracked', 'conv2.1.0.se_module.fc1.weight', 'conv2.1.0.se_module.fc1.bias', 'conv2.1.0.se_module.fc2.weight', 'conv2.1.0.se_module.fc2.bias', 'conv2.1.0.downsample.0.weight', 'conv2.1.0.downsample.1.weight', 'conv2.1.0.downsample.1.bias', 'conv2.1.0.downsample.1.running_mean', 'conv2.1.0.downsample.1.running_var', 'conv2.1.0.downsample.1.num_batches_tracked', 'conv2.1.1.conv1.weight', 'conv2.1.1.bn1.weight', 'conv2.1.1.bn1.bias', 'conv2.1.1.bn1.running_mean', 'conv2.1.1.bn1.running_var', 'conv2.1.1.bn1.num_batches_tracked', 'conv2.1.1.conv2.weight', 'conv2.1.1.bn2.weight', 'conv2.1.1.bn2.bias', 'conv2.1.1.bn2.running_mean', 'conv2.1.1.bn2.running_var', 'conv2.1.1.bn2.num_batches_tracked', 'conv2.1.1.conv3.weight', 'conv2.1.1.bn3.weight', 'conv2.1.1.bn3.bias', 'conv2.1.1.bn3.running_mean', 'conv2.1.1.bn3.running_var', 'conv2.1.1.bn3.num_batches_tracked', 'conv2.1.1.se_module.fc1.weight', 'conv2.1.1.se_module.fc1.bias', 'conv2.1.1.se_module.fc2.weight', 'conv2.1.1.se_module.fc2.bias', 'conv2.1.2.conv1.weight', 'conv2.1.2.bn1.weight', 'conv2.1.2.bn1.bias', 'conv2.1.2.bn1.running_mean', 'conv2.1.2.bn1.running_var', 'conv2.1.2.bn1.num_batches_tracked', 'conv2.1.2.conv2.weight', 'conv2.1.2.bn2.weight', 'conv2.1.2.bn2.bias', 'conv2.1.2.bn2.running_mean', 'conv2.1.2.bn2.running_var', 'conv2.1.2.bn2.num_batches_tracked', 'conv2.1.2.conv3.weight', 'conv2.1.2.bn3.weight', 'conv2.1.2.bn3.bias', 'conv2.1.2.bn3.running_mean', 'conv2.1.2.bn3.running_var', 'conv2.1.2.bn3.num_batches_tracked', 'conv2.1.2.se_module.fc1.weight', 'conv2.1.2.se_module.fc1.bias', 'conv2.1.2.se_module.fc2.weight', 'conv2.1.2.se_module.fc2.bias', 'conv3.0.conv1.weight', 'conv3.0.bn1.weight', 'conv3.0.bn1.bias', 'conv3.0.bn1.running_mean', 'conv3.0.bn1.running_var', 'conv3.0.bn1.num_batches_tracked', 'conv3.0.conv2.weight', 'conv3.0.bn2.weight', 'conv3.0.bn2.bias', 'conv3.0.bn2.running_mean', 'conv3.0.bn2.running_var', 'conv3.0.bn2.num_batches_tracked', 'conv3.0.conv3.weight', 'conv3.0.bn3.weight', 'conv3.0.bn3.bias', 'conv3.0.bn3.running_mean', 'conv3.0.bn3.running_var', 'conv3.0.bn3.num_batches_tracked', 'conv3.0.se_module.fc1.weight', 'conv3.0.se_module.fc1.bias', 'conv3.0.se_module.fc2.weight', 'conv3.0.se_module.fc2.bias', 'conv3.0.downsample.0.weight', 'conv3.0.downsample.1.weight', 'conv3.0.downsample.1.bias', 'conv3.0.downsample.1.running_mean', 'conv3.0.downsample.1.running_var', 'conv3.0.downsample.1.num_batches_tracked', 'conv3.1.conv1.weight', 'conv3.1.bn1.weight', 'conv3.1.bn1.bias', 'conv3.1.bn1.running_mean', 'conv3.1.bn1.running_var', 'conv3.1.bn1.num_batches_tracked', 'conv3.1.conv2.weight', 'conv3.1.bn2.weight', 'conv3.1.bn2.bias', 'conv3.1.bn2.running_mean', 'conv3.1.bn2.running_var', 'conv3.1.bn2.num_batches_tracked', 'conv3.1.conv3.weight', 'conv3.1.bn3.weight', 'conv3.1.bn3.bias', 'conv3.1.bn3.running_mean', 'conv3.1.bn3.running_var', 'conv3.1.bn3.num_batches_tracked', 'conv3.1.se_module.fc1.weight', 'conv3.1.se_module.fc1.bias', 'conv3.1.se_module.fc2.weight', 'conv3.1.se_module.fc2.bias', 'conv3.2.conv1.weight', 'conv3.2.bn1.weight', 'conv3.2.bn1.bias', 'conv3.2.bn1.running_mean', 'conv3.2.bn1.running_var', 'conv3.2.bn1.num_batches_tracked', 'conv3.2.conv2.weight', 'conv3.2.bn2.weight', 'conv3.2.bn2.bias', 'conv3.2.bn2.running_mean', 'conv3.2.bn2.running_var', 'conv3.2.bn2.num_batches_tracked', 'conv3.2.conv3.weight', 'conv3.2.bn3.weight', 'conv3.2.bn3.bias', 'conv3.2.bn3.running_mean', 'conv3.2.bn3.running_var', 'conv3.2.bn3.num_batches_tracked', 'conv3.2.se_module.fc1.weight', 'conv3.2.se_module.fc1.bias', 'conv3.2.se_module.fc2.weight', 'conv3.2.se_module.fc2.bias', 'conv3.3.conv1.weight', 'conv3.3.bn1.weight', 'conv3.3.bn1.bias', 'conv3.3.bn1.running_mean', 'conv3.3.bn1.running_var', 'conv3.3.bn1.num_batches_tracked', 'conv3.3.conv2.weight', 'conv3.3.bn2.weight', 'conv3.3.bn2.bias', 'conv3.3.bn2.running_mean', 'conv3.3.bn2.running_var', 'conv3.3.bn2.num_batches_tracked', 'conv3.3.conv3.weight', 'conv3.3.bn3.weight', 'conv3.3.bn3.bias', 'conv3.3.bn3.running_mean', 'conv3.3.bn3.running_var', 'conv3.3.bn3.num_batches_tracked', 'conv3.3.se_module.fc1.weight', 'conv3.3.se_module.fc1.bias', 'conv3.3.se_module.fc2.weight', 'conv3.3.se_module.fc2.bias', 'conv4.0.0.conv1.weight', 'conv4.0.0.bn1.weight', 'conv4.0.0.bn1.bias', 'conv4.0.0.bn1.running_mean', 'conv4.0.0.bn1.running_var', 'conv4.0.0.bn1.num_batches_tracked', 'conv4.0.0.conv2.weight', 'conv4.0.0.bn2.weight', 'conv4.0.0.bn2.bias', 'conv4.0.0.bn2.running_mean', 'conv4.0.0.bn2.running_var', 'conv4.0.0.bn2.num_batches_tracked', 'conv4.0.0.conv3.weight', 'conv4.0.0.bn3.weight', 'conv4.0.0.bn3.bias', 'conv4.0.0.bn3.running_mean', 'conv4.0.0.bn3.running_var', 'conv4.0.0.bn3.num_batches_tracked', 'conv4.0.0.se_module.fc1.weight', 'conv4.0.0.se_module.fc1.bias', 'conv4.0.0.se_module.fc2.weight', 'conv4.0.0.se_module.fc2.bias', 'conv4.0.0.downsample.0.weight', 'conv4.0.0.downsample.1.weight', 'conv4.0.0.downsample.1.bias', 'conv4.0.0.downsample.1.running_mean', 'conv4.0.0.downsample.1.running_var', 'conv4.0.0.downsample.1.num_batches_tracked', 'conv4.0.1.conv1.weight', 'conv4.0.1.bn1.weight', 'conv4.0.1.bn1.bias', 'conv4.0.1.bn1.running_mean', 'conv4.0.1.bn1.running_var', 'conv4.0.1.bn1.num_batches_tracked', 'conv4.0.1.conv2.weight', 'conv4.0.1.bn2.weight', 'conv4.0.1.bn2.bias', 'conv4.0.1.bn2.running_mean', 'conv4.0.1.bn2.running_var', 'conv4.0.1.bn2.num_batches_tracked', 'conv4.0.1.conv3.weight', 'conv4.0.1.bn3.weight', 'conv4.0.1.bn3.bias', 'conv4.0.1.bn3.running_mean', 'conv4.0.1.bn3.running_var', 'conv4.0.1.bn3.num_batches_tracked', 'conv4.0.1.se_module.fc1.weight', 'conv4.0.1.se_module.fc1.bias', 'conv4.0.1.se_module.fc2.weight', 'conv4.0.1.se_module.fc2.bias', 'conv4.0.2.conv1.weight', 'conv4.0.2.bn1.weight', 'conv4.0.2.bn1.bias', 'conv4.0.2.bn1.running_mean', 'conv4.0.2.bn1.running_var', 'conv4.0.2.bn1.num_batches_tracked', 'conv4.0.2.conv2.weight', 'conv4.0.2.bn2.weight', 'conv4.0.2.bn2.bias', 'conv4.0.2.bn2.running_mean', 'conv4.0.2.bn2.running_var', 'conv4.0.2.bn2.num_batches_tracked', 'conv4.0.2.conv3.weight', 'conv4.0.2.bn3.weight', 'conv4.0.2.bn3.bias', 'conv4.0.2.bn3.running_mean', 'conv4.0.2.bn3.running_var', 'conv4.0.2.bn3.num_batches_tracked', 'conv4.0.2.se_module.fc1.weight', 'conv4.0.2.se_module.fc1.bias', 'conv4.0.2.se_module.fc2.weight', 'conv4.0.2.se_module.fc2.bias', 'conv4.0.3.conv1.weight', 'conv4.0.3.bn1.weight', 'conv4.0.3.bn1.bias', 'conv4.0.3.bn1.running_mean', 'conv4.0.3.bn1.running_var', 'conv4.0.3.bn1.num_batches_tracked', 'conv4.0.3.conv2.weight', 'conv4.0.3.bn2.weight', 'conv4.0.3.bn2.bias', 'conv4.0.3.bn2.running_mean', 'conv4.0.3.bn2.running_var', 'conv4.0.3.bn2.num_batches_tracked', 'conv4.0.3.conv3.weight', 'conv4.0.3.bn3.weight', 'conv4.0.3.bn3.bias', 'conv4.0.3.bn3.running_mean', 'conv4.0.3.bn3.running_var', 'conv4.0.3.bn3.num_batches_tracked', 'conv4.0.3.se_module.fc1.weight', 'conv4.0.3.se_module.fc1.bias', 'conv4.0.3.se_module.fc2.weight', 'conv4.0.3.se_module.fc2.bias', 'conv4.0.4.conv1.weight', 'conv4.0.4.bn1.weight', 'conv4.0.4.bn1.bias', 'conv4.0.4.bn1.running_mean', 'conv4.0.4.bn1.running_var', 'conv4.0.4.bn1.num_batches_tracked', 'conv4.0.4.conv2.weight', 'conv4.0.4.bn2.weight', 'conv4.0.4.bn2.bias', 'conv4.0.4.bn2.running_mean', 'conv4.0.4.bn2.running_var', 'conv4.0.4.bn2.num_batches_tracked', 'conv4.0.4.conv3.weight', 'conv4.0.4.bn3.weight', 'conv4.0.4.bn3.bias', 'conv4.0.4.bn3.running_mean', 'conv4.0.4.bn3.running_var', 'conv4.0.4.bn3.num_batches_tracked', 'conv4.0.4.se_module.fc1.weight', 'conv4.0.4.se_module.fc1.bias', 'conv4.0.4.se_module.fc2.weight', 'conv4.0.4.se_module.fc2.bias', 'conv4.0.5.conv1.weight', 'conv4.0.5.bn1.weight', 'conv4.0.5.bn1.bias', 'conv4.0.5.bn1.running_mean', 'conv4.0.5.bn1.running_var', 'conv4.0.5.bn1.num_batches_tracked', 'conv4.0.5.conv2.weight', 'conv4.0.5.bn2.weight', 'conv4.0.5.bn2.bias', 'conv4.0.5.bn2.running_mean', 'conv4.0.5.bn2.running_var', 'conv4.0.5.bn2.num_batches_tracked', 'conv4.0.5.conv3.weight', 'conv4.0.5.bn3.weight', 'conv4.0.5.bn3.bias', 'conv4.0.5.bn3.running_mean', 'conv4.0.5.bn3.running_var', 'conv4.0.5.bn3.num_batches_tracked', 'conv4.0.5.se_module.fc1.weight', 'conv4.0.5.se_module.fc1.bias', 'conv4.0.5.se_module.fc2.weight', 'conv4.0.5.se_module.fc2.bias', 'conv4.1.kernel', 'conv5.0.conv1.weight', 'conv5.0.bn1.weight', 'conv5.0.bn1.bias', 'conv5.0.bn1.running_mean', 'conv5.0.bn1.running_var', 'conv5.0.bn1.num_batches_tracked', 'conv5.0.conv2.weight', 'conv5.0.bn2.weight', 'conv5.0.bn2.bias', 'conv5.0.bn2.running_mean', 'conv5.0.bn2.running_var', 'conv5.0.bn2.num_batches_tracked', 'conv5.0.conv3.weight', 'conv5.0.bn3.weight', 'conv5.0.bn3.bias', 'conv5.0.bn3.running_mean', 'conv5.0.bn3.running_var', 'conv5.0.bn3.num_batches_tracked', 'conv5.0.se_module.fc1.weight', 'conv5.0.se_module.fc1.bias', 'conv5.0.se_module.fc2.weight', 'conv5.0.se_module.fc2.bias', 'conv5.0.downsample.0.weight', 'conv5.0.downsample.1.weight', 'conv5.0.downsample.1.bias', 'conv5.0.downsample.1.running_mean', 'conv5.0.downsample.1.running_var', 'conv5.0.downsample.1.num_batches_tracked', 'conv5.1.conv1.weight', 'conv5.1.bn1.weight', 'conv5.1.bn1.bias', 'conv5.1.bn1.running_mean', 'conv5.1.bn1.running_var', 'conv5.1.bn1.num_batches_tracked', 'conv5.1.conv2.weight', 'conv5.1.bn2.weight', 'conv5.1.bn2.bias', 'conv5.1.bn2.running_mean', 'conv5.1.bn2.running_var', 'conv5.1.bn2.num_batches_tracked', 'conv5.1.conv3.weight', 'conv5.1.bn3.weight', 'conv5.1.bn3.bias', 'conv5.1.bn3.running_mean', 'conv5.1.bn3.running_var', 'conv5.1.bn3.num_batches_tracked', 'conv5.1.se_module.fc1.weight', 'conv5.1.se_module.fc1.bias', 'conv5.1.se_module.fc2.weight', 'conv5.1.se_module.fc2.bias', 'conv5.2.conv1.weight', 'conv5.2.bn1.weight', 'conv5.2.bn1.bias', 'conv5.2.bn1.running_mean', 'conv5.2.bn1.running_var', 'conv5.2.bn1.num_batches_tracked', 'conv5.2.conv2.weight', 'conv5.2.bn2.weight', 'conv5.2.bn2.bias', 'conv5.2.bn2.running_mean', 'conv5.2.bn2.running_var', 'conv5.2.bn2.num_batches_tracked', 'conv5.2.conv3.weight', 'conv5.2.bn3.weight', 'conv5.2.bn3.bias', 'conv5.2.bn3.running_mean', 'conv5.2.bn3.running_var', 'conv5.2.bn3.num_batches_tracked', 'conv5.2.se_module.fc1.weight', 'conv5.2.se_module.fc1.bias', 'conv5.2.se_module.fc2.weight', 'conv5.2.se_module.fc2.bias'])"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["new_state_dict['conv4.1.kernel']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XewRTISJ2wLj","executionInfo":{"status":"ok","timestamp":1696720786780,"user_tz":180,"elapsed":12,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"ce585152-ff75-4260-9346-dd9487de8614"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[0.8587, 0.8625, 0.8522],\n","          [0.8579, 0.8720, 0.8644],\n","          [0.8582, 0.8721, 0.8740]]],\n","\n","\n","        [[[0.9578, 0.9684, 0.9657],\n","          [0.9663, 0.9753, 0.9839],\n","          [0.9698, 0.9863, 1.0018]]],\n","\n","\n","        [[[0.9698, 0.9490, 0.9654],\n","          [0.9630, 0.9392, 0.9583],\n","          [0.9760, 0.9523, 0.9522]]],\n","\n","\n","        ...,\n","\n","\n","        [[[1.0046, 1.0239, 1.0180],\n","          [1.0073, 1.0259, 1.0182],\n","          [1.0069, 1.0232, 1.0117]]],\n","\n","\n","        [[[0.9930, 0.9592, 0.9623],\n","          [0.9786, 0.9569, 0.9564],\n","          [0.9892, 0.9620, 0.9717]]],\n","\n","\n","        [[[0.8807, 0.9469, 0.9273],\n","          [0.9179, 1.0304, 0.9772],\n","          [0.8749, 0.9682, 0.9200]]]], device='cuda:0')"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["new_state_dict['conv4.0.5.se_module.fc2.bias']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_WS6yrBjzN2Z","executionInfo":{"status":"ok","timestamp":1696720786780,"user_tz":180,"elapsed":6,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"1b5969ab-24c2-49ac-a11e-066614fdf1cc"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.4775, -0.3292,  0.2764,  ..., -0.5161, -0.4927, -0.8829],\n","       device='cuda:0')"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","model = SeResNext50_Unet_2Ssum().cuda()\n","\n","before_load_state = {k: v.clone() for k, v in model.state_dict().items()}\n","\n","for layers, ops, k_sizes, shape_types in random_combinations:\n","    for layer, morph_op, k_size, shape in zip(layers, ops, k_sizes, shape_types):\n","      print(layer, morph_op, k_size, shape)\n","      if layer == 'after_conv3':\n","          in_channels = get_out_channels_from_conv(model,'conv3')\n","          morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","          model.conv3 = nn.Sequential(model.conv3, morph_layer)\n","      elif layer == 'after_conv4':\n","          in_channels = get_out_channels_from_conv(model,'conv4')\n","          morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","          model.conv4 = nn.Sequential(model.conv4, morph_layer)\n","      elif layer == 'after_conv5':\n","          in_channels = get_out_channels_from_conv(model,'conv5')\n","          morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","          model.conv5 = nn.Sequential(model.conv5, morph_layer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-HOiyZuO2X7-","executionInfo":{"status":"ok","timestamp":1696721214348,"user_tz":180,"elapsed":313549,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"522782e0-183f-462e-cd5e-9423923a499c"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\" to /root/.cache/torch/hub/checkpoints/se_resnext50_32x4d-a260b3a4.pth\n","100%|██████████| 105M/105M [05:10<00:00, 356kB/s]"]},{"output_type":"stream","name":"stdout","text":["after_conv4 erosion 3 square\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["print(len(model.state_dict().keys()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EI61hDZL2piD","executionInfo":{"status":"ok","timestamp":1696721214348,"user_tz":180,"elapsed":42,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"e5771388-54ac-4bfb-da59-42ed26228be1"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["407\n"]}]},{"cell_type":"code","source":["model.state_dict().keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PjhNNkh_3_L_","executionInfo":{"status":"ok","timestamp":1696721214348,"user_tz":180,"elapsed":15,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"8b5b366e-c701-4cdc-91dd-520ee6d1d082"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['conv6.layer.0.weight', 'conv6.layer.0.bias', 'conv6_2.layer.0.weight', 'conv6_2.layer.0.bias', 'conv7.layer.0.weight', 'conv7.layer.0.bias', 'conv7_2.layer.0.weight', 'conv7_2.layer.0.bias', 'conv8.layer.0.weight', 'conv8.layer.0.bias', 'conv8_2.layer.0.weight', 'conv8_2.layer.0.bias', 'conv9.layer.0.weight', 'conv9.layer.0.bias', 'conv9_2.layer.0.weight', 'conv9_2.layer.0.bias', 'conv10.layer.0.weight', 'conv10.layer.0.bias', 'conv10_s.0.layer.0.weight', 'conv10_s.0.layer.0.bias', 'conv10_s.1.weight', 'conv10_s.1.bias', 'res.weight', 'res.bias', 'conv1.0.weight', 'conv1.1.weight', 'conv1.1.bias', 'conv1.1.running_mean', 'conv1.1.running_var', 'conv1.1.num_batches_tracked', 'conv2.1.0.conv1.weight', 'conv2.1.0.bn1.weight', 'conv2.1.0.bn1.bias', 'conv2.1.0.bn1.running_mean', 'conv2.1.0.bn1.running_var', 'conv2.1.0.bn1.num_batches_tracked', 'conv2.1.0.conv2.weight', 'conv2.1.0.bn2.weight', 'conv2.1.0.bn2.bias', 'conv2.1.0.bn2.running_mean', 'conv2.1.0.bn2.running_var', 'conv2.1.0.bn2.num_batches_tracked', 'conv2.1.0.conv3.weight', 'conv2.1.0.bn3.weight', 'conv2.1.0.bn3.bias', 'conv2.1.0.bn3.running_mean', 'conv2.1.0.bn3.running_var', 'conv2.1.0.bn3.num_batches_tracked', 'conv2.1.0.se_module.fc1.weight', 'conv2.1.0.se_module.fc1.bias', 'conv2.1.0.se_module.fc2.weight', 'conv2.1.0.se_module.fc2.bias', 'conv2.1.0.downsample.0.weight', 'conv2.1.0.downsample.1.weight', 'conv2.1.0.downsample.1.bias', 'conv2.1.0.downsample.1.running_mean', 'conv2.1.0.downsample.1.running_var', 'conv2.1.0.downsample.1.num_batches_tracked', 'conv2.1.1.conv1.weight', 'conv2.1.1.bn1.weight', 'conv2.1.1.bn1.bias', 'conv2.1.1.bn1.running_mean', 'conv2.1.1.bn1.running_var', 'conv2.1.1.bn1.num_batches_tracked', 'conv2.1.1.conv2.weight', 'conv2.1.1.bn2.weight', 'conv2.1.1.bn2.bias', 'conv2.1.1.bn2.running_mean', 'conv2.1.1.bn2.running_var', 'conv2.1.1.bn2.num_batches_tracked', 'conv2.1.1.conv3.weight', 'conv2.1.1.bn3.weight', 'conv2.1.1.bn3.bias', 'conv2.1.1.bn3.running_mean', 'conv2.1.1.bn3.running_var', 'conv2.1.1.bn3.num_batches_tracked', 'conv2.1.1.se_module.fc1.weight', 'conv2.1.1.se_module.fc1.bias', 'conv2.1.1.se_module.fc2.weight', 'conv2.1.1.se_module.fc2.bias', 'conv2.1.2.conv1.weight', 'conv2.1.2.bn1.weight', 'conv2.1.2.bn1.bias', 'conv2.1.2.bn1.running_mean', 'conv2.1.2.bn1.running_var', 'conv2.1.2.bn1.num_batches_tracked', 'conv2.1.2.conv2.weight', 'conv2.1.2.bn2.weight', 'conv2.1.2.bn2.bias', 'conv2.1.2.bn2.running_mean', 'conv2.1.2.bn2.running_var', 'conv2.1.2.bn2.num_batches_tracked', 'conv2.1.2.conv3.weight', 'conv2.1.2.bn3.weight', 'conv2.1.2.bn3.bias', 'conv2.1.2.bn3.running_mean', 'conv2.1.2.bn3.running_var', 'conv2.1.2.bn3.num_batches_tracked', 'conv2.1.2.se_module.fc1.weight', 'conv2.1.2.se_module.fc1.bias', 'conv2.1.2.se_module.fc2.weight', 'conv2.1.2.se_module.fc2.bias', 'conv3.0.conv1.weight', 'conv3.0.bn1.weight', 'conv3.0.bn1.bias', 'conv3.0.bn1.running_mean', 'conv3.0.bn1.running_var', 'conv3.0.bn1.num_batches_tracked', 'conv3.0.conv2.weight', 'conv3.0.bn2.weight', 'conv3.0.bn2.bias', 'conv3.0.bn2.running_mean', 'conv3.0.bn2.running_var', 'conv3.0.bn2.num_batches_tracked', 'conv3.0.conv3.weight', 'conv3.0.bn3.weight', 'conv3.0.bn3.bias', 'conv3.0.bn3.running_mean', 'conv3.0.bn3.running_var', 'conv3.0.bn3.num_batches_tracked', 'conv3.0.se_module.fc1.weight', 'conv3.0.se_module.fc1.bias', 'conv3.0.se_module.fc2.weight', 'conv3.0.se_module.fc2.bias', 'conv3.0.downsample.0.weight', 'conv3.0.downsample.1.weight', 'conv3.0.downsample.1.bias', 'conv3.0.downsample.1.running_mean', 'conv3.0.downsample.1.running_var', 'conv3.0.downsample.1.num_batches_tracked', 'conv3.1.conv1.weight', 'conv3.1.bn1.weight', 'conv3.1.bn1.bias', 'conv3.1.bn1.running_mean', 'conv3.1.bn1.running_var', 'conv3.1.bn1.num_batches_tracked', 'conv3.1.conv2.weight', 'conv3.1.bn2.weight', 'conv3.1.bn2.bias', 'conv3.1.bn2.running_mean', 'conv3.1.bn2.running_var', 'conv3.1.bn2.num_batches_tracked', 'conv3.1.conv3.weight', 'conv3.1.bn3.weight', 'conv3.1.bn3.bias', 'conv3.1.bn3.running_mean', 'conv3.1.bn3.running_var', 'conv3.1.bn3.num_batches_tracked', 'conv3.1.se_module.fc1.weight', 'conv3.1.se_module.fc1.bias', 'conv3.1.se_module.fc2.weight', 'conv3.1.se_module.fc2.bias', 'conv3.2.conv1.weight', 'conv3.2.bn1.weight', 'conv3.2.bn1.bias', 'conv3.2.bn1.running_mean', 'conv3.2.bn1.running_var', 'conv3.2.bn1.num_batches_tracked', 'conv3.2.conv2.weight', 'conv3.2.bn2.weight', 'conv3.2.bn2.bias', 'conv3.2.bn2.running_mean', 'conv3.2.bn2.running_var', 'conv3.2.bn2.num_batches_tracked', 'conv3.2.conv3.weight', 'conv3.2.bn3.weight', 'conv3.2.bn3.bias', 'conv3.2.bn3.running_mean', 'conv3.2.bn3.running_var', 'conv3.2.bn3.num_batches_tracked', 'conv3.2.se_module.fc1.weight', 'conv3.2.se_module.fc1.bias', 'conv3.2.se_module.fc2.weight', 'conv3.2.se_module.fc2.bias', 'conv3.3.conv1.weight', 'conv3.3.bn1.weight', 'conv3.3.bn1.bias', 'conv3.3.bn1.running_mean', 'conv3.3.bn1.running_var', 'conv3.3.bn1.num_batches_tracked', 'conv3.3.conv2.weight', 'conv3.3.bn2.weight', 'conv3.3.bn2.bias', 'conv3.3.bn2.running_mean', 'conv3.3.bn2.running_var', 'conv3.3.bn2.num_batches_tracked', 'conv3.3.conv3.weight', 'conv3.3.bn3.weight', 'conv3.3.bn3.bias', 'conv3.3.bn3.running_mean', 'conv3.3.bn3.running_var', 'conv3.3.bn3.num_batches_tracked', 'conv3.3.se_module.fc1.weight', 'conv3.3.se_module.fc1.bias', 'conv3.3.se_module.fc2.weight', 'conv3.3.se_module.fc2.bias', 'conv4.0.0.conv1.weight', 'conv4.0.0.bn1.weight', 'conv4.0.0.bn1.bias', 'conv4.0.0.bn1.running_mean', 'conv4.0.0.bn1.running_var', 'conv4.0.0.bn1.num_batches_tracked', 'conv4.0.0.conv2.weight', 'conv4.0.0.bn2.weight', 'conv4.0.0.bn2.bias', 'conv4.0.0.bn2.running_mean', 'conv4.0.0.bn2.running_var', 'conv4.0.0.bn2.num_batches_tracked', 'conv4.0.0.conv3.weight', 'conv4.0.0.bn3.weight', 'conv4.0.0.bn3.bias', 'conv4.0.0.bn3.running_mean', 'conv4.0.0.bn3.running_var', 'conv4.0.0.bn3.num_batches_tracked', 'conv4.0.0.se_module.fc1.weight', 'conv4.0.0.se_module.fc1.bias', 'conv4.0.0.se_module.fc2.weight', 'conv4.0.0.se_module.fc2.bias', 'conv4.0.0.downsample.0.weight', 'conv4.0.0.downsample.1.weight', 'conv4.0.0.downsample.1.bias', 'conv4.0.0.downsample.1.running_mean', 'conv4.0.0.downsample.1.running_var', 'conv4.0.0.downsample.1.num_batches_tracked', 'conv4.0.1.conv1.weight', 'conv4.0.1.bn1.weight', 'conv4.0.1.bn1.bias', 'conv4.0.1.bn1.running_mean', 'conv4.0.1.bn1.running_var', 'conv4.0.1.bn1.num_batches_tracked', 'conv4.0.1.conv2.weight', 'conv4.0.1.bn2.weight', 'conv4.0.1.bn2.bias', 'conv4.0.1.bn2.running_mean', 'conv4.0.1.bn2.running_var', 'conv4.0.1.bn2.num_batches_tracked', 'conv4.0.1.conv3.weight', 'conv4.0.1.bn3.weight', 'conv4.0.1.bn3.bias', 'conv4.0.1.bn3.running_mean', 'conv4.0.1.bn3.running_var', 'conv4.0.1.bn3.num_batches_tracked', 'conv4.0.1.se_module.fc1.weight', 'conv4.0.1.se_module.fc1.bias', 'conv4.0.1.se_module.fc2.weight', 'conv4.0.1.se_module.fc2.bias', 'conv4.0.2.conv1.weight', 'conv4.0.2.bn1.weight', 'conv4.0.2.bn1.bias', 'conv4.0.2.bn1.running_mean', 'conv4.0.2.bn1.running_var', 'conv4.0.2.bn1.num_batches_tracked', 'conv4.0.2.conv2.weight', 'conv4.0.2.bn2.weight', 'conv4.0.2.bn2.bias', 'conv4.0.2.bn2.running_mean', 'conv4.0.2.bn2.running_var', 'conv4.0.2.bn2.num_batches_tracked', 'conv4.0.2.conv3.weight', 'conv4.0.2.bn3.weight', 'conv4.0.2.bn3.bias', 'conv4.0.2.bn3.running_mean', 'conv4.0.2.bn3.running_var', 'conv4.0.2.bn3.num_batches_tracked', 'conv4.0.2.se_module.fc1.weight', 'conv4.0.2.se_module.fc1.bias', 'conv4.0.2.se_module.fc2.weight', 'conv4.0.2.se_module.fc2.bias', 'conv4.0.3.conv1.weight', 'conv4.0.3.bn1.weight', 'conv4.0.3.bn1.bias', 'conv4.0.3.bn1.running_mean', 'conv4.0.3.bn1.running_var', 'conv4.0.3.bn1.num_batches_tracked', 'conv4.0.3.conv2.weight', 'conv4.0.3.bn2.weight', 'conv4.0.3.bn2.bias', 'conv4.0.3.bn2.running_mean', 'conv4.0.3.bn2.running_var', 'conv4.0.3.bn2.num_batches_tracked', 'conv4.0.3.conv3.weight', 'conv4.0.3.bn3.weight', 'conv4.0.3.bn3.bias', 'conv4.0.3.bn3.running_mean', 'conv4.0.3.bn3.running_var', 'conv4.0.3.bn3.num_batches_tracked', 'conv4.0.3.se_module.fc1.weight', 'conv4.0.3.se_module.fc1.bias', 'conv4.0.3.se_module.fc2.weight', 'conv4.0.3.se_module.fc2.bias', 'conv4.0.4.conv1.weight', 'conv4.0.4.bn1.weight', 'conv4.0.4.bn1.bias', 'conv4.0.4.bn1.running_mean', 'conv4.0.4.bn1.running_var', 'conv4.0.4.bn1.num_batches_tracked', 'conv4.0.4.conv2.weight', 'conv4.0.4.bn2.weight', 'conv4.0.4.bn2.bias', 'conv4.0.4.bn2.running_mean', 'conv4.0.4.bn2.running_var', 'conv4.0.4.bn2.num_batches_tracked', 'conv4.0.4.conv3.weight', 'conv4.0.4.bn3.weight', 'conv4.0.4.bn3.bias', 'conv4.0.4.bn3.running_mean', 'conv4.0.4.bn3.running_var', 'conv4.0.4.bn3.num_batches_tracked', 'conv4.0.4.se_module.fc1.weight', 'conv4.0.4.se_module.fc1.bias', 'conv4.0.4.se_module.fc2.weight', 'conv4.0.4.se_module.fc2.bias', 'conv4.0.5.conv1.weight', 'conv4.0.5.bn1.weight', 'conv4.0.5.bn1.bias', 'conv4.0.5.bn1.running_mean', 'conv4.0.5.bn1.running_var', 'conv4.0.5.bn1.num_batches_tracked', 'conv4.0.5.conv2.weight', 'conv4.0.5.bn2.weight', 'conv4.0.5.bn2.bias', 'conv4.0.5.bn2.running_mean', 'conv4.0.5.bn2.running_var', 'conv4.0.5.bn2.num_batches_tracked', 'conv4.0.5.conv3.weight', 'conv4.0.5.bn3.weight', 'conv4.0.5.bn3.bias', 'conv4.0.5.bn3.running_mean', 'conv4.0.5.bn3.running_var', 'conv4.0.5.bn3.num_batches_tracked', 'conv4.0.5.se_module.fc1.weight', 'conv4.0.5.se_module.fc1.bias', 'conv4.0.5.se_module.fc2.weight', 'conv4.0.5.se_module.fc2.bias', 'conv4.1.kernel', 'conv5.0.conv1.weight', 'conv5.0.bn1.weight', 'conv5.0.bn1.bias', 'conv5.0.bn1.running_mean', 'conv5.0.bn1.running_var', 'conv5.0.bn1.num_batches_tracked', 'conv5.0.conv2.weight', 'conv5.0.bn2.weight', 'conv5.0.bn2.bias', 'conv5.0.bn2.running_mean', 'conv5.0.bn2.running_var', 'conv5.0.bn2.num_batches_tracked', 'conv5.0.conv3.weight', 'conv5.0.bn3.weight', 'conv5.0.bn3.bias', 'conv5.0.bn3.running_mean', 'conv5.0.bn3.running_var', 'conv5.0.bn3.num_batches_tracked', 'conv5.0.se_module.fc1.weight', 'conv5.0.se_module.fc1.bias', 'conv5.0.se_module.fc2.weight', 'conv5.0.se_module.fc2.bias', 'conv5.0.downsample.0.weight', 'conv5.0.downsample.1.weight', 'conv5.0.downsample.1.bias', 'conv5.0.downsample.1.running_mean', 'conv5.0.downsample.1.running_var', 'conv5.0.downsample.1.num_batches_tracked', 'conv5.1.conv1.weight', 'conv5.1.bn1.weight', 'conv5.1.bn1.bias', 'conv5.1.bn1.running_mean', 'conv5.1.bn1.running_var', 'conv5.1.bn1.num_batches_tracked', 'conv5.1.conv2.weight', 'conv5.1.bn2.weight', 'conv5.1.bn2.bias', 'conv5.1.bn2.running_mean', 'conv5.1.bn2.running_var', 'conv5.1.bn2.num_batches_tracked', 'conv5.1.conv3.weight', 'conv5.1.bn3.weight', 'conv5.1.bn3.bias', 'conv5.1.bn3.running_mean', 'conv5.1.bn3.running_var', 'conv5.1.bn3.num_batches_tracked', 'conv5.1.se_module.fc1.weight', 'conv5.1.se_module.fc1.bias', 'conv5.1.se_module.fc2.weight', 'conv5.1.se_module.fc2.bias', 'conv5.2.conv1.weight', 'conv5.2.bn1.weight', 'conv5.2.bn1.bias', 'conv5.2.bn1.running_mean', 'conv5.2.bn1.running_var', 'conv5.2.bn1.num_batches_tracked', 'conv5.2.conv2.weight', 'conv5.2.bn2.weight', 'conv5.2.bn2.bias', 'conv5.2.bn2.running_mean', 'conv5.2.bn2.running_var', 'conv5.2.bn2.num_batches_tracked', 'conv5.2.conv3.weight', 'conv5.2.bn3.weight', 'conv5.2.bn3.bias', 'conv5.2.bn3.running_mean', 'conv5.2.bn3.running_var', 'conv5.2.bn3.num_batches_tracked', 'conv5.2.se_module.fc1.weight', 'conv5.2.se_module.fc1.bias', 'conv5.2.se_module.fc2.weight', 'conv5.2.se_module.fc2.bias'])"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["model.state_dict()['conv4.1.kernel']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__7mkbOf2rGH","executionInfo":{"status":"ok","timestamp":1696721214348,"user_tz":180,"elapsed":13,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"ca83a5b9-7d08-4565-974a-00d7f424478f"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[1., 1., 1.],\n","          [1., 1., 1.],\n","          [1., 1., 1.]]],\n","\n","\n","        [[[1., 1., 1.],\n","          [1., 1., 1.],\n","          [1., 1., 1.]]],\n","\n","\n","        [[[1., 1., 1.],\n","          [1., 1., 1.],\n","          [1., 1., 1.]]],\n","\n","\n","        ...,\n","\n","\n","        [[[1., 1., 1.],\n","          [1., 1., 1.],\n","          [1., 1., 1.]]],\n","\n","\n","        [[[1., 1., 1.],\n","          [1., 1., 1.],\n","          [1., 1., 1.]]],\n","\n","\n","        [[[1., 1., 1.],\n","          [1., 1., 1.],\n","          [1., 1., 1.]]]])"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["model.state_dict()['conv4.0.5.se_module.fc2.bias']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sZA21_yM44ZP","executionInfo":{"status":"ok","timestamp":1696721214348,"user_tz":180,"elapsed":8,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"a3ecb534-b92a-473f-939b-798cc151b0a5"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.5167, -0.3189,  0.2552,  ..., -0.5132, -0.4954, -0.8254],\n","       device='cuda:0')"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["model.load_state_dict(new_state_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJk7enpW28sv","executionInfo":{"status":"ok","timestamp":1696721214348,"user_tz":180,"elapsed":6,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"587c8de9-4026-484a-de83-2a3a4f05ae0b"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["model.state_dict()['conv4.1.kernel']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p4NncKqD3AhB","executionInfo":{"status":"ok","timestamp":1696721214348,"user_tz":180,"elapsed":5,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"7fc555de-9f18-4fb7-d56b-84ccf7fbeaa2"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[0.8587, 0.8625, 0.8522],\n","          [0.8579, 0.8720, 0.8644],\n","          [0.8582, 0.8721, 0.8740]]],\n","\n","\n","        [[[0.9578, 0.9684, 0.9657],\n","          [0.9663, 0.9753, 0.9839],\n","          [0.9698, 0.9863, 1.0018]]],\n","\n","\n","        [[[0.9698, 0.9490, 0.9654],\n","          [0.9630, 0.9392, 0.9583],\n","          [0.9760, 0.9523, 0.9522]]],\n","\n","\n","        ...,\n","\n","\n","        [[[1.0046, 1.0239, 1.0180],\n","          [1.0073, 1.0259, 1.0182],\n","          [1.0069, 1.0232, 1.0117]]],\n","\n","\n","        [[[0.9930, 0.9592, 0.9623],\n","          [0.9786, 0.9569, 0.9564],\n","          [0.9892, 0.9620, 0.9717]]],\n","\n","\n","        [[[0.8807, 0.9469, 0.9273],\n","          [0.9179, 1.0304, 0.9772],\n","          [0.8749, 0.9682, 0.9200]]]])"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["model.state_dict()['conv4.0.5.se_module.fc2.bias']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bxWJaUa5zQpu","executionInfo":{"status":"ok","timestamp":1696721214348,"user_tz":180,"elapsed":4,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"4902fb3d-5624-4a0e-bfbc-1a96b74a83fc"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.4775, -0.3292,  0.2764,  ..., -0.5161, -0.4927, -0.8829],\n","       device='cuda:0')"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qFTYPcdBccNc","outputId":"79b0b506-13ee-4229-d406-de599ed2e2e1","executionInfo":{"status":"ok","timestamp":1696641052694,"user_tz":180,"elapsed":9684568,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 21; lr 0.0002020; Loss 0.0901 (0.0827); cce_loss 0.0901 (0.0827); Dice 0.4783 (0.5428): 100%|██████████| 1227/1227 [41:58<00:00,  2.05s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 21; lr 0.0002020; Loss 0.0827; CCE_loss 0.0827; Dice 0.5428\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 22; lr 0.0002020; Loss 0.0093 (0.0851); cce_loss 0.0093 (0.0851); Dice 0.5368 (0.5523): 100%|██████████| 1227/1227 [15:45<00:00,  1.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 22; lr 0.0002020; Loss 0.0851; CCE_loss 0.0851; Dice 0.5523\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 23; lr 0.0002020; Loss 0.0756 (0.0801); cce_loss 0.0756 (0.0801); Dice 0.6609 (0.5560): 100%|██████████| 1227/1227 [13:00<00:00,  1.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 23; lr 0.0002020; Loss 0.0801; CCE_loss 0.0801; Dice 0.5560\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 24; lr 0.0002020; Loss 0.0481 (0.0797); cce_loss 0.0481 (0.0797); Dice 0.6296 (0.5458): 100%|██████████| 1227/1227 [12:36<00:00,  1.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 24; lr 0.0002020; Loss 0.0797; CCE_loss 0.0797; Dice 0.5458\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 91/91 [10:57<00:00,  7.22s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.7863853423341499, Dice: 1.0, F1: 0.6948362033344999, F1_0: 0.938681712923361, F1_1: 0.45916770097207205, F1_2: 0.7299286801713935, F1_3: 0.8744413582507283\n","score: [0.7863853423341499, 1.0, 0.6948362033344999, 0.938681712923361, 0.45916770097207205, 0.7299286801713935, 0.8744413582507283]\tscore_best: 0.7863853423341499\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 25; lr 0.0002020; Loss 0.1791 (0.0798); cce_loss 0.1791 (0.0798); Dice 0.3817 (0.5392): 100%|██████████| 1227/1227 [40:47<00:00,  1.99s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 25; lr 0.0000505; Loss 0.0798; CCE_loss 0.0798; Dice 0.5392\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 26; lr 0.0000505; Loss 0.0419 (0.0737); cce_loss 0.0419 (0.0737); Dice 0.6212 (0.5534): 100%|██████████| 1227/1227 [15:47<00:00,  1.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 26; lr 0.0001010; Loss 0.0737; CCE_loss 0.0737; Dice 0.5534\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 91/91 [10:21<00:00,  6.83s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.8188044728703034, Dice: 1.0, F1: 0.7411492469575764, F1_0: 0.9450747106209787, F1_1: 0.535283645126491, F1_2: 0.7567591441785775, F1_3: 0.8700788790154154\n","score: [0.8188044728703034, 1.0, 0.7411492469575764, 0.9450747106209787, 0.535283645126491, 0.7567591441785775, 0.8700788790154154]\tscore_best: 0.8188044728703034\n","Time: 161.399 min\n"]}],"source":["import ssl\n","\n","for layers, ops, k_sizes, shape_types in random_combinations:\n","    # Crie um novo modelo aqui. Você pode usar algo como:\n","    ssl._create_default_https_context = ssl._create_unverified_context\n","    t0 = timeit.default_timer()\n","    # model = SeResNext50_Unet_2Ssum().cuda()\n","\n","    # for layer, morph_op, k_size, shape in zip(layers, ops, k_sizes, shape_types):\n","    #   if layer == 'after_conv3':\n","    #       in_channels = get_out_channels_from_conv(model,'conv3')\n","    #       morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","    #       model.conv3 = nn.Sequential(model.conv3, morph_layer)\n","    #   elif layer == 'after_conv4':\n","    #       in_channels = get_out_channels_from_conv(model,'conv4')\n","    #       morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","    #       model.conv4 = nn.Sequential(model.conv4, morph_layer)\n","    #   elif layer == 'after_conv5':\n","    #       in_channels = get_out_channels_from_conv(model,'conv5')\n","    #       morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","    #       model.conv5 = nn.Sequential(model.conv5, morph_layer)\n","\n","    params = model.parameters()\n","\n","    optimizer = AdamW(params, lr=0.000202, weight_decay=1e-6)     #0.002\n","\n","    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[5, 13, 19, 23, 28, 47, 50, 60, 70, 90, 110, 130, 150, 170, 180, 190], gamma=0.5)\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    model = nn.DataParallel(model).cuda()\n","\n","    seg_loss = ComboLoss({'dice': 0.5, 'focal': 2.0}, per_image=False).cuda()\n","    ce_loss = nn.CrossEntropyLoss().cuda()\n","    seg_lossSeesaw = None#SeesawLoss2().cuda()\n","\n","    best_score = 0\n","    torch.cuda.empty_cache()\n","    for epoch in [21,22,23,24]:\n","        train_metrics = train_epoch(epoch, seg_loss, ce_loss, seg_lossSeesaw, model, optimizer, scheduler, train_data_loader)\n","        torch.cuda.empty_cache()\n","        if epoch % 2 == 0 and epoch >= 22:\n","          best_score, d = evaluate_val(val_data_loader, best_score, model, snapshot_name, epoch)\n","          torch.save(model.state_dict(), f'/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/{layer}_{ops}_{k_sizes}_{shape_types}2.pth')\n","        results = pd.read_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_3_top_train2.csv')\n","        try:\n","          results_iter = pd.DataFrame([[epoch, train_metrics[0], train_metrics[1].avg, train_metrics[2].avg, train_metrics[3].avg, layers, ops, k_sizes, shape_types, d[0], d[1], d[2], d[3], d[4], d[5], d[6]]],columns=results.columns)\n","        except:\n","          results_iter = pd.DataFrame([[epoch, train_metrics[0], train_metrics[1].avg, train_metrics[2].avg, train_metrics[3].avg, layers, ops, k_sizes, shape_types, None, None, None, None, None, None, None]],columns=results.columns)\n","        results = pd.concat([results,results_iter])\n","        d = None\n","        results.to_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_3_top_train2.csv', index=False)\n","\n","    try:\n","      #del model\n","      torch.cuda.empty_cache()\n","    except:\n","      None\n","\n","    elapsed = timeit.default_timer() - t0\n","    print('Time: {:.3f} min'.format(elapsed / 60))"]},{"cell_type":"code","source":["import ssl\n","\n","for layers, ops, k_sizes, shape_types in random_combinations:\n","    # Crie um novo modelo aqui. Você pode usar algo como:\n","    ssl._create_default_https_context = ssl._create_unverified_context\n","    t0 = timeit.default_timer()\n","    # model = SeResNext50_Unet_2Ssum().cuda()\n","\n","    # for layer, morph_op, k_size, shape in zip(layers, ops, k_sizes, shape_types):\n","    #   if layer == 'after_conv3':\n","    #       in_channels = get_out_channels_from_conv(model,'conv3')\n","    #       morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","    #       model.conv3 = nn.Sequential(model.conv3, morph_layer)\n","    #   elif layer == 'after_conv4':\n","    #       in_channels = get_out_channels_from_conv(model,'conv4')\n","    #       morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","    #       model.conv4 = nn.Sequential(model.conv4, morph_layer)\n","    #   elif layer == 'after_conv5':\n","    #       in_channels = get_out_channels_from_conv(model,'conv5')\n","    #       morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","    #       model.conv5 = nn.Sequential(model.conv5, morph_layer)\n","\n","    params = model.parameters()\n","\n","    optimizer = AdamW(params, lr=0.000202, weight_decay=1e-6)     #0.002\n","\n","    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[5, 13, 19, 23, 28, 47, 50, 60, 70, 90, 110, 130, 150, 170, 180, 190], gamma=0.5)\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    model = nn.DataParallel(model).cuda()\n","\n","    seg_loss = ComboLoss({'dice': 0.5, 'focal': 2.0}, per_image=False).cuda()\n","    ce_loss = nn.CrossEntropyLoss().cuda()\n","    seg_lossSeesaw = None#SeesawLoss2().cuda()\n","\n","    best_score = 0\n","    torch.cuda.empty_cache()\n","    for epoch in [27,28,29,30,31,32]:\n","        train_metrics = train_epoch(epoch, seg_loss, ce_loss, seg_lossSeesaw, model, optimizer, scheduler, train_data_loader)\n","        torch.cuda.empty_cache()\n","        if epoch % 2 == 0 and epoch >= 29:\n","          best_score, d = evaluate_val(val_data_loader, best_score, model, snapshot_name, epoch)\n","          torch.save(model.state_dict(), f'/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/{layer}_{ops}_{k_sizes}_{shape_types}2.pth')\n","        results = pd.read_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_3_top_train2.csv')\n","        try:\n","          results_iter = pd.DataFrame([[epoch, train_metrics[0], train_metrics[1].avg, train_metrics[2].avg, train_metrics[3].avg, layers, ops, k_sizes, shape_types, d[0], d[1], d[2], d[3], d[4], d[5], d[6]]],columns=results.columns)\n","        except:\n","          results_iter = pd.DataFrame([[epoch, train_metrics[0], train_metrics[1].avg, train_metrics[2].avg, train_metrics[3].avg, layers, ops, k_sizes, shape_types, None, None, None, None, None, None, None]],columns=results.columns)\n","        results = pd.concat([results,results_iter])\n","        d = None\n","        results.to_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_3_top_train2.csv', index=False)\n","\n","    try:\n","      #del model\n","      torch.cuda.empty_cache()\n","    except:\n","      None\n","\n","    elapsed = timeit.default_timer() - t0\n","    print('Time: {:.3f} min'.format(elapsed / 60))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u8PO1bIrxPrp","executionInfo":{"status":"ok","timestamp":1696652305389,"user_tz":180,"elapsed":9870288,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"9a07328b-5007-4dc6-e381-72646b378fb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 27; lr 0.0002020; Loss 0.0345 (0.0773); cce_loss 0.0345 (0.0773); Dice 0.3996 (0.5417): 100%|██████████| 1227/1227 [42:56<00:00,  2.10s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 27; lr 0.0002020; Loss 0.0773; CCE_loss 0.0773; Dice 0.5417\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 28; lr 0.0002020; Loss 0.0596 (0.0784); cce_loss 0.0596 (0.0784); Dice 0.6058 (0.5693): 100%|██████████| 1227/1227 [16:29<00:00,  1.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 28; lr 0.0002020; Loss 0.0784; CCE_loss 0.0784; Dice 0.5693\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 29; lr 0.0002020; Loss 0.0775 (0.0760); cce_loss 0.0775 (0.0760); Dice 0.5857 (0.5642): 100%|██████████| 1227/1227 [13:30<00:00,  1.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 29; lr 0.0002020; Loss 0.0760; CCE_loss 0.0760; Dice 0.5642\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 30; lr 0.0002020; Loss 0.0962 (0.0757); cce_loss 0.0962 (0.0757); Dice 0.5835 (0.5603): 100%|██████████| 1227/1227 [12:49<00:00,  1.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 30; lr 0.0002020; Loss 0.0757; CCE_loss 0.0757; Dice 0.5603\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 91/91 [11:25<00:00,  7.53s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.819808207852567, Dice: 1.0, F1: 0.7425831540750957, F1_0: 0.942676316509576, F1_1: 0.5532870916428011, F1_2: 0.7586163228978131, F1_3: 0.8331730660707434\n","score: [0.819808207852567, 1.0, 0.7425831540750957, 0.942676316509576, 0.5532870916428011, 0.7586163228978131, 0.8331730660707434]\tscore_best: 0.819808207852567\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 31; lr 0.0002020; Loss 0.0854 (0.0750); cce_loss 0.0854 (0.0750); Dice 0.5779 (0.5656): 100%|██████████| 1227/1227 [41:26<00:00,  2.03s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 31; lr 0.0000505; Loss 0.0750; CCE_loss 0.0750; Dice 0.5656\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 32; lr 0.0000505; Loss 0.0672 (0.0729); cce_loss 0.0672 (0.0729); Dice 0.6535 (0.5811): 100%|██████████| 1227/1227 [15:58<00:00,  1.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 32; lr 0.0001010; Loss 0.0729; CCE_loss 0.0729; Dice 0.5811\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 91/91 [09:45<00:00,  6.43s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.8266764636277932, Dice: 1.0, F1: 0.7523949480397047, F1_0: 0.9499579683903601, F1_1: 0.5672986908994863, F1_2: 0.7423086206059769, F1_3: 0.8667031358213011\n","score: [0.8266764636277932, 1.0, 0.7523949480397047, 0.9499579683903601, 0.5672986908994863, 0.7423086206059769, 0.8667031358213011]\tscore_best: 0.8266764636277932\n","Time: 164.497 min\n"]}]},{"cell_type":"code","source":["import ssl\n","\n","for layers, ops, k_sizes, shape_types in random_combinations:\n","    # Crie um novo modelo aqui. Você pode usar algo como:\n","    ssl._create_default_https_context = ssl._create_unverified_context\n","    t0 = timeit.default_timer()\n","    # model = SeResNext50_Unet_2Ssum().cuda()\n","\n","    # for layer, morph_op, k_size, shape in zip(layers, ops, k_sizes, shape_types):\n","    #   if layer == 'after_conv3':\n","    #       in_channels = get_out_channels_from_conv(model,'conv3')\n","    #       morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","    #       model.conv3 = nn.Sequential(model.conv3, morph_layer)\n","    #   elif layer == 'after_conv4':\n","    #       in_channels = get_out_channels_from_conv(model,'conv4')\n","    #       morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","    #       model.conv4 = nn.Sequential(model.conv4, morph_layer)\n","    #   elif layer == 'after_conv5':\n","    #       in_channels = get_out_channels_from_conv(model,'conv5')\n","    #       morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","    #       model.conv5 = nn.Sequential(model.conv5, morph_layer)\n","\n","    params = model.parameters()\n","\n","    optimizer = AdamW(params, lr=0.000202, weight_decay=1e-6)     #0.002\n","\n","    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[5, 13, 19, 23, 28, 47, 50, 60, 70, 90, 110, 130, 150, 170, 180, 190], gamma=0.5)\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    model = nn.DataParallel(model).cuda()\n","\n","    seg_loss = ComboLoss({'dice': 0.5, 'focal': 2.0}, per_image=False).cuda()\n","    ce_loss = nn.CrossEntropyLoss().cuda()\n","    seg_lossSeesaw = None#SeesawLoss2().cuda()\n","\n","    best_score = 0\n","    torch.cuda.empty_cache()\n","    for epoch in [33,34,35,36,37,38,39,40]:\n","        train_metrics = train_epoch(epoch, seg_loss, ce_loss, seg_lossSeesaw, model, optimizer, scheduler, train_data_loader)\n","        torch.cuda.empty_cache()\n","        if epoch % 2 == 0 and epoch >= 37:\n","          best_score, d = evaluate_val(val_data_loader, best_score, model, snapshot_name, epoch)\n","          torch.save(model.state_dict(), f'/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/{layer}_{ops}_{k_sizes}_{shape_types}2.pth')\n","        results = pd.read_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_3_top_train2.csv')\n","        try:\n","          results_iter = pd.DataFrame([[epoch, train_metrics[0], train_metrics[1].avg, train_metrics[2].avg, train_metrics[3].avg, layers, ops, k_sizes, shape_types, d[0], d[1], d[2], d[3], d[4], d[5], d[6]]],columns=results.columns)\n","        except:\n","          results_iter = pd.DataFrame([[epoch, train_metrics[0], train_metrics[1].avg, train_metrics[2].avg, train_metrics[3].avg, layers, ops, k_sizes, shape_types, None, None, None, None, None, None, None]],columns=results.columns)\n","        results = pd.concat([results,results_iter])\n","        d = None\n","        results.to_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_3_top_train2.csv', index=False)\n","\n","    try:\n","      #del model\n","      torch.cuda.empty_cache()\n","    except:\n","      None\n","\n","    elapsed = timeit.default_timer() - t0\n","    print('Time: {:.3f} min'.format(elapsed / 60))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PuLB9MTEuAD","executionInfo":{"status":"ok","timestamp":1696734753716,"user_tz":180,"elapsed":8689950,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"c4be1e66-afba-4389-a5da-6add1d54e1a8"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["epoch: 33; lr 0.0002020; Loss 0.0721 (0.0756); cce_loss 0.0721 (0.0756); Dice 0.5956 (0.5853): 100%|██████████| 1227/1227 [18:57<00:00,  1.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 33; lr 0.0002020; Loss 0.0756; CCE_loss 0.0756; Dice 0.5853\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 34; lr 0.0002020; Loss 0.0545 (0.0738); cce_loss 0.0545 (0.0738); Dice 0.6327 (0.5748): 100%|██████████| 1227/1227 [12:29<00:00,  1.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 34; lr 0.0002020; Loss 0.0738; CCE_loss 0.0738; Dice 0.5748\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 35; lr 0.0002020; Loss 0.1256 (0.0742); cce_loss 0.1256 (0.0742); Dice 0.5672 (0.5796): 100%|██████████| 1227/1227 [12:39<00:00,  1.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 35; lr 0.0002020; Loss 0.0742; CCE_loss 0.0742; Dice 0.5796\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 36; lr 0.0002020; Loss 0.2070 (0.0736); cce_loss 0.2070 (0.0736); Dice 0.6423 (0.5906): 100%|██████████| 1227/1227 [12:30<00:00,  1.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 36; lr 0.0002020; Loss 0.0736; CCE_loss 0.0736; Dice 0.5906\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 37; lr 0.0002020; Loss 0.0586 (0.0687); cce_loss 0.0586 (0.0687); Dice 0.5372 (0.5674): 100%|██████████| 1227/1227 [12:34<00:00,  1.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 37; lr 0.0000505; Loss 0.0687; CCE_loss 0.0687; Dice 0.5674\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 38; lr 0.0000505; Loss 0.0730 (0.0703); cce_loss 0.0730 (0.0703); Dice 0.4322 (0.5905): 100%|██████████| 1227/1227 [12:36<00:00,  1.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 38; lr 0.0001010; Loss 0.0703; CCE_loss 0.0703; Dice 0.5905\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 91/91 [09:28<00:00,  6.25s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.8134190049851064, Dice: 1.0, F1: 0.733455721407295, F1_0: 0.9374085192805923, F1_1: 0.5275361607165789, F1_2: 0.7436847195517206, F1_3: 0.8721346893383659\n","score: [0.8134190049851064, 1.0, 0.733455721407295, 0.9374085192805923, 0.5275361607165789, 0.7436847195517206, 0.8721346893383659]\tscore_best: 0.8134190049851064\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 39; lr 0.0001010; Loss 0.1077 (0.0670); cce_loss 0.1077 (0.0670); Dice 0.5588 (0.5725): 100%|██████████| 1227/1227 [30:04<00:00,  1.47s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 39; lr 0.0001010; Loss 0.0670; CCE_loss 0.0670; Dice 0.5725\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 40; lr 0.0001010; Loss 0.0503 (0.0667); cce_loss 0.0503 (0.0667); Dice 0.6148 (0.5737): 100%|██████████| 1227/1227 [14:56<00:00,  1.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 40; lr 0.0001010; Loss 0.0667; CCE_loss 0.0667; Dice 0.5737\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 91/91 [07:52<00:00,  5.19s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.817186338719071, Dice: 1.0, F1: 0.7388376267415301, F1_0: 0.9369578030262539, F1_1: 0.5381334700668324, F1_2: 0.7443513117024813, F1_3: 0.8734337535639449\n","score: [0.817186338719071, 1.0, 0.7388376267415301, 0.9369578030262539, 0.5381334700668324, 0.7443513117024813, 0.8734337535639449]\tscore_best: 0.817186338719071\n","Time: 144.829 min\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), f'/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/{layer}_{ops}_{k_sizes}_{shape_types}.pth')"],"metadata":{"id":"8HWZGOAHSs4o"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GNFOX5WcrlLP"},"outputs":[],"source":["#del model\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHGryCXssKzG"},"outputs":[],"source":["#torch.cuda.empty_cache()\n","#del model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRg_hj2MH2e7"},"outputs":[],"source":["#model.load_state_dict(torch.load('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/modelBDANet_test_train_3.pth'))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["xZ0IEwYcRzAl","TlRP-duNXnpK","yUSNtuVeTUwl","pTW3yt84TQMf","FXa807PbTjxn","Ifa7HjK2FVf3","tHCAM2vfRK7e"],"machine_shape":"hm","provenance":[],"gpuType":"A100","toc_visible":true,"authorship_tag":"ABX9TyPPiNbDvwbmfvrX2crFb8/0"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}