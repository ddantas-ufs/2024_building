{"cells":[{"cell_type":"markdown","metadata":{"id":"xZ0IEwYcRzAl"},"source":["## Requirements"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43159,"status":"ok","timestamp":1698149360276,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"iCeN-zCbXqfV","outputId":"12cb497b-ac1f-4c7d-90c1-3f2796145f64"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1698149360277,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"NI4_17gjk3dU","outputId":"7fab2f18-5f6e-4130-bdf4-24e0d52465bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}],"source":["print(1)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"k-JXF71-k4O3","executionInfo":{"status":"ok","timestamp":1698149360277,"user_tz":180,"elapsed":6,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["#import os\n","#os.kill(os.getpid(), 9)"]},{"cell_type":"markdown","metadata":{"id":"TlRP-duNXnpK"},"source":["## Iniciation"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109638,"status":"ok","timestamp":1698149469910,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"uOyfTgUDztSD","outputId":"a532a479-b740-429f-e54a-55d2bc7d3794"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 2.1.0+cu118\n","Uninstalling torch-2.1.0+cu118:\n","  Successfully uninstalled torch-2.1.0+cu118\n","Found existing installation: torchvision 0.16.0+cu118\n","Uninstalling torchvision-0.16.0+cu118:\n","  Successfully uninstalled torchvision-0.16.0+cu118\n","Found existing installation: torchaudio 2.1.0+cu118\n","Uninstalling torchaudio-2.1.0+cu118:\n","  Successfully uninstalled torchaudio-2.1.0+cu118\n","Looking in indexes: https://download.pytorch.org/whl/cu118\n","Collecting torch\n","  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp310-cp310-linux_x86_64.whl (2325.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m827.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision\n","  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp310-cp310-linux_x86_64.whl (6.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchaudio\n","  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp310-cp310-linux_x86_64.whl (3.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: torch, torchvision, torchaudio\n","Successfully installed torch-2.1.0+cu118 torchaudio-2.1.0+cu118 torchvision-0.16.0+cu118\n"]}],"source":["!pip uninstall torch torchvision torchaudio -y\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1698149469910,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"dSxd1Sjk6_2a","outputId":"e5cff614-27a5-48d9-90ca-6801b4a2cda1"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1438,"status":"ok","timestamp":1698149471344,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"K0WhAIMi0KX1","outputId":"1f775431-2475-433d-b1e8-a3a6cae835fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.1.0+cu118\n"]}],"source":["import torch\n","print(torch.__version__)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":716,"status":"ok","timestamp":1698149472055,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"ejMJGR8w0nmM","outputId":"b12077f4-4146-4671-b2d4-ea30ffcae11a"},"outputs":[{"output_type":"stream","name":"stdout","text":["absl-py==1.4.0\n","aiohttp==3.8.6\n","aiosignal==1.3.1\n","alabaster==0.7.13\n","albumentations==1.3.1\n","altair==4.2.2\n","anyio==3.7.1\n","appdirs==1.4.4\n","argon2-cffi==23.1.0\n","argon2-cffi-bindings==21.2.0\n","array-record==0.5.0\n","arviz==0.15.1\n","astropy==5.3.4\n","astunparse==1.6.3\n","async-timeout==4.0.3\n","atpublic==4.0\n","attrs==23.1.0\n","audioread==3.0.1\n","autograd==1.6.2\n","Babel==2.13.0\n","backcall==0.2.0\n","beautifulsoup4==4.11.2\n","bidict==0.22.1\n","bigframes==0.10.0\n","bleach==6.1.0\n","blinker==1.4\n","blis==0.7.11\n","blosc2==2.0.0\n","bokeh==3.2.2\n","bqplot==0.12.42\n","branca==0.6.0\n","build==1.0.3\n","CacheControl==0.13.1\n","cachetools==5.3.1\n","catalogue==2.0.10\n","certifi==2023.7.22\n","cffi==1.16.0\n","chardet==5.2.0\n","charset-normalizer==3.3.0\n","chex==0.1.7\n","click==8.1.7\n","click-plugins==1.1.1\n","cligj==0.7.2\n","cloudpickle==2.2.1\n","cmake==3.27.7\n","cmdstanpy==1.2.0\n","colorcet==3.0.1\n","colorlover==0.3.0\n","colour==0.1.5\n","community==1.0.0b1\n","confection==0.1.3\n","cons==0.4.6\n","contextlib2==21.6.0\n","contourpy==1.1.1\n","cryptography==41.0.4\n","cufflinks==0.17.3\n","cupy-cuda11x==11.0.0\n","cvxopt==1.3.2\n","cvxpy==1.3.2\n","cycler==0.12.1\n","cymem==2.0.8\n","Cython==3.0.4\n","dask==2023.8.1\n","datascience==0.17.6\n","db-dtypes==1.1.1\n","dbus-python==1.2.18\n","debugpy==1.6.6\n","decorator==4.4.2\n","defusedxml==0.7.1\n","diskcache==5.6.3\n","distributed==2023.8.1\n","distro==1.7.0\n","dlib==19.24.2\n","dm-tree==0.1.8\n","docutils==0.18.1\n","dopamine-rl==4.0.6\n","duckdb==0.8.1\n","earthengine-api==0.1.375\n","easydict==1.10\n","ecos==2.0.12\n","editdistance==0.6.2\n","eerepr==0.0.4\n","en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl#sha256=83276fc78a70045627144786b52e1f2728ad5e29e5e43916ec37ea9c26a11212\n","entrypoints==0.4\n","et-xmlfile==1.1.0\n","etils==1.5.1\n","etuples==0.3.9\n","exceptiongroup==1.1.3\n","fastai==2.7.13\n","fastcore==1.5.29\n","fastdownload==0.0.7\n","fastjsonschema==2.18.1\n","fastprogress==1.0.3\n","fastrlock==0.8.2\n","filelock==3.12.4\n","fiona==1.9.5\n","firebase-admin==5.3.0\n","Flask==2.2.5\n","flatbuffers==23.5.26\n","flax==0.7.4\n","folium==0.14.0\n","fonttools==4.43.1\n","frozendict==2.3.8\n","frozenlist==1.4.0\n","fsspec==2023.6.0\n","future==0.18.3\n","gast==0.5.4\n","gcsfs==2023.6.0\n","GDAL==3.4.3\n","gdown==4.6.6\n","geemap==0.28.2\n","gensim==4.3.2\n","geocoder==1.38.1\n","geographiclib==2.0\n","geopandas==0.13.2\n","geopy==2.3.0\n","gin-config==0.5.0\n","glob2==0.7\n","google==2.0.3\n","google-api-core==2.11.1\n","google-api-python-client==2.84.0\n","google-auth==2.17.3\n","google-auth-httplib2==0.1.1\n","google-auth-oauthlib==1.0.0\n","google-cloud-bigquery==3.12.0\n","google-cloud-bigquery-connection==1.12.1\n","google-cloud-bigquery-storage==2.22.0\n","google-cloud-core==2.3.3\n","google-cloud-datastore==2.15.2\n","google-cloud-firestore==2.11.1\n","google-cloud-functions==1.13.3\n","google-cloud-iam==2.12.2\n","google-cloud-language==2.9.1\n","google-cloud-resource-manager==1.10.4\n","google-cloud-storage==2.8.0\n","google-cloud-translate==3.11.3\n","google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz#sha256=c5a2da9a372b66a92fd9c3331efca7e591a029ca013967a39a7a18ff1d2fef5d\n","google-crc32c==1.5.0\n","google-pasta==0.2.0\n","google-resumable-media==2.6.0\n","googleapis-common-protos==1.61.0\n","googledrivedownloader==0.4\n","graphviz==0.20.1\n","greenlet==3.0.0\n","grpc-google-iam-v1==0.12.6\n","grpcio==1.59.0\n","grpcio-status==1.48.2\n","gspread==3.4.2\n","gspread-dataframe==3.3.1\n","gym==0.25.2\n","gym-notices==0.0.8\n","h5netcdf==1.2.0\n","h5py==3.9.0\n","holidays==0.35\n","holoviews==1.17.1\n","html5lib==1.1\n","httpimport==1.3.1\n","httplib2==0.22.0\n","humanize==4.7.0\n","hyperopt==0.2.7\n","ibis-framework==6.2.0\n","idna==3.4\n","imageio==2.31.5\n","imageio-ffmpeg==0.4.9\n","imagesize==1.4.1\n","imbalanced-learn==0.10.1\n","imgaug==0.4.0\n","importlib-metadata==6.8.0\n","importlib-resources==6.1.0\n","imutils==0.5.4\n","inflect==7.0.0\n","iniconfig==2.0.0\n","install==1.3.5\n","intel-openmp==2023.2.0\n","ipyevents==2.0.2\n","ipyfilechooser==0.6.0\n","ipykernel==5.5.6\n","ipyleaflet==0.17.4\n","ipython==7.34.0\n","ipython-genutils==0.2.0\n","ipython-sql==0.5.0\n","ipytree==0.2.2\n","ipywidgets==7.7.1\n","itsdangerous==2.1.2\n","jax==0.4.16\n","jaxlib @ https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.16+cuda11.cudnn86-cp310-cp310-manylinux2014_x86_64.whl#sha256=78b3a9acfda4bfaae8a1dc112995d56454020f5c02dba4d24c40c906332efd4a\n","jeepney==0.7.1\n","jieba==0.42.1\n","Jinja2==3.1.2\n","joblib==1.3.2\n","jsonpickle==3.0.2\n","jsonschema==4.19.1\n","jsonschema-specifications==2023.7.1\n","jupyter-client==6.1.12\n","jupyter-console==6.1.0\n","jupyter-server==1.24.0\n","jupyter_core==5.4.0\n","jupyterlab-pygments==0.2.2\n","jupyterlab-widgets==3.0.9\n","kaggle==1.5.16\n","keras==2.14.0\n","keyring==23.5.0\n","kiwisolver==1.4.5\n","langcodes==3.3.0\n","launchpadlib==1.10.16\n","lazr.restfulclient==0.14.4\n","lazr.uri==1.0.6\n","lazy_loader==0.3\n","libclang==16.0.6\n","librosa==0.10.1\n","lida==0.0.10\n","lightgbm==4.0.0\n","linkify-it-py==2.0.2\n","llmx==0.0.15a0\n","llvmlite==0.39.1\n","locket==1.0.0\n","logical-unification==0.4.6\n","lxml==4.9.3\n","malloy==2023.1058\n","Markdown==3.5\n","markdown-it-py==3.0.0\n","MarkupSafe==2.1.3\n","matplotlib==3.7.1\n","matplotlib-inline==0.1.6\n","matplotlib-venn==0.11.9\n","mdit-py-plugins==0.4.0\n","mdurl==0.1.2\n","miniKanren==1.0.3\n","missingno==0.5.2\n","mistune==0.8.4\n","mizani==0.9.3\n","mkl==2023.2.0\n","ml-dtypes==0.2.0\n","mlxtend==0.22.0\n","more-itertools==10.1.0\n","moviepy==1.0.3\n","mpmath==1.3.0\n","msgpack==1.0.7\n","multidict==6.0.4\n","multipledispatch==1.0.0\n","multitasking==0.0.11\n","murmurhash==1.0.10\n","music21==9.1.0\n","natsort==8.4.0\n","nbclassic==1.0.0\n","nbclient==0.8.0\n","nbconvert==6.5.4\n","nbformat==5.9.2\n","nest-asyncio==1.5.8\n","networkx==3.2\n","nibabel==4.0.2\n","nltk==3.8.1\n","notebook==6.5.5\n","notebook_shim==0.2.3\n","numba==0.56.4\n","numexpr==2.8.7\n","numpy==1.23.5\n","oauth2client==4.1.3\n","oauthlib==3.2.2\n","opencv-contrib-python==4.8.0.76\n","opencv-python==4.8.0.76\n","opencv-python-headless==4.8.1.78\n","openpyxl==3.1.2\n","opt-einsum==3.3.0\n","optax==0.1.7\n","orbax-checkpoint==0.4.1\n","osqp==0.6.2.post8\n","packaging==23.2\n","pandas==1.5.3\n","pandas-datareader==0.10.0\n","pandas-gbq==0.17.9\n","pandas-stubs==1.5.3.230304\n","pandocfilters==1.5.0\n","panel==1.2.3\n","param==1.13.0\n","parso==0.8.3\n","parsy==2.1\n","partd==1.4.1\n","pathlib==1.0.1\n","pathy==0.10.2\n","patsy==0.5.3\n","peewee==3.17.0\n","pexpect==4.8.0\n","pickleshare==0.7.5\n","Pillow==9.4.0\n","pip-tools==6.13.0\n","platformdirs==3.11.0\n","plotly==5.15.0\n","plotnine==0.12.3\n","pluggy==1.3.0\n","polars==0.17.3\n","pooch==1.7.0\n","portpicker==1.5.2\n","prefetch-generator==1.0.3\n","preshed==3.0.9\n","prettytable==3.9.0\n","proglog==0.1.10\n","progressbar2==4.2.0\n","prometheus-client==0.17.1\n","promise==2.3\n","prompt-toolkit==3.0.39\n","prophet==1.1.5\n","proto-plus==1.22.3\n","protobuf==3.20.3\n","psutil==5.9.5\n","psycopg2==2.9.9\n","ptyprocess==0.7.0\n","py-cpuinfo==9.0.0\n","py4j==0.10.9.7\n","pyarrow==9.0.0\n","pyasn1==0.5.0\n","pyasn1-modules==0.3.0\n","pycocotools==2.0.7\n","pycparser==2.21\n","pyct==0.5.0\n","pydantic==1.10.13\n","pydata-google-auth==1.8.2\n","pydot==1.4.2\n","pydot-ng==2.0.0\n","pydotplus==2.0.2\n","PyDrive==1.3.1\n","PyDrive2==1.6.3\n","pyerfa==2.0.1.1\n","pygame==2.5.2\n","Pygments==2.16.1\n","PyGObject==3.42.1\n","PyJWT==2.3.0\n","pymc==5.7.2\n","pymystem3==0.2.0\n","PyOpenGL==3.1.7\n","pyOpenSSL==23.2.0\n","pyparsing==3.1.1\n","pyperclip==1.8.2\n","pyproj==3.6.1\n","pyproject_hooks==1.0.0\n","pyshp==2.3.1\n","PySocks==1.7.1\n","pytensor==2.14.2\n","pytest==7.4.2\n","python-apt==0.0.0\n","python-box==7.1.1\n","python-dateutil==2.8.2\n","python-louvain==0.16\n","python-slugify==8.0.1\n","python-utils==3.8.1\n","pytz==2023.3.post1\n","pyviz_comms==3.0.0\n","PyWavelets==1.4.1\n","PyYAML==6.0.1\n","pyzmq==23.2.1\n","qdldl==0.1.7.post0\n","qudida==0.0.4\n","ratelim==0.1.6\n","referencing==0.30.2\n","regex==2023.6.3\n","requests==2.31.0\n","requests-oauthlib==1.3.1\n","requirements-parser==0.5.0\n","rich==13.6.0\n","rpds-py==0.10.6\n","rpy2==3.4.2\n","rsa==4.9\n","scikit-image==0.19.3\n","scikit-learn==1.2.2\n","scipy==1.11.3\n","scooby==0.8.0\n","scs==3.2.3\n","seaborn==0.12.2\n","SecretStorage==3.3.1\n","Send2Trash==1.8.2\n","shapely==2.0.2\n","six==1.16.0\n","sklearn-pandas==2.2.0\n","smart-open==6.4.0\n","sniffio==1.3.0\n","snowballstemmer==2.2.0\n","sortedcontainers==2.4.0\n","soundfile==0.12.1\n","soupsieve==2.5\n","soxr==0.3.7\n","spacy==3.6.1\n","spacy-legacy==3.0.12\n","spacy-loggers==1.0.5\n","Sphinx==5.0.2\n","sphinxcontrib-applehelp==1.0.7\n","sphinxcontrib-devhelp==1.0.5\n","sphinxcontrib-htmlhelp==2.0.4\n","sphinxcontrib-jsmath==1.0.1\n","sphinxcontrib-qthelp==1.0.6\n","sphinxcontrib-serializinghtml==1.1.9\n","SQLAlchemy==2.0.22\n","sqlglot==17.16.2\n","sqlparse==0.4.4\n","srsly==2.4.8\n","stanio==0.3.0\n","statsmodels==0.14.0\n","sympy==1.12\n","tables==3.8.0\n","tabulate==0.9.0\n","tbb==2021.10.0\n","tblib==2.0.0\n","tenacity==8.2.3\n","tensorboard==2.14.1\n","tensorboard-data-server==0.7.1\n","tensorflow==2.14.0\n","tensorflow-datasets==4.9.3\n","tensorflow-estimator==2.14.0\n","tensorflow-gcs-config==2.14.0\n","tensorflow-hub==0.15.0\n","tensorflow-io-gcs-filesystem==0.34.0\n","tensorflow-metadata==1.14.0\n","tensorflow-probability==0.22.0\n","tensorstore==0.1.45\n","termcolor==2.3.0\n","terminado==0.17.1\n","text-unidecode==1.3\n","textblob==0.17.1\n","tf-slim==1.1.0\n","thinc==8.1.12\n","threadpoolctl==3.2.0\n","tifffile==2023.9.26\n","tinycss2==1.2.1\n","toml==0.10.2\n","tomli==2.0.1\n","toolz==0.12.0\n","torch==2.1.0+cu118\n","torchaudio==2.1.0+cu118\n","torchdata==0.7.0\n","torchsummary==1.5.1\n","torchtext==0.16.0\n","torchvision==0.16.0+cu118\n","tornado==6.3.2\n","tqdm==4.66.1\n","traitlets==5.7.1\n","traittypes==0.2.1\n","triton==2.1.0\n","tweepy==4.13.0\n","typer==0.9.0\n","types-pytz==2023.3.1.1\n","types-setuptools==68.2.0.0\n","typing_extensions==4.5.0\n","tzlocal==5.1\n","uc-micro-py==1.0.2\n","uritemplate==4.1.1\n","urllib3==2.0.7\n","vega-datasets==0.9.0\n","wadllib==1.3.6\n","wasabi==1.1.2\n","wcwidth==0.2.8\n","webcolors==1.13\n","webencodings==0.5.1\n","websocket-client==1.6.4\n","Werkzeug==3.0.0\n","widgetsnbextension==3.6.6\n","wordcloud==1.9.2\n","wrapt==1.14.1\n","xarray==2023.7.0\n","xarray-einstats==0.6.0\n","xgboost==2.0.0\n","xlrd==2.0.1\n","xxhash==3.4.1\n","xyzservices==2023.10.0\n","yarl==1.9.2\n","yellowbrick==1.5\n","yfinance==0.2.31\n","zict==3.0.0\n","zipp==3.17.0\n"]}],"source":["!pip freeze"]},{"cell_type":"markdown","metadata":{"id":"yUSNtuVeTUwl"},"source":["## Optimazer"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"HZMtCepVRDaJ","executionInfo":{"status":"ok","timestamp":1698149472056,"user_tz":180,"elapsed":4,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["# Based on https://github.com/pytorch/pytorch/pull/3740\n","import torch\n","import math\n","\n","\n","class AdamW(torch.optim.Optimizer):\n","    \"\"\"Implements AdamW algorithm.\n","\n","    It has been proposed in `Fixing Weight Decay Regularization in Adam`_.\n","\n","    Arguments:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float, optional): learning rate (default: 1e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of gradient and its square (default: (0.9, 0.999))\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability (default: 1e-8)\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","\n","    .. Fixing Weight Decay Regularization in Adam:\n","    https://arxiv.org/abs/1711.05101\n","    \"\"\"\n","\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=0):\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay)\n","        super(AdamW, self).__init__(params, defaults)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n","\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","\n","                # according to the paper, this penalty should come after the bias correction\n","                # if group['weight_decay'] != 0:\n","                #     grad = grad.add(group['weight_decay'], p.data)\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","\n","                denom = exp_avg_sq.sqrt().add_(group['eps'])\n","\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n","\n","                # w = w - wd * lr * w\n","                if group['weight_decay'] != 0:\n","                    p.data.add_(-group['weight_decay'] * group['lr'], p.data)\n","\n","                # w = w - lr * w.grad\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                # w = w - wd * lr * w - lr * w.grad\n","                # See http://www.fast.ai/2018/07/02/adam-weight-decay/\n","\n","        return loss"]},{"cell_type":"markdown","metadata":{"id":"pTW3yt84TQMf"},"source":["## Losses"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"MR5-L7m_SCwH","executionInfo":{"status":"ok","timestamp":1698149472596,"user_tz":180,"elapsed":543,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch.autograd import Variable\n","\n","try:\n","    from itertools import ifilterfalse\n","except ImportError:  # py3k\n","    from itertools import filterfalse\n","\n","eps = 1e-6\n","\n","def dice_round(preds, trues):\n","    preds = preds.float()\n","    return soft_dice_loss(preds, trues)\n","\n","\n","def iou_round(preds, trues):\n","    preds = preds.float()\n","    return jaccard(preds, trues)\n","\n","\n","def soft_dice_loss(outputs, targets, per_image=False):\n","    batch_size = outputs.size()[0]\n","    if not per_image:\n","        batch_size = 1\n","    dice_target = targets.contiguous().view(batch_size, -1).float()\n","    dice_output = outputs.contiguous().view(batch_size, -1)\n","    intersection = torch.sum(dice_output * dice_target, dim=1)\n","    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1) + eps\n","    loss = (1 - (2 * intersection + eps) / union).mean()\n","    return loss\n","\n","\n","def jaccard(outputs, targets, per_image=False):\n","    batch_size = outputs.size()[0]\n","    if not per_image:\n","        batch_size = 1\n","    dice_target = targets.contiguous().view(batch_size, -1).float()\n","    dice_output = outputs.contiguous().view(batch_size, -1)\n","    intersection = torch.sum(dice_output * dice_target, dim=1)\n","    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1) - intersection + eps\n","    losses = 1 - (intersection + eps) / union\n","    return losses.mean()\n","\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True, per_image=False):\n","        super().__init__()\n","        self.size_average = size_average\n","        self.register_buffer('weight', weight)\n","        self.per_image = per_image\n","\n","    def forward(self, input, target):\n","        return soft_dice_loss(input, target, per_image=self.per_image)\n","\n","\n","class JaccardLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True, per_image=False):\n","        super().__init__()\n","        self.size_average = size_average\n","        self.register_buffer('weight', weight)\n","        self.per_image = per_image\n","\n","    def forward(self, input, target):\n","        return jaccard(input, target, per_image=self.per_image)\n","\n","\n","class StableBCELoss(nn.Module):\n","    def __init__(self):\n","        super(StableBCELoss, self).__init__()\n","\n","    def forward(self, input, target):\n","        input = input.float().view(-1)\n","        target = target.float().view(-1)\n","        neg_abs = - input.abs()\n","        # todo check correctness\n","        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n","        return loss.mean()\n","\n","\n","class ComboLoss(nn.Module):\n","    def __init__(self, weights, per_image=False):\n","        super().__init__()\n","        self.weights = weights\n","        self.bce = StableBCELoss()\n","        self.dice = DiceLoss(per_image=False)\n","        self.jaccard = JaccardLoss(per_image=False)\n","        self.lovasz = LovaszLoss(per_image=per_image)\n","        self.lovasz_sigmoid = LovaszLossSigmoid(per_image=per_image)\n","        self.focal = FocalLoss2d()\n","        self.mapping = {'bce': self.bce,\n","                        'dice': self.dice,\n","                        'focal': self.focal,\n","                        'jaccard': self.jaccard,\n","                        'lovasz': self.lovasz,\n","                        'lovasz_sigmoid': self.lovasz_sigmoid}\n","        self.expect_sigmoid = {'dice', 'focal', 'jaccard', 'lovasz_sigmoid'}\n","        self.values = {}\n","\n","    def forward(self, outputs, targets):\n","        loss = 0\n","        weights = self.weights\n","        sigmoid_input = torch.sigmoid(outputs)\n","        for k, v in weights.items():\n","            if not v:\n","                continue\n","            val = self.mapping[k](sigmoid_input if k in self.expect_sigmoid else outputs, targets)\n","            self.values[k] = val\n","            loss += self.weights[k] * val\n","        return loss\n","\n","\n","def lovasz_grad(gt_sorted):\n","    \"\"\"\n","    Computes gradient of the Lovasz extension w.r.t sorted errors\n","    See Alg. 1 in paper\n","    \"\"\"\n","    p = len(gt_sorted)\n","    gts = gt_sorted.sum()\n","    intersection = gts.float() - gt_sorted.float().cumsum(0)\n","    union = gts.float() + (1 - gt_sorted).float().cumsum(0)\n","    jaccard = 1. - intersection / union\n","    if p > 1:  # cover 1-pixel case\n","        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n","    return jaccard\n","\n","\n","def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n","      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class id\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n","                    for log, lab in zip(logits, labels))\n","    else:\n","        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n","    return loss\n","\n","\n","def lovasz_hinge_flat(logits, labels):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n","      labels: [P] Tensor, binary ground truth labels (0 or 1)\n","      ignore: label to ignore\n","    \"\"\"\n","    if len(labels) == 0:\n","        # only void pixels, the gradients should be 0\n","        return logits.sum() * 0.\n","    signs = 2. * labels.float() - 1.\n","    errors = (1. - logits * Variable(signs))\n","    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n","    perm = perm.data\n","    gt_sorted = labels[perm]\n","    grad = lovasz_grad(gt_sorted)\n","    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n","    return loss\n","\n","\n","def flatten_binary_scores(scores, labels, ignore=None):\n","    \"\"\"\n","    Flattens predictions in the batch (binary case)\n","    Remove labels equal to 'ignore'\n","    \"\"\"\n","    scores = scores.view(-1)\n","    labels = labels.view(-1)\n","    if ignore is None:\n","        return scores, labels\n","    valid = (labels != ignore)\n","    vscores = scores[valid]\n","    vlabels = labels[valid]\n","    return vscores, vlabels\n","\n","\n","def lovasz_sigmoid(probas, labels, per_image=False, ignore=None):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n","      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n","      only_present: average only on classes present in ground truth\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class labels\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_sigmoid_flat(*flatten_binary_scores(prob.unsqueeze(0), lab.unsqueeze(0), ignore))\n","                          for prob, lab in zip(probas, labels))\n","    else:\n","        loss = lovasz_sigmoid_flat(*flatten_binary_scores(probas, labels, ignore))\n","    return loss\n","\n","\n","def lovasz_sigmoid_flat(probas, labels):\n","    \"\"\"\n","    Multi-class Lovasz-Softmax loss\n","      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n","      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n","      only_present: average only on classes present in ground truth\n","    \"\"\"\n","    fg = labels.float()\n","    errors = (Variable(fg) - probas).abs()\n","    errors_sorted, perm = torch.sort(errors, 0, descending=True)\n","    perm = perm.data\n","    fg_sorted = fg[perm]\n","    loss = torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted)))\n","    return loss\n","\n","\n","def mean(l, ignore_nan=False, empty=0):\n","    \"\"\"\n","    nanmean compatible with generators.\n","    \"\"\"\n","    l = iter(l)\n","    if ignore_nan:\n","        l = ifilterfalse(np.isnan, l)\n","    try:\n","        n = 1\n","        acc = next(l)\n","    except StopIteration:\n","        if empty == 'raise':\n","            raise ValueError('Empty mean')\n","        return empty\n","    for n, v in enumerate(l, 2):\n","        acc += v\n","    if n == 1:\n","        return acc\n","    return acc / n\n","\n","\n","class LovaszLoss(nn.Module):\n","    def __init__(self, ignore_index=255, per_image=True):\n","        super().__init__()\n","        self.ignore_index = ignore_index\n","        self.per_image = per_image\n","\n","    def forward(self, outputs, targets):\n","        outputs = outputs.contiguous()\n","        targets = targets.contiguous()\n","        return lovasz_hinge(outputs, targets, per_image=self.per_image, ignore=self.ignore_index)\n","\n","\n","class LovaszLossSigmoid(nn.Module):\n","    def __init__(self, ignore_index=255, per_image=True):\n","        super().__init__()\n","        self.ignore_index = ignore_index\n","        self.per_image = per_image\n","\n","    def forward(self, outputs, targets):\n","        outputs = outputs.contiguous()\n","        targets = targets.contiguous()\n","        return lovasz_sigmoid(outputs, targets, per_image=self.per_image, ignore=self.ignore_index)\n","\n","\n","class FocalLoss2d(nn.Module):\n","    def __init__(self, gamma=2, ignore_index=255):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.ignore_index = ignore_index\n","\n","    def forward(self, outputs, targets):\n","        outputs = outputs.contiguous()\n","        targets = targets.contiguous()\n","        # eps = 1e-8\n","        non_ignored = targets.view(-1) != self.ignore_index\n","        targets = targets.view(-1)[non_ignored].float()\n","        outputs = outputs.contiguous().view(-1)[non_ignored]\n","        outputs = torch.clamp(outputs, eps, 1. - eps)\n","        targets = torch.clamp(targets, eps, 1. - eps)\n","        pt = (1 - targets) * (1 - outputs) + targets * outputs\n","        return (-(1. - pt) ** self.gamma * torch.log(pt)).mean()"]},{"cell_type":"markdown","metadata":{"id":"FXa807PbTjxn"},"source":["## Utils"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"7Fs2XBvfToVf","executionInfo":{"status":"ok","timestamp":1698149472596,"user_tz":180,"elapsed":6,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","#### Augmentations\n","def shift_image(img, shift_pnt):\n","    M = np.float32([[1, 0, shift_pnt[0]], [0, 1, shift_pnt[1]]])\n","    res = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]), borderMode=cv2.BORDER_REFLECT_101)\n","    return res\n","\n","\n","def rotate_image(image, angle, scale, rot_pnt):\n","    rot_mat = cv2.getRotationMatrix2D(rot_pnt, angle, scale)\n","    result = cv2.warpAffine(image, rot_mat, (image.shape[1], image.shape[0]), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101) #INTER_NEAREST\n","    return result\n","\n","\n","def gauss_noise(img, var=30):\n","    row, col, ch = img.shape\n","    mean = var\n","    sigma = var**0.5\n","    gauss = np.random.normal(mean,sigma,(row,col,ch))\n","    gauss = gauss.reshape(row,col,ch)\n","    gauss = (gauss - np.min(gauss)).astype(np.uint8)\n","    return np.clip(img.astype(np.int32) + gauss, 0, 255).astype('uint8')\n","\n","\n","def clahe(img, clipLimit=2.0, tileGridSize=(5,5)):\n","    img_yuv = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n","    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n","    img_yuv[:, :, 0] = clahe.apply(img_yuv[:, :, 0])\n","    img_output = cv2.cvtColor(img_yuv, cv2.COLOR_LAB2RGB)\n","    return img_output\n","\n","\n","def _blend(img1, img2, alpha):\n","    return np.clip(img1 * alpha + (1 - alpha) * img2, 0, 255).astype('uint8')\n","\n","\n","_alpha = np.asarray([0.114, 0.587, 0.299]).reshape((1, 1, 3))\n","def _grayscale(img):\n","    return np.sum(_alpha * img, axis=2, keepdims=True)\n","\n","\n","def saturation(img, alpha):\n","    gs = _grayscale(img)\n","    return _blend(img, gs, alpha)\n","\n","\n","def brightness(img, alpha):\n","    gs = np.zeros_like(img)\n","    return _blend(img, gs, alpha)\n","\n","\n","def contrast(img, alpha):\n","    gs = _grayscale(img)\n","    gs = np.repeat(gs.mean(), 3)\n","    return _blend(img, gs, alpha)\n","\n","\n","def change_hsv(img, h, s, v):\n","    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","    hsv = hsv.astype(int)\n","    hsv[:,:,0] += h\n","    hsv[:,:,0] = np.clip(hsv[:,:,0], 0, 255)\n","    hsv[:,:,1] += s\n","    hsv[:,:,1] = np.clip(hsv[:,:,1], 0, 255)\n","    hsv[:,:,2] += v\n","    hsv[:,:,2] = np.clip(hsv[:,:,2], 0, 255)\n","    hsv = hsv.astype('uint8')\n","    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","    return img\n","\n","def shift_channels(img, b_shift, g_shift, r_shift):\n","    img = img.astype(int)\n","    img[:,:,0] += b_shift\n","    img[:,:,0] = np.clip(img[:,:,0], 0, 255)\n","    img[:,:,1] += g_shift\n","    img[:,:,1] = np.clip(img[:,:,1], 0, 255)\n","    img[:,:,2] += r_shift\n","    img[:,:,2] = np.clip(img[:,:,2], 0, 255)\n","    img = img.astype('uint8')\n","    return img\n","\n","def invert(img):\n","    return 255 - img\n","\n","def channel_shuffle(img):\n","    ch_arr = [0, 1, 2]\n","    np.random.shuffle(ch_arr)\n","    img = img[..., ch_arr]\n","    return img\n","\n","#######\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","\n","def preprocess_inputs(x):\n","    x = np.asarray(x, dtype='float32')\n","    x /= 127\n","    x -= 1\n","    return x\n","\n","\n","def dice(im1, im2, empty_score=1.0):\n","    \"\"\"\n","    Computes the Dice coefficient, a measure of set similarity.\n","    Parameters\n","    ----------\n","    im1 : array-like, bool\n","        Any array of arbitrary size. If not boolean, will be converted.\n","    im2 : array-like, bool\n","        Any other array of identical size. If not boolean, will be converted.\n","    Returns\n","    -------\n","    dice : float\n","        Dice coefficient as a float on range [0,1].\n","        Maximum similarity = 1\n","        No similarity = 0\n","        Both are empty (sum eq to zero) = empty_score\n","\n","    Notes\n","    -----\n","    The order of inputs for `dice` is irrelevant. The result will be\n","    identical if `im1` and `im2` are switched.\n","    \"\"\"\n","    im1 = np.asarray(im1).astype(np.bool)\n","    im2 = np.asarray(im2).astype(np.bool)\n","\n","    if im1.shape != im2.shape:\n","        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n","\n","    im_sum = im1.sum() + im2.sum()\n","    if im_sum == 0:\n","        return empty_score\n","\n","    # Compute Dice coefficient\n","    intersection = np.logical_and(im1, im2)\n","\n","    return 2. * intersection.sum() / im_sum\n","\n","\n","def iou(im1, im2, empty_score=1.0):\n","    im1 = np.asarray(im1).astype(np.bool)\n","    im2 = np.asarray(im2).astype(np.bool)\n","\n","    if im1.shape != im2.shape:\n","        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n","\n","    union = np.logical_or(im1, im2)\n","    im_sum = union.sum()\n","    if im_sum == 0:\n","        return empty_score\n","\n","    # Compute Dice coefficient\n","    intersection = np.logical_and(im1, im2)\n","\n","    return intersection.sum() / im_sum"]},{"cell_type":"markdown","metadata":{"id":"Ifa7HjK2FVf3"},"source":["## modelMscale"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"RoOkAXB2n4nS","executionInfo":{"status":"ok","timestamp":1698149477686,"user_tz":180,"elapsed":2793,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import Conv2D, Activation, UpSampling2D, Concatenate, Input\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.initializers import HeNormal\n","from tensorflow.keras import backend as K\n","\n","class ConvRelu(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3):\n","        super(ConvRelu, self).__init__()\n","        self.layer = nn.Sequential(\n","            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1),\n","            nn.ReLU(inplace=True, )\n","        )\n","    #@autocast()\n","    def forward(self, x):\n","        return self.layer(x)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"sDIsasbMn5W8","executionInfo":{"status":"ok","timestamp":1698149477686,"user_tz":180,"elapsed":7,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["\"\"\"\n","ResNet code gently borrowed from\n","https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n","\"\"\"\n","\n","from collections import OrderedDict\n","import math\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils import model_zoo\n","\n","__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n","           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n","\n","pretrained_settings = {\n","    'senet154': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnet50': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnet101': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnet152': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnext50_32x4d': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","    'se_resnext101_32x4d': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 224, 224],\n","            'input_range': [0, 1],\n","            'mean': [0.485, 0.456, 0.406],\n","            'std': [0.229, 0.224, 0.225],\n","            'num_classes': 1000\n","        }\n","    },\n","}\n","\n","\n","class SEModule(nn.Module):\n","\n","    def __init__(self, channels, reduction, concat=False):\n","        super(SEModule, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n","                             padding=0)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n","                             padding=0)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        module_input = x\n","        x = self.avg_pool(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.sigmoid(x)\n","        return module_input * x\n","\n","class SCSEModule(nn.Module):\n","    # according to https://arxiv.org/pdf/1808.08127.pdf concat is better\n","    def __init__(self, channels, reduction=16, concat=False):\n","        super(SCSEModule, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n","                             padding=0)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n","                             padding=0)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        self.spatial_se = nn.Sequential(nn.Conv2d(channels, 1, kernel_size=1,\n","                                                  stride=1, padding=0, bias=False),\n","                                        nn.Sigmoid())\n","        self.concat = concat\n","\n","    def forward(self, x):\n","        module_input = x\n","\n","        x = self.avg_pool(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        chn_se = self.sigmoid(x)\n","        chn_se = chn_se * module_input\n","\n","        spa_se = self.spatial_se(module_input)\n","        spa_se = module_input * spa_se\n","        if self.concat:\n","            return torch.cat([chn_se, spa_se], dim=1)\n","        else:\n","            return chn_se + spa_se\n","\n","class Bottleneck(nn.Module):\n","    \"\"\"\n","    Base class for bottlenecks that implements `forward()` method.\n","    \"\"\"\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out = self.se_module(out) + residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class SEBottleneck(Bottleneck):\n","    \"\"\"\n","    Bottleneck for SENet154.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None):\n","        super(SEBottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes * 2)\n","        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n","                               stride=stride, padding=1, groups=groups,\n","                               bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes * 4)\n","        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n","                               bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SCSEBottleneck(Bottleneck):\n","    \"\"\"\n","    Bottleneck for SENet154.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None):\n","        super(SCSEBottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes * 2)\n","        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n","                               stride=stride, padding=1, groups=groups,\n","                               bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes * 4)\n","        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n","                               bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SCSEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SEResNetBottleneck(Bottleneck):\n","    \"\"\"\n","    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n","    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n","    (the latter is used in the torchvision implementation of ResNet).\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None):\n","        super(SEResNetBottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n","                               stride=stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n","                               groups=groups, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SEResNeXtBottleneck(Bottleneck):\n","    \"\"\"\n","    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None, base_width=4):\n","        super(SEResNeXtBottleneck, self).__init__()\n","        width = math.floor(planes * (base_width / 64)) * groups\n","        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n","                               stride=1)\n","        self.bn1 = nn.BatchNorm2d(width)\n","        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n","                               padding=1, groups=groups, bias=False)\n","        self.bn2 = nn.BatchNorm2d(width)\n","        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","\n","class SCSEResNeXtBottleneck(Bottleneck):\n","    \"\"\"\n","    ResNeXt bottleneck type C with a Concurrent Spatial Squeeze-and-Excitation module.\n","    \"\"\"\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n","                 downsample=None, base_width=4, final=False):\n","        super(SCSEResNeXtBottleneck, self).__init__()\n","        width = math.floor(planes * (base_width / 64)) * groups\n","        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n","                               stride=1)\n","        self.bn1 = nn.BatchNorm2d(width)\n","        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n","                               padding=1, groups=groups, bias=False)\n","        self.bn2 = nn.BatchNorm2d(width)\n","        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.se_module = SCSEModule(planes * 4, reduction=reduction)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","\n","class SENet(nn.Module):\n","\n","    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n","                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n","                 downsample_padding=1, num_classes=1000):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        block (nn.Module): Bottleneck class.\n","            - For SENet154: SEBottleneck\n","            - For SE-ResNet models: SEResNetBottleneck\n","            - For SE-ResNeXt models:  SEResNeXtBottleneck\n","        layers (list of ints): Number of residual blocks for 4 layers of the\n","            network (layer1...layer4).\n","        groups (int): Number of groups for the 3x3 convolution in each\n","            bottleneck block.\n","            - For SENet154: 64\n","            - For SE-ResNet models: 1\n","            - For SE-ResNeXt models:  32\n","        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n","            - For all models: 16\n","        dropout_p (float or None): Drop probability for the Dropout layer.\n","            If `None` the Dropout layer is not used.\n","            - For SENet154: 0.2\n","            - For SE-ResNet models: None\n","            - For SE-ResNeXt models: None\n","        inplanes (int):  Number of input channels for layer1.\n","            - For SENet154: 128\n","            - For SE-ResNet models: 64\n","            - For SE-ResNeXt models: 64\n","        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n","            a single 7x7 convolution in layer0.\n","            - For SENet154: True\n","            - For SE-ResNet models: False\n","            - For SE-ResNeXt models: False\n","        downsample_kernel_size (int): Kernel size for downsampling convolutions\n","            in layer2, layer3 and layer4.\n","            - For SENet154: 3\n","            - For SE-ResNet models: 1\n","            - For SE-ResNeXt models: 1\n","        downsample_padding (int): Padding for downsampling convolutions in\n","            layer2, layer3 and layer4.\n","            - For SENet154: 1\n","            - For SE-ResNet models: 0\n","            - For SE-ResNeXt models: 0\n","        num_classes (int): Number of outputs in `last_linear` layer.\n","            - For all models: 1000\n","        \"\"\"\n","        super(SENet, self).__init__()\n","        self.inplanes = inplanes\n","        if input_3x3:\n","            layer0_modules = [\n","                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n","                                    bias=False)),\n","                ('bn1', nn.BatchNorm2d(64)),\n","                ('relu1', nn.ReLU(inplace=True)),\n","                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n","                                    bias=False)),\n","                ('bn2', nn.BatchNorm2d(64)),\n","                ('relu2', nn.ReLU(inplace=True)),\n","                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n","                                    bias=False)),\n","                ('bn3', nn.BatchNorm2d(inplanes)),\n","                ('relu3', nn.ReLU(inplace=True)),\n","            ]\n","        else:\n","            layer0_modules = [\n","                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n","                                    padding=3, bias=False)),\n","                ('bn1', nn.BatchNorm2d(inplanes)),\n","                ('relu1', nn.ReLU(inplace=True)),\n","            ]\n","        # To preserve compatibility with Caffe weights `ceil_mode=True`\n","        # is used instead of `padding=1`.\n","        self.pool = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n","        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n","        self.layer1 = self._make_layer(\n","            block,\n","            planes=64,\n","            blocks=layers[0],\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=1,\n","            downsample_padding=0\n","        )\n","        self.layer2 = self._make_layer(\n","            block,\n","            planes=128,\n","            blocks=layers[1],\n","            stride=2,\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=downsample_kernel_size,\n","            downsample_padding=downsample_padding\n","        )\n","        self.layer3 = self._make_layer(\n","            block,\n","            planes=256,\n","            blocks=layers[2],\n","            stride=2,\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=downsample_kernel_size,\n","            downsample_padding=downsample_padding\n","        )\n","        self.layer4 = self._make_layer(\n","            block,\n","            planes=512,\n","            blocks=layers[3],\n","            stride=2,\n","            groups=groups,\n","            reduction=reduction,\n","            downsample_kernel_size=downsample_kernel_size,\n","            downsample_padding=downsample_padding\n","        )\n","        self.avg_pool = nn.AvgPool2d(7, stride=1)\n","        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n","        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n","        self._initialize_weights()\n","\n","    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n","                    downsample_kernel_size=1, downsample_padding=0):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=downsample_kernel_size, stride=stride,\n","                          padding=downsample_padding, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n","                            downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, groups, reduction))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","                m.weight.data = nn.init.kaiming_normal_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def features(self, x):\n","        x = self.layer0(x)\n","        x = self.pool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        return x\n","\n","    def logits(self, x):\n","        x = self.avg_pool(x)\n","        if self.dropout is not None:\n","            x = self.dropout(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.last_linear(x)\n","        return x\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.logits(x)\n","        return x\n","\n","\n","def initialize_pretrained_model(model, num_classes, settings):\n","    assert num_classes == settings['num_classes'], \\\n","        'num_classes should be {}, but is {}'.format(\n","            settings['num_classes'], num_classes)\n","    model.load_state_dict(model_zoo.load_url(settings['url']), strict=False)\n","    model.input_space = settings['input_space']\n","    model.input_size = settings['input_size']\n","    model.input_range = settings['input_range']\n","    model.mean = settings['mean']\n","    model.std = settings['std']\n","\n","\n","def senet154(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n","                  dropout_p=0.2, num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['senet154'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","def scsenet154(num_classes=1000, pretrained='imagenet'):\n","    print(\"scsenet154\")\n","    model = SENet(SCSEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n","                  dropout_p=0.2, num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['senet154'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnet50(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnet50'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnet101(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnet101'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnet152(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnet152'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def scse_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SCSEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model\n","\n","\n","def se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n","    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n","                  dropout_p=None, inplanes=64, input_3x3=False,\n","                  downsample_kernel_size=1, downsample_padding=0,\n","                  num_classes=num_classes)\n","    if pretrained is not None:\n","        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n","        initialize_pretrained_model(model, num_classes, settings)\n","    return model"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"YEmiREpPoCWI","executionInfo":{"status":"ok","timestamp":1698149477687,"user_tz":180,"elapsed":6,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["class SeResNext50_Unet_2Ssum(nn.Module):\n","    def __init__(self, pretrained='imagenet', **kwargs):\n","        super(SeResNext50_Unet_2Ssum, self).__init__()\n","\n","        encoder_filters = [64, 256, 512, 1024, 2048]\n","        decoder_filters = np.asarray([64, 96, 128, 256, 512]) // 2\n","\n","        self.conv6 = ConvRelu(encoder_filters[-1], decoder_filters[-1])\n","        self.conv6_2 = ConvRelu(decoder_filters[-1] + encoder_filters[-2], decoder_filters[-1])\n","        self.conv7 = ConvRelu(decoder_filters[-1], decoder_filters[-2])\n","        self.conv7_2 = ConvRelu(decoder_filters[-2] + encoder_filters[-3] , decoder_filters[-2] )\n","        self.conv8 = ConvRelu(decoder_filters[-2], decoder_filters[-3])\n","        self.conv8_2 = ConvRelu(decoder_filters[-3] + encoder_filters[-4] , decoder_filters[-3])\n","        self.conv9 = ConvRelu(decoder_filters[-3], decoder_filters[-4])\n","        self.conv9_2 = ConvRelu(decoder_filters[-4] + encoder_filters[-5], decoder_filters[-4])\n","\n","        # self.convx9_3 = ConvRelu(encoder_filters[-4], encoder_filters[-4])\n","\n","\n","        self.conv10 = ConvRelu(decoder_filters[-4], decoder_filters[-5])\n","        self.conv10_s = nn.Sequential(ConvRelu(decoder_filters[-5], decoder_filters[-5]),\n","                                      nn.Conv2d(decoder_filters[-5] , 1, 1, stride=1, padding=0),\n","                                      nn.Sigmoid())\n","        # self.convxx = nn.Sequential(ConvRelu(decoder_filters[-5]*2, decoder_filters[-5]*2),\n","        #                             nn.Conv2d(decoder_filters[-5] * 2, decoder_filters[-5], 1, stride=1, padding=0))\n","\n","        self.res = nn.Conv2d(decoder_filters[-5] * 2, 5, 1, stride=1, padding=0)\n","\n","        self._initialize_weights()\n","\n","        encoder = se_resnext50_32x4d(pretrained=pretrained)\n","        #encoder = torchvision.models.resnet50(pretrained=pretrained)\n","        self.conv1 = nn.Sequential(encoder.layer0.conv1, encoder.layer0.bn1, encoder.layer0.relu1) #encoder.layer0.conv1\n","        self.conv2 = nn.Sequential(encoder.pool, encoder.layer1)\n","        self.conv3 = encoder.layer2\n","        self.conv4 = encoder.layer3\n","        self.conv5 = encoder.layer4\n","\n","\n","    def forward1(self, x):\n","        batch_size, C, H, W = x.shape\n","        xx = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n","\n","        enc1 = self.conv1(x)\n","        enc2 = self.conv2(enc1)\n","        enc3 = self.conv3(enc2)\n","        enc4 = self.conv4(enc3)\n","        enc5 = self.conv5(enc4)\n","\n","        encx1 = self.conv1(xx)   # 64 128 128\n","        encx2 = self.conv2(encx1) # 64\n","        encx3 = self.conv3(encx2) # 32\n","        encx4 = self.conv4(encx3) # 16\n","        encx5 = self.conv5(encx4) # 8\n","\n","        dec6 = self.conv6(F.interpolate(enc5, scale_factor=2))\n","        dec6 = self.conv6_2(torch.cat([dec6, enc4 ], 1))\n","        dec7 = self.conv7(F.interpolate(dec6, scale_factor=2))\n","        dec7 = self.conv7_2(torch.cat([dec7, enc3], 1))\n","        dec8 = self.conv8(F.interpolate(dec7, scale_factor=2))\n","        dec8 = self.conv8_2(torch.cat([dec8, enc2], 1))\n","        dec9 = self.conv9(F.interpolate(dec8, scale_factor=2))\n","        dec9 = self.conv9_2(torch.cat([dec9,  enc1], 1))   #256\n","\n","        decx6 = self.conv6(F.interpolate(encx5, scale_factor=2))\n","        decx6 = self.conv6_2(torch.cat([decx6, encx4 ], 1))\n","        decx7 = self.conv7(F.interpolate(decx6, scale_factor=2))\n","        decx7 = self.conv7_2(torch.cat([decx7, encx3], 1))\n","        decx8 = self.conv8(F.interpolate(decx7, scale_factor=2))\n","        decx8 = self.conv8_2(torch.cat([decx8, encx2], 1))\n","        decx9 = self.conv9(F.interpolate(decx8, scale_factor=2))\n","        decx9 = self.conv9_2(torch.cat([decx9,  encx1], 1))   #128\n","        #decx9 = self.convx9_3(F.interpolate(decx9, scale_factor=4))\n","\n","\n","        dec10 = self.conv10(F.interpolate(dec9, scale_factor=2))\n","        alpha = self.conv10_s(dec10)\n","        decx10 = self.conv10(F.interpolate(decx9, scale_factor=4))\n","\n","        dec = alpha * dec10 + (1-alpha)*decx10\n","\n","        return dec\n","\n","    def forward(self, x):\n","        dec10_0 = self.forward1(x[:, :3, :, :])\n","        dec10_1 = self.forward1(x[:, 3:, :, :])\n","\n","        dec10 = torch.cat([dec10_0, dec10_1], 1)\n","\n","        return self.res(dec10)\n","\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n","                m.weight.data = nn.init.kaiming_normal_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"6LDy5nU9FYrM","executionInfo":{"status":"ok","timestamp":1698149477687,"user_tz":180,"elapsed":6,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","def create_square_kernel(size):\n","    return torch.ones((1, 1, size, size))\n","\n","def create_circle_kernel(size):\n","    y, x = np.ogrid[-(size-1)//2:(size-1)//2+1, -(size-1)//2:(size-1)//2+1]\n","    mask = x*x + y*y <= ((size-1)//2)**2\n","    return torch.Tensor(mask.astype(float)).view(1, 1, size, size)\n","\n","class MorphologicalLayer(nn.Module):\n","    def __init__(self, in_channels, kernel_size=3, shape='square', morph_op='erosion'):\n","        super(MorphologicalLayer, self).__init__()\n","        self.kernel_size = kernel_size\n","        self.padding = kernel_size // 2\n","        self.morph_op = morph_op\n","\n","        if shape == 'square':\n","            self.kernel = create_square_kernel(kernel_size)\n","        elif shape == 'circle':\n","            self.kernel = create_circle_kernel(kernel_size)\n","        else:\n","            raise ValueError(\"Invalid shape\")\n","\n","        print(\"In channels:\", in_channels)\n","        self.kernel = self.kernel.repeat(in_channels, 1, 1, 1)\n","        print(\"Kernel size after repeat:\", self.kernel.size())\n","\n","\n","        self.kernel = nn.Parameter(self.kernel, requires_grad=True)\n","\n","    def forward(self, x):\n","        if self.morph_op == 'erosion':\n","            return F.conv2d(-x, self.kernel, padding=self.padding, groups=x.size(1)) * -1\n","        elif self.morph_op == 'dilation':\n","            return F.conv2d(x, self.kernel, padding=self.padding, groups=x.size(1))\n","        elif self.morph_op == 'opening':\n","            return F.conv2d(x, self.kernel, padding=self.padding, groups=x.size(1)) * -1\n","        elif self.morph_op == 'closing':\n","            return F.conv2d(-x, self.kernel, padding=self.padding, groups=x.size(1))\n","        else:\n","            raise ValueError(\"Invalid morphological operation\")"]},{"cell_type":"code","source":["class SeResNext50_Unet_Double(nn.Module):\n","    def __init__(self, pretrained='imagenet', **kwargs):\n","        super(SeResNext50_Unet_Double, self).__init__()\n","\n","        encoder_filters = [64, 256, 512, 1024, 2048]\n","        decoder_filters = np.asarray([64, 96, 128, 256, 512]) // 2\n","\n","        self.conv6 = ConvRelu(encoder_filters[-1], decoder_filters[-1])\n","        self.conv6_2 = ConvRelu(decoder_filters[-1] + encoder_filters[-2], decoder_filters[-1])\n","        self.conv7 = ConvRelu(decoder_filters[-1], decoder_filters[-2])\n","        self.conv7_2 = ConvRelu(decoder_filters[-2] + encoder_filters[-3], decoder_filters[-2])\n","        self.conv8 = ConvRelu(decoder_filters[-2], decoder_filters[-3])\n","        self.conv8_2 = ConvRelu(decoder_filters[-3] + encoder_filters[-4], decoder_filters[-3])\n","        self.conv9 = ConvRelu(decoder_filters[-3], decoder_filters[-4])\n","        self.conv9_2 = ConvRelu(decoder_filters[-4] + encoder_filters[-5], decoder_filters[-4])\n","        self.conv10 = ConvRelu(decoder_filters[-4], decoder_filters[-5])\n","\n","\n","        self.res = nn.Conv2d(decoder_filters[-5] * 2, 5, 1, stride=1, padding=0)\n","\n","        self._initialize_weights()\n","\n","        encoder = se_resnext50_32x4d(pretrained=pretrained)\n","        #encoder = torchvision.models.resnet50(pretrained=pretrained)\n","\n","        # conv1_new = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        # _w = encoder.layer0.conv1.state_dict()\n","        # _w['weight'] = torch.cat([0.5 * _w['weight'], 0.5 * _w['weight']], 1)\n","        # conv1_new.load_state_dict(_w)\n","        self.conv1 = nn.Sequential(encoder.layer0.conv1, encoder.layer0.bn1, encoder.layer0.relu1) #encoder.layer0.conv1\n","        self.conv2 = nn.Sequential(encoder.pool, encoder.layer1)\n","        self.conv3 = encoder.layer2\n","        self.conv4 = encoder.layer3\n","        self.conv5 = encoder.layer4\n","\n","\n","    def forward1(self, x):\n","        batch_size, C, H, W = x.shape\n","\n","        enc1 = self.conv1(x)\n","        enc2 = self.conv2(enc1)\n","        enc3 = self.conv3(enc2)\n","        enc4 = self.conv4(enc3)\n","        enc5 = self.conv5(enc4)\n","\n","        dec6 = self.conv6(F.interpolate(enc5, scale_factor=2))\n","        dec6 = self.conv6_2(torch.cat([dec6, enc4 ], 1))\n","\n","        dec7 = self.conv7(F.interpolate(dec6, scale_factor=2))\n","        dec7 = self.conv7_2(torch.cat([dec7, enc3 ], 1))\n","\n","        dec8 = self.conv8(F.interpolate(dec7, scale_factor=2))\n","        dec8 = self.conv8_2(torch.cat([dec8, enc2 ], 1))\n","\n","        dec9 = self.conv9(F.interpolate(dec8, scale_factor=2))\n","        dec9 = self.conv9_2(torch.cat([dec9,  enc1  ], 1))\n","\n","        dec10 = self.conv10(F.interpolate(dec9, scale_factor=2))\n","\n","        return dec10\n","\n","\n","    def forward(self, x):\n","        dec10_0 = self.forward1(x[:, :3, :, :])\n","        dec10_1 = self.forward1(x[:, 3:, :, :])\n","\n","        dec10 = torch.cat([dec10_0, dec10_1], 1)\n","\n","        return self.res(dec10)\n","\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n","                m.weight.data = nn.init.kaiming_normal_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()"],"metadata":{"id":"anOG7SLeltag","executionInfo":{"status":"ok","timestamp":1698149477687,"user_tz":180,"elapsed":5,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"qVNa9uDwFOx8","executionInfo":{"status":"ok","timestamp":1698149477687,"user_tz":180,"elapsed":5,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["# Testar operações acontecendo ao mesmo tempo\n","# Variar tamanho das janelas\n","# Mudar elemento estruturante (quadrado ou circulo)\n","\n","# Testar randon search\n","# Fazer testes curtos (Com 200 ou 100 imagens)"]},{"cell_type":"markdown","metadata":{"id":"tHCAM2vfRK7e"},"source":["## LoadData"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114726,"status":"ok","timestamp":1698149753202,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"Un2B5Ov0F8f-","outputId":"904b290e-ff60-49a7-c5dc-3c1e939bbb4b"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [01:33<00:00, 46.73s/it]\n","100%|██████████| 1/1 [00:22<00:00, 22.10s/it]\n"]}],"source":["import os\n","os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n","os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n","os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n","\n","from os import path, makedirs, listdir\n","import sys\n","import numpy as np\n","np.random.seed(1)\n","import random\n","random.seed(1)\n","\n","import torch\n","from torch import nn\n","from torch.backends import cudnn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.optim.lr_scheduler as lr_scheduler\n","\n","#from apex import amp\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import timeit\n","import cv2\n","\n","from imgaug import augmenters as iaa\n","\n","from skimage.morphology import square, dilation\n","\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import accuracy_score\n","\n","import gc\n","\n","cv2.setNumThreads(0)\n","cv2.ocl.setUseOpenCL(False)\n","\n","train_dirs = ['/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/tier3','/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/train']\n","\n","val_dirs = ['/content/drive/MyDrive/Modeling Satelities Images Building Damaged/data/test']\n","\n","models_folder = '/content/drive/MyDrive/Modeling Satelities Images Building Damaged/src/train/weights'\n","\n","loc_folder = 'pred_loc_val'\n","\n","input_shape = (512, 512)\n","\n","\n","all_files = []\n","for d in tqdm(train_dirs):\n","    for f in sorted(listdir(path.join(d, 'images'))):\n","        if '_pre_disaster.png' in f:\n","            post_disaster_file = f.replace('_pre_disaster.png', '_post_disaster.png')\n","            if path.exists(path.join(d, 'images', post_disaster_file)):\n","                all_files.append(path.join(d, 'images', f))\n","\n","all_files2 = []\n","for d in tqdm(val_dirs):\n","    for f in sorted(listdir(path.join(d, 'images'))):\n","        if '_pre_disaster.png' in f:\n","            post_disaster_file = f.replace('_pre_disaster.png', '_post_disaster.png')\n","            if path.exists(path.join(d, 'images', post_disaster_file)):\n","                all_files2.append(path.join(d, 'images', f))"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"3_V8N6ykQ2g0","executionInfo":{"status":"ok","timestamp":1698149753202,"user_tz":180,"elapsed":12,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["def rand_bbox(size, lam):\n","    W = size[0]\n","    H = size[1]\n","    cut_rat = np.sqrt(1. - lam)\n","    cut_w = int(W * cut_rat)\n","    cut_h = int(H * cut_rat)\n","    # if cut_rat > 0.9:\n","    #   cut_w = np.int(W * cut_rat * 0.9)\n","    #   cut_h = np.int(H * cut_rat * 0.9)\n","\n","    # uniform\n","    cx = np.random.randint(W)\n","    cy = np.random.randint(H)\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n","\n","    return bbx1, bby1, bbx2, bby2\n","\n","class TrainData(Dataset):\n","    def __init__(self, train_idxs, low, high):\n","        super().__init__()\n","        self.train_idxs = train_idxs\n","        self.elastic = iaa.ElasticTransformation(alpha=(0.25, 1.2), sigma=0.2)\n","        self.low =low\n","        self.high = high\n","\n","    def __len__(self):\n","        return len(self.train_idxs)\n","\n","    def __getitem__(self, idx):\n","        _idx = self.train_idxs[idx]\n","\n","        fn = all_files[_idx]\n","\n","        img = cv2.imread(fn, cv2.IMREAD_COLOR)\n","        img2 = cv2.imread(fn.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","        if img is None and img2 is None:\n","          sample = {'img': None, 'msk': None, 'lbl_msk': None, 'fn': None}\n","        elif img is None or img2 is None:\n","          sample = {'img': None, 'msk': None, 'lbl_msk': None, 'fn': None}\n","\n","        msk0 = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        lbl_msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        msk1 = np.zeros_like(lbl_msk1)\n","        msk2 = np.zeros_like(lbl_msk1)\n","        msk3 = np.zeros_like(lbl_msk1)\n","        msk4 = np.zeros_like(lbl_msk1)\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","        msk1[lbl_msk1 == 1] = 255\n","\n","        try:\n","          if random.random() > 0.87:\n","              lam = np.random.beta(2, 1.8)\n","              rand_inx = torch.randint(low=self.low,high=self.high,size=(1,))\n","              ttt = self.train_idxs[rand_inx]\n","              fn_rand = all_files[ttt]\n","              img_random = cv2.imread(fn_rand, cv2.IMREAD_COLOR)\n","              img2_random = cv2.imread(fn_rand.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","              msk0_random = cv2.imread(fn_rand.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","              lbl_msk1_random = cv2.imread(fn_rand.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","              bbx1, bby1, bbx2, bby2 = rand_bbox((1024, 1024), lam)\n","              img[bbx1:bbx2, bby1:bby2, :] = img_random[bbx1:bbx2, bby1:bby2, :]\n","              img2[bbx1:bbx2, bby1:bby2, :] = img2_random[bbx1:bbx2, bby1:bby2, :]\n","              msk0[bbx1:bbx2, bby1:bby2] = msk0_random[bbx1:bbx2, bby1:bby2]\n","              lbl_msk1[bbx1:bbx2, bby1:bby2] = lbl_msk1_random[bbx1:bbx2, bby1:bby2]\n","              # lbl_msk1[lbl_msk1==1]=70\n","              # lbl_msk1[lbl_msk1==2]=130\n","              # lbl_msk1[lbl_msk1==3]=190\n","              # lbl_msk1[lbl_msk1==4]=255\n","              # cv2.imshow('input_image', lbl_msk1)\n","              # cv2.waitKey(5000)\n","        except:\n","          None\n","\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","        msk1[lbl_msk1 == 1] = 255\n","\n","        try:\n","          if random.random() > 0.5:\n","              img = img[::-1, ...]\n","              img2 = img2[::-1, ...]\n","              msk0 = msk0[::-1, ...]\n","              msk1 = msk1[::-1, ...]\n","              msk2 = msk2[::-1, ...]\n","              msk3 = msk3[::-1, ...]\n","              msk4 = msk4[::-1, ...]\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.05:\n","              rot = random.randrange(4)\n","              if rot > 0:\n","                img = np.rot90(img, k=rot)\n","                img2 = np.rot90(img2, k=rot)\n","                msk0 = np.rot90(msk0, k=rot)\n","                msk1 = np.rot90(msk1, k=rot)\n","                msk2 = np.rot90(msk2, k=rot)\n","                msk3 = np.rot90(msk3, k=rot)\n","                msk4 = np.rot90(msk4, k=rot)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.8:\n","              shift_pnt = (random.randint(-320, 320), random.randint(-320, 320))\n","              img = shift_image(img, shift_pnt)\n","              img2 = shift_image(img2, shift_pnt)\n","              msk0 = shift_image(msk0, shift_pnt)\n","              msk1 = shift_image(msk1, shift_pnt)\n","              msk2 = shift_image(msk2, shift_pnt)\n","              msk3 = shift_image(msk3, shift_pnt)\n","              msk4 = shift_image(msk4, shift_pnt)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.2:\n","              rot_pnt =  (img.shape[0] // 2 + random.randint(-320, 320), img.shape[1] // 2 + random.randint(-320, 320))\n","              scale = 0.9 + random.random() * 0.2\n","              angle = random.randint(0, 20) - 10\n","              if (angle != 0) or (scale != 1):\n","                  img = rotate_image(img, angle, scale, rot_pnt)\n","                  img2 = rotate_image(img2, angle, scale, rot_pnt)\n","                  msk0 = rotate_image(msk0, angle, scale, rot_pnt)\n","                  msk1 = rotate_image(msk1, angle, scale, rot_pnt)\n","                  msk2 = rotate_image(msk2, angle, scale, rot_pnt)\n","                  msk3 = rotate_image(msk3, angle, scale, rot_pnt)\n","                  msk4 = rotate_image(msk4, angle, scale, rot_pnt)\n","        except:\n","          None\n","\n","        crop_size = input_shape[0]\n","\n","        try:\n","          if random.random() > 0.1:\n","              crop_size = random.randint(int(input_shape[0] / 1.15), int(input_shape[0] / 0.85))\n","        except:\n","          None\n","\n","        try:\n","          bst_x0 = random.randint(0, img.shape[1] - crop_size)\n","          bst_y0 = random.randint(0, img.shape[0] - crop_size)\n","          bst_sc = -1\n","          try_cnt = random.randint(1, 10)\n","          for i in range(try_cnt):\n","              x0 = random.randint(0, img.shape[1] - crop_size)\n","              y0 = random.randint(0, img.shape[0] - crop_size)\n","              _sc = msk2[y0:y0+crop_size, x0:x0+crop_size].sum() * 5 + msk3[y0:y0+crop_size, x0:x0+crop_size].sum() * 5 + msk4[y0:y0+crop_size, x0:x0+crop_size].sum() * 2 + msk1[y0:y0+crop_size, x0:x0+crop_size].sum()\n","              if _sc > bst_sc:\n","                  bst_sc = _sc\n","                  bst_x0 = x0\n","                  bst_y0 = y0\n","          x0 = bst_x0\n","          y0 = bst_y0\n","          img = img[y0:y0+crop_size, x0:x0+crop_size, :]\n","          img2 = img2[y0:y0+crop_size, x0:x0+crop_size, :]\n","          msk0 = msk0[y0:y0+crop_size, x0:x0+crop_size]\n","          msk1 = msk1[y0:y0+crop_size, x0:x0+crop_size]\n","          msk2 = msk2[y0:y0+crop_size, x0:x0+crop_size]\n","          msk3 = msk3[y0:y0+crop_size, x0:x0+crop_size]\n","          msk4 = msk4[y0:y0+crop_size, x0:x0+crop_size]\n","\n","          if crop_size != input_shape[0]:\n","            img = cv2.resize(img, input_shape, interpolation=cv2.INTER_LINEAR)\n","            img2 = cv2.resize(img2, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk0 = cv2.resize(msk0, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk1 = cv2.resize(msk1, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk2 = cv2.resize(msk2, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk3 = cv2.resize(msk3, input_shape, interpolation=cv2.INTER_LINEAR)\n","            msk4 = cv2.resize(msk4, input_shape, interpolation=cv2.INTER_LINEAR)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.96:\n","              img = shift_channels(img, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","          elif random.random() > 0.96:\n","              img2 = shift_channels(img2, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","\n","          if random.random() > 0.96:\n","              img = change_hsv(img, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","          elif random.random() > 0.96:\n","              img2 = change_hsv(img2, random.randint(-5, 5), random.randint(-5, 5), random.randint(-5, 5))\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img = clahe(img)\n","              elif random.random() > 0.96:\n","                  img = gauss_noise(img)\n","              elif random.random() > 0.96:\n","                  img = cv2.blur(img, (3, 3))\n","          elif random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img = saturation(img, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img = brightness(img, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img = contrast(img, 0.9 + random.random() * 0.2)\n","\n","          if random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img2 = clahe(img2)\n","              elif random.random() > 0.96:\n","                  img2 = gauss_noise(img2)\n","              elif random.random() > 0.96:\n","                  img2 = cv2.blur(img2, (3, 3))\n","          elif random.random() > 0.9:\n","              if random.random() > 0.96:\n","                  img2 = saturation(img2, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img2 = brightness(img2, 0.9 + random.random() * 0.2)\n","              elif random.random() > 0.96:\n","                  img2 = contrast(img2, 0.9 + random.random() * 0.2)\n","        except:\n","          None\n","\n","        try:\n","          if random.random() > 0.96:\n","              el_det = self.elastic.to_deterministic()\n","              img = el_det.augment_image(img)\n","\n","          if random.random() > 0.96:\n","              el_det = self.elastic.to_deterministic()\n","              img2 = el_det.augment_image(img2)\n","        except:\n","          None\n","\n","        msk0 = msk0[..., np.newaxis]\n","        msk1 = msk1[..., np.newaxis]\n","        msk2 = msk2[..., np.newaxis]\n","        msk3 = msk3[..., np.newaxis]\n","        msk4 = msk4[..., np.newaxis]\n","\n","        msk = np.concatenate([msk0, msk1, msk2, msk3, msk4], axis=2)\n","        msk = (msk > 127)\n","\n","        msk[..., 0] = True\n","        msk[..., 1] = dilation(msk[..., 1], square(5))\n","        msk[..., 2] = dilation(msk[..., 2], square(5))\n","        msk[..., 3] = dilation(msk[..., 3], square(5))\n","        msk[..., 4] = dilation(msk[..., 4], square(5))\n","        msk[..., 1][msk[..., 2:].max(axis=2)] = False\n","        msk[..., 3][msk[..., 2]] = False\n","        msk[..., 4][msk[..., 2]] = False\n","        msk[..., 4][msk[..., 3]] = False\n","        msk[..., 0][msk[..., 1:].max(axis=2)] = False\n","        msk = msk * 1\n","\n","        lbl_msk = msk.argmax(axis=2)\n","\n","        img = np.concatenate([img, img2], axis=2)\n","        img = preprocess_inputs(img)\n","\n","        img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n","        msk = torch.from_numpy(msk.transpose((2, 0, 1))).long()\n","\n","        sample = {'img': img, 'msk': msk, 'lbl_msk': lbl_msk, 'fn': fn}\n","        return sample\n","\n","\n","class ValData(Dataset):\n","    def __init__(self, image_idxs):\n","        super().__init__()\n","        self.image_idxs = image_idxs\n","\n","    def __len__(self):\n","        return len(self.image_idxs)\n","\n","    def __getitem__(self, idx):\n","        _idx = self.image_idxs[idx]\n","\n","        fn = all_files2[_idx]\n","\n","        img = cv2.imread(fn, cv2.IMREAD_COLOR)\n","        img2 = cv2.imread(fn.replace('_pre_disaster', '_post_disaster'), cv2.IMREAD_COLOR)\n","        msk_loc = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)# > (0.3*255)\n","\n","        msk0 = cv2.imread(fn.replace('/images/', '/targets/').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        lbl_msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","        msk1 = np.zeros_like(lbl_msk1)\n","        msk2 = np.zeros_like(lbl_msk1)\n","        msk3 = np.zeros_like(lbl_msk1)\n","        msk4 = np.zeros_like(lbl_msk1)\n","        msk1[lbl_msk1 == 1] = 255\n","        msk2[lbl_msk1 == 2] = 255\n","        msk3[lbl_msk1 == 3] = 255\n","        msk4[lbl_msk1 == 4] = 255\n","\n","        msk0 = msk0[..., np.newaxis]\n","        msk1 = msk1[..., np.newaxis]\n","        msk2 = msk2[..., np.newaxis]\n","        msk3 = msk3[..., np.newaxis]\n","        msk4 = msk4[..., np.newaxis]\n","\n","        msk = np.concatenate([msk0, msk1, msk2, msk3, msk4], axis=2)\n","        msk = (msk > 127)\n","\n","        msk = msk * 1\n","\n","        lbl_msk = msk[..., 1:].argmax(axis=2)\n","\n","        img = np.concatenate([img, img2], axis=2)\n","        img = preprocess_inputs(img)\n","\n","        img = torch.from_numpy(img.transpose((2, 0, 1))).float()\n","        msk = torch.from_numpy(msk.transpose((2, 0, 1))).long()\n","\n","        sample = {'img': img, 'msk': msk, 'lbl_msk': lbl_msk, 'fn': fn, 'msk_loc': msk_loc}\n","        return sample\n","\n","\n","def validate(net, data_loader):\n","    dices0 = []\n","\n","    tp = np.zeros((5,))\n","    fp = np.zeros((5,))\n","    fn = np.zeros((5,))\n","\n","    _thr = 0.3\n","\n","    with torch.no_grad():\n","        for i, sample in enumerate(tqdm(data_loader)):\n","            msks = sample[\"msk\"].numpy()\n","            lbl_msk = sample[\"lbl_msk\"].numpy()\n","            imgs = sample[\"img\"].cuda(non_blocking=True)\n","            msk_loc = sample[\"msk_loc\"].numpy() * 1\n","            out = model(imgs)\n","\n","            msk_pred = msk_loc\n","            msk_damage_pred = torch.softmax(out, dim=1).cpu().numpy()[:, 1:, ...]\n","\n","            for j in range(msks.shape[0]):\n","                tp[4] += np.logical_and(msks[j, 0] > 0, msk_pred[j] > 0).sum()\n","                fn[4] += np.logical_and(msks[j, 0] < 1, msk_pred[j] > 0).sum()\n","                fp[4] += np.logical_and(msks[j, 0] > 0, msk_pred[j] < 1).sum()\n","\n","\n","                targ = lbl_msk[j][msks[j, 0] > 0]\n","                pred = msk_damage_pred[j].argmax(axis=0)\n","                pred = pred * (msk_pred[j] > _thr)\n","                pred = pred[msks[j, 0] > 0]\n","                for c in range(4):\n","                    tp[c] += np.logical_and(pred == c, targ == c).sum()\n","                    fn[c] += np.logical_and(pred != c, targ == c).sum()\n","                    fp[c] += np.logical_and(pred == c, targ != c).sum()\n","\n","    d0 = 2 * tp[4] / (2 * tp[4] + fp[4] + fn[4])\n","\n","    f1_sc = np.zeros((4,))\n","    for c in range(4):\n","        f1_sc[c] = 2 * tp[c] / (2 * tp[c] + fp[c] + fn[c])\n","\n","    f1 = 4 / np.sum(1.0 / (f1_sc + 1e-6))\n","\n","    sc = 0.3 * d0 + 0.7 * f1\n","    print(\"Val Score: {}, Dice: {}, F1: {}, F1_0: {}, F1_1: {}, F1_2: {}, F1_3: {}\".format(sc, d0, f1, f1_sc[0], f1_sc[1], f1_sc[2], f1_sc[3]))\n","    return [sc, d0, f1, f1_sc[0], f1_sc[1], f1_sc[2], f1_sc[3]]\n","\n","\n","def evaluate_val(data_val, best_score, model, snapshot_name, current_epoch):\n","    model = model.eval()\n","    d = validate(model, data_loader=data_val)\n","    score = d[0]\n","\n","    if score > best_score:\n","        torch.save({\n","            'epoch': current_epoch + 1,\n","            'state_dict': model.state_dict(),\n","            'best_score': d,\n","        }, path.join(models_folder, snapshot_name + '_best'))\n","        best_score = score\n","\n","    print(\"score: {}\\tscore_best: {}\".format(d, best_score))\n","    return d[0],d\n","\n","\n","def train_epoch(current_epoch, seg_loss, ce_loss, seg_seesaw, model, optimizer, scheduler, train_data_loader):\n","    losses = AverageMeter()\n","    losses1 = AverageMeter()\n","\n","    dices = AverageMeter()\n","\n","    iterator = tqdm(train_data_loader)\n","    model.train()\n","    for i, sample in enumerate(iterator):\n","        imgs = sample[\"img\"].cuda(non_blocking=True)\n","        msks = sample[\"msk\"].cuda(non_blocking=True)\n","        lbl_msk = sample[\"lbl_msk\"].cuda(non_blocking=True)\n","\n","        out = model(imgs)\n","\n","        # loss0 = seg_loss(out[:, 0, ...], msks[:, 0, ...])\n","        # loss1 = seg_loss(out[:, 1, ...], msks[:, 1, ...])\n","        # loss2 = seg_loss(out[:, 2, ...], msks[:, 2, ...])\n","        # loss3 = seg_loss(out[:, 3, ...], msks[:, 3, ...])\n","        # loss4 = seg_loss(out[:, 4, ...], msks[:, 4, ...])\n","\n","        loss5 = ce_loss(out, lbl_msk)\n","        #loss5 = seg_seesaw(out, lbl_msk)\n","        loss = loss5\n","        #loss = 0.1 * loss0 + 0.1 * loss1 + 0.3 * loss2 + 0.3 * loss3 + 0.2 * loss4 + loss5 * 2\n","\n","        with torch.no_grad():\n","            _probs = 1 - torch.sigmoid(out[:, 0, ...])\n","            dice_sc = 1 - dice_round(_probs, 1 - msks[:, 0, ...])\n","\n","        losses.update(loss.item(), imgs.size(0))\n","        losses1.update(loss5.item(), imgs.size(0))\n","\n","        dices.update(dice_sc, imgs.size(0))\n","\n","        iterator.set_description(\n","            \"epoch: {}; lr {:.7f}; Loss {loss.val:.4f} ({loss.avg:.4f}); cce_loss {loss1.val:.4f} ({loss1.avg:.4f}); Dice {dice.val:.4f} ({dice.avg:.4f})\".format(\n","                current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","\n","        optimizer.zero_grad()\n","        # loss.backward()\n","        with amp.scale_loss(loss, optimizer) as scaled_loss:\n","            scaled_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), 0.999)\n","        optimizer.step()\n","\n","    scheduler.step()\n","\n","    print(\"epoch: {}; lr {:.7f}; Loss {loss.avg:.4f}; CCE_loss {loss1.avg:.4f}; Dice {dice.avg:.4f}\".format(\n","            current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","    return scheduler.get_lr()[-1], losses, losses1, dices\n","\n","def train_epoch(current_epoch, seg_loss, ce_loss, seg_seesaw, model, optimizer, scheduler, train_data_loader):\n","    losses = AverageMeter()\n","    losses1 = AverageMeter()\n","\n","    dices = AverageMeter()\n","\n","    iterator = tqdm(train_data_loader)\n","    model.train()\n","    for i, sample in enumerate(iterator):\n","        if sample[\"img\"] is None or sample[\"msk\"] is None or sample[\"lbl_msk\"] is None:\n","              continue\n","        imgs = sample[\"img\"].cuda(non_blocking=True)\n","        msks = sample[\"msk\"].cuda(non_blocking=True)\n","        lbl_msk = sample[\"lbl_msk\"].cuda(non_blocking=True)\n","\n","        out = model(imgs)\n","\n","        loss5 = ce_loss(out, lbl_msk)\n","        loss = loss5\n","\n","        with torch.no_grad():\n","            _probs = 1 - torch.sigmoid(out[:, 0, ...])\n","            dice_sc = 1 - dice_round(_probs, 1 - msks[:, 0, ...])\n","\n","        losses.update(loss.item(), imgs.size(0))\n","        losses1.update(loss5.item(), imgs.size(0))\n","\n","        dices.update(dice_sc, imgs.size(0))\n","\n","        iterator.set_description(\n","            \"epoch: {}; lr {:.7f}; Loss {loss.val:.4f} ({loss.avg:.4f}); cce_loss {loss1.val:.4f} ({loss1.avg:.4f}); Dice {dice.val:.4f} ({dice.avg:.4f})\".format(\n","                current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.999)\n","        optimizer.step()\n","\n","    scheduler.step()\n","\n","    print(\"epoch: {}; lr {:.7f}; Loss {loss.avg:.4f}; CCE_loss {loss1.avg:.4f}; Dice {dice.avg:.4f}\".format(\n","            current_epoch, scheduler.get_lr()[-1], loss=losses, loss1=losses1, dice=dices))\n","    return scheduler.get_lr()[-1], losses, losses1, dices"]},{"cell_type":"code","source":[],"metadata":{"id":"MVzwPdb2NMel","executionInfo":{"status":"ok","timestamp":1698149753202,"user_tz":180,"elapsed":2,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8053164,"status":"ok","timestamp":1698157806364,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"AL3hRfYVR6t7","outputId":"415c095c-c910-4987-e36a-834229911321"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9162/9162 [1:58:07<00:00,  1.29it/s]\n","100%|██████████| 906/906 [16:04<00:00,  1.06s/it]\n","100%|██████████| 906/906 [00:00<00:00, 1749557.75it/s]\n","100%|██████████| 9070/9070 [00:00<00:00, 422579.95it/s]\n","100%|██████████| 9070/9070 [00:00<00:00, 432210.88it/s]\n"]}],"source":["# ttt = np.asarray([True, False, True, True])\n","# p1=ttt[0:].max()\n","# p2=ttt[1].max()\n","# ttt2 = np.asarray([True, True, False, False])\n","# p3=ttt2[1].max()\n","# p4=ttt2[2:3].max()\n","t0 = timeit.default_timer()\n","\n","makedirs(models_folder, exist_ok=True)\n","\n","seed = 13\n","#seed=0\n","# vis_dev = sys.argv[2]\n","\n","# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = vis_dev\n","\n","cudnn.benchmark = True\n","\n","batch_size = 12\n","val_batch_size = 10\n","\n","snapshot_name = 'res50_cls_2Ssum_{}_0'.format(seed)\n","\n","file_classes = []\n","for fn in tqdm(all_files):\n","    fl = np.zeros((4,), dtype=bool)\n","    msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","    for c in range(1, 5):\n","        fl[c-1] = c in msk1\n","    file_classes.append(fl)\n","file_classes = np.asarray(file_classes)\n","\n","file_classes2 = []\n","for fn in tqdm(all_files2):\n","    fl = np.zeros((4,), dtype=bool)\n","    msk1 = cv2.imread(fn.replace('/images/', '/targets/').replace('_pre_disaster', '_post_disaster').replace('.png', '_target.png'), cv2.IMREAD_UNCHANGED)\n","    for c in range(1, 5):\n","        fl[c-1] = c in msk1\n","    file_classes2.append(fl)\n","file_classes2 = np.asarray(file_classes2)\n","\n","train_idxs0, val_idxs = train_test_split(np.arange(len(all_files)), test_size=0.01, random_state=seed)\n","\n","val_idxs0 = np.arange(len(all_files2))\n","\n","val_idxs = []\n","for i in tqdm(val_idxs0):\n","    val_idxs.append(i)\n","\n","np.random.seed(seed + 1234)\n","random.seed(seed + 1234)\n","\n","train_idxs = []\n","for i in tqdm(train_idxs0):\n","    train_idxs.append(i)\n","    if file_classes[i, 1:].max():\n","        train_idxs.append(i)\n","    # if file_classes[i, 2].max():\n","    #     train_idxs.append(i)\n","low1 = len(train_idxs)\n","for i in tqdm(train_idxs0):\n","    if file_classes[i, 1:3].max():\n","        train_idxs.append(i)\n","# for i in train_idxs0:\n","#     if file_classes[i, 1].max():\n","#         train_idxs.append(i)\n","high1 = len(train_idxs)\n","\n","train_idxs = np.asarray(train_idxs)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1698157806364,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"ZxguzzcmqDBX","outputId":"a3755979-12f0-453d-c955-8a1f92d82f71"},"outputs":[{"output_type":"stream","name":"stdout","text":["steps_per_epoch 1227 validation_steps 90\n"]}],"source":["steps_per_epoch = int(len(train_idxs) // batch_size)\n","validation_steps = int(len(val_idxs) // val_batch_size)\n","\n","print('steps_per_epoch', steps_per_epoch, 'validation_steps', validation_steps)\n","\n","data_train = TrainData(train_idxs, low1, high1)\n","val_train = ValData(val_idxs)\n","\n","train_data_loader = DataLoader(data_train, batch_size=int(batch_size), num_workers=6, shuffle=True, pin_memory=False, drop_last=True)\n","val_data_loader = DataLoader(val_train, batch_size=int(val_batch_size), num_workers=6, shuffle=False, pin_memory=False)"]},{"cell_type":"markdown","metadata":{"id":"T-rqSugmZwEp"},"source":["## Train"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"UIavFyXpaPYX","executionInfo":{"status":"ok","timestamp":1698157808214,"user_tz":180,"elapsed":8,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["from itertools import product\n","import random\n","\n","# Camadas candidatas para adicionar operações morfológicas.\n","candidate_layers = ['after_conv3', 'after_conv4', 'after_conv5']\n","\n","# Tipos de operações morfológicas para experimentar.\n","morph_ops = ['erosion', 'dilation', 'opening', 'closing']\n","\n","# Tamanhos de kernel para experimentar.\n","kernel_sizes = [3, 5, 7]\n","\n","# Tipos de elementos estruturantes.\n","shapes = ['square', 'circle']\n","\n","# Número de configurações aleatórias para testar.\n","num_random_configs = 72"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"rhk9LcAhi_s4","executionInfo":{"status":"ok","timestamp":1698157808214,"user_tz":180,"elapsed":7,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["# Gere um subconjunto aleatório de todas as combinações possíveis.\n","random_combinations = []\n","for _ in range(num_random_configs):\n","    num_layers = random.randint(1, len(candidate_layers))  # Número de camadas morfológicas para adicionar\n","    layers = random.sample(candidate_layers, num_layers)  # Escolha aleatória das camadas\n","    ops = random.choices(morph_ops, k=num_layers)  # Escolha aleatória das operações\n","    k_sizes = random.choices(kernel_sizes, k=num_layers)  # Escolha aleatória dos tamanhos de kernel\n","    shape_types = random.choices(shapes, k=num_layers)  # Escolha aleatória dos tipos de elementos estruturantes\n","    random_combinations.append((layers, ops, k_sizes, shape_types))\n","\n","def get_morph_layer(morph_op, kernel_size, shape, in_channels):\n","    return MorphologicalLayer(in_channels=in_channels, kernel_size=kernel_size, shape=shape, morph_op=morph_op)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1698157808215,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"RY6l-CxCLijd","outputId":"489fc693-83ef-4721-bd45-2809cf1c8c1d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['after_conv3'], ['closing'], [7], ['circle'])"]},"metadata":{},"execution_count":24}],"source":["random_combinations[0]"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"VRs0hOngMBe-","executionInfo":{"status":"ok","timestamp":1698157808215,"user_tz":180,"elapsed":6,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["random_combinations = [\n","(['after_conv2'],\n"," ['opening'],\n"," [5],\n"," ['square']),\n","]"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1698157808215,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"8UkZiy8vMwjE","outputId":"818adeb4-5783-4aeb-fee3-86dc9a5c18e6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(['after_conv2'], ['opening'], [5], ['square'])]"]},"metadata":{},"execution_count":26}],"source":["random_combinations"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"ymFmmlD7cgBF","executionInfo":{"status":"ok","timestamp":1698157808216,"user_tz":180,"elapsed":5,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["def get_out_channels_from_conv(model, conv_name):\n","    conv_block = getattr(model, conv_name, None)\n","\n","    if conv_block is None:\n","        print(f\"{conv_name} not found in the model.\")\n","        return None\n","\n","    if not isinstance(conv_block, nn.Sequential):\n","        print(f\"{conv_name} is not an instance of nn.Sequential.\")\n","        return None\n","\n","    for layer in conv_block:\n","        if isinstance(layer, nn.Module) and hasattr(layer, 'conv3'):\n","            return layer.conv3.out_channels\n","\n","    return None\n","\n","def get_out_channels_from_conv2(model, conv_name):\n","    for name, module in model.named_modules():\n","        if conv_name in name and hasattr(module, 'out_channels'):\n","            return module.out_channels\n","    print(f\"{conv_name} not found in the model or it does not have 'out_channels' attribute.\")\n","    return None\n","\n","def get_out_channels_from_conv2(model, conv_name):\n","    for name, module in model.named_modules():\n","        print(f\"Checking module: {name}\")  # Imprime o nome do módulo que está sendo verificado\n","        if conv_name in name and hasattr(module, 'out_channels'):\n","            print(f\"Found module: {name}, out_channels: {module.out_channels}\")  # Imprime os canais de saída quando encontrar o módulo\n","\n","            # Adiciona uma condição para verificar se está acessando a camada correta\n","            if 'conv3' in name:\n","                return module.out_channels\n","\n","    print(f\"{conv_name} not found in the model or it does not have 'out_channels' attribute.\")\n","    return None"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1698157808216,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"},"user_tz":180},"id":"lz2cyVFPjKyq","outputId":"499b802e-fdb5-4a9d-fb92-f02a80e56b35"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['after_conv2'], ['opening'], [5], ['square'])"]},"metadata":{},"execution_count":28}],"source":["random_combinations[0]"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"Z313YLyMn2XX","executionInfo":{"status":"ok","timestamp":1698157810296,"user_tz":180,"elapsed":2084,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[],"source":["import pandas as pd\n","results = pd.DataFrame(None,columns=['epoch', 'lr', 'loss',\n","                                     'CCE_loss', 'dice_train',\n","                                     'layers', 'ops', 'k_sizes',\n","                                     'shape_types','val_score','dice_val',\n","                                     'f1_score','F1_0','F1_1','F1_2',\n","                                     'F1_3'])\n","results.to_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_3_top_train4.csv', index=False)"]},{"cell_type":"code","source":["# Importando as bibliotecas necessárias\n","import torch\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","# Supondo que `model` é a instância do seu modelo\n","model = SeResNext50_Unet_Double()\n","\n","# Carregar os pesos\n","weights_path = '/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/download_weights/weights/res50_cls_cce_0_tuned_best'\n","weights = torch.load(weights_path, map_location='cuda')\n","print(weights['epoch'])\n","print(weights['best_score'])\n","\n","new_state_dict = {key[7:]: value for key, value in weights['state_dict'].items()}\n","\n","model.load_state_dict(new_state_dict)\n","\n","model = model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z-sIvwEBlN0W","executionInfo":{"status":"ok","timestamp":1698172489970,"user_tz":180,"elapsed":1610,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"597a6647-6a22-4d84-d580-43e83758af41"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n","0.7979383764620159\n"]}]},{"cell_type":"code","source":["# Adicionar a camada morfológica após a camada `conv10`\n","random_combinations = [\n","    (['opening'], [5], ['square']),\n","]\n","\n","morph_ops, k_sizes, shapes = random_combinations[0]\n","morph_op, k_size, shape = morph_ops[0], k_sizes[0], shapes[0]\n","\n","# Como o decoder_filters[-5] é a saída da camada conv10\n","decoder_filters = [32, 48, 64, 128, 256]  # Copiado do seu código anterior\n","in_channels = decoder_filters[-5]\n","morph_layer = get_morph_layer(morph_op, k_size, shape, in_channels)\n","\n","# Combinando a camada `conv10` e a nova camada morfológica\n","model.conv10 = nn.Sequential(model.conv10, morph_layer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3w_qVGVpnUqF","executionInfo":{"status":"ok","timestamp":1698172490665,"user_tz":180,"elapsed":2,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"0a036f53-5c84-4351-b2dc-e49778132806"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["In channels: 32\n","Kernel size after repeat: torch.Size([32, 1, 5, 5])\n"]}]},{"cell_type":"code","source":["model.state_dict().keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qOcSu6j0nVq8","executionInfo":{"status":"ok","timestamp":1698172492583,"user_tz":180,"elapsed":5,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"3d77f926-7b90-41cd-f594-060cc04e0485"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['conv6.layer.0.weight', 'conv6.layer.0.bias', 'conv6_2.layer.0.weight', 'conv6_2.layer.0.bias', 'conv7.layer.0.weight', 'conv7.layer.0.bias', 'conv7_2.layer.0.weight', 'conv7_2.layer.0.bias', 'conv8.layer.0.weight', 'conv8.layer.0.bias', 'conv8_2.layer.0.weight', 'conv8_2.layer.0.bias', 'conv9.layer.0.weight', 'conv9.layer.0.bias', 'conv9_2.layer.0.weight', 'conv9_2.layer.0.bias', 'conv10.0.layer.0.weight', 'conv10.0.layer.0.bias', 'conv10.1.kernel', 'res.weight', 'res.bias', 'conv1.0.weight', 'conv1.1.weight', 'conv1.1.bias', 'conv1.1.running_mean', 'conv1.1.running_var', 'conv1.1.num_batches_tracked', 'conv2.1.0.conv1.weight', 'conv2.1.0.bn1.weight', 'conv2.1.0.bn1.bias', 'conv2.1.0.bn1.running_mean', 'conv2.1.0.bn1.running_var', 'conv2.1.0.bn1.num_batches_tracked', 'conv2.1.0.conv2.weight', 'conv2.1.0.bn2.weight', 'conv2.1.0.bn2.bias', 'conv2.1.0.bn2.running_mean', 'conv2.1.0.bn2.running_var', 'conv2.1.0.bn2.num_batches_tracked', 'conv2.1.0.conv3.weight', 'conv2.1.0.bn3.weight', 'conv2.1.0.bn3.bias', 'conv2.1.0.bn3.running_mean', 'conv2.1.0.bn3.running_var', 'conv2.1.0.bn3.num_batches_tracked', 'conv2.1.0.se_module.fc1.weight', 'conv2.1.0.se_module.fc1.bias', 'conv2.1.0.se_module.fc2.weight', 'conv2.1.0.se_module.fc2.bias', 'conv2.1.0.downsample.0.weight', 'conv2.1.0.downsample.1.weight', 'conv2.1.0.downsample.1.bias', 'conv2.1.0.downsample.1.running_mean', 'conv2.1.0.downsample.1.running_var', 'conv2.1.0.downsample.1.num_batches_tracked', 'conv2.1.1.conv1.weight', 'conv2.1.1.bn1.weight', 'conv2.1.1.bn1.bias', 'conv2.1.1.bn1.running_mean', 'conv2.1.1.bn1.running_var', 'conv2.1.1.bn1.num_batches_tracked', 'conv2.1.1.conv2.weight', 'conv2.1.1.bn2.weight', 'conv2.1.1.bn2.bias', 'conv2.1.1.bn2.running_mean', 'conv2.1.1.bn2.running_var', 'conv2.1.1.bn2.num_batches_tracked', 'conv2.1.1.conv3.weight', 'conv2.1.1.bn3.weight', 'conv2.1.1.bn3.bias', 'conv2.1.1.bn3.running_mean', 'conv2.1.1.bn3.running_var', 'conv2.1.1.bn3.num_batches_tracked', 'conv2.1.1.se_module.fc1.weight', 'conv2.1.1.se_module.fc1.bias', 'conv2.1.1.se_module.fc2.weight', 'conv2.1.1.se_module.fc2.bias', 'conv2.1.2.conv1.weight', 'conv2.1.2.bn1.weight', 'conv2.1.2.bn1.bias', 'conv2.1.2.bn1.running_mean', 'conv2.1.2.bn1.running_var', 'conv2.1.2.bn1.num_batches_tracked', 'conv2.1.2.conv2.weight', 'conv2.1.2.bn2.weight', 'conv2.1.2.bn2.bias', 'conv2.1.2.bn2.running_mean', 'conv2.1.2.bn2.running_var', 'conv2.1.2.bn2.num_batches_tracked', 'conv2.1.2.conv3.weight', 'conv2.1.2.bn3.weight', 'conv2.1.2.bn3.bias', 'conv2.1.2.bn3.running_mean', 'conv2.1.2.bn3.running_var', 'conv2.1.2.bn3.num_batches_tracked', 'conv2.1.2.se_module.fc1.weight', 'conv2.1.2.se_module.fc1.bias', 'conv2.1.2.se_module.fc2.weight', 'conv2.1.2.se_module.fc2.bias', 'conv3.0.conv1.weight', 'conv3.0.bn1.weight', 'conv3.0.bn1.bias', 'conv3.0.bn1.running_mean', 'conv3.0.bn1.running_var', 'conv3.0.bn1.num_batches_tracked', 'conv3.0.conv2.weight', 'conv3.0.bn2.weight', 'conv3.0.bn2.bias', 'conv3.0.bn2.running_mean', 'conv3.0.bn2.running_var', 'conv3.0.bn2.num_batches_tracked', 'conv3.0.conv3.weight', 'conv3.0.bn3.weight', 'conv3.0.bn3.bias', 'conv3.0.bn3.running_mean', 'conv3.0.bn3.running_var', 'conv3.0.bn3.num_batches_tracked', 'conv3.0.se_module.fc1.weight', 'conv3.0.se_module.fc1.bias', 'conv3.0.se_module.fc2.weight', 'conv3.0.se_module.fc2.bias', 'conv3.0.downsample.0.weight', 'conv3.0.downsample.1.weight', 'conv3.0.downsample.1.bias', 'conv3.0.downsample.1.running_mean', 'conv3.0.downsample.1.running_var', 'conv3.0.downsample.1.num_batches_tracked', 'conv3.1.conv1.weight', 'conv3.1.bn1.weight', 'conv3.1.bn1.bias', 'conv3.1.bn1.running_mean', 'conv3.1.bn1.running_var', 'conv3.1.bn1.num_batches_tracked', 'conv3.1.conv2.weight', 'conv3.1.bn2.weight', 'conv3.1.bn2.bias', 'conv3.1.bn2.running_mean', 'conv3.1.bn2.running_var', 'conv3.1.bn2.num_batches_tracked', 'conv3.1.conv3.weight', 'conv3.1.bn3.weight', 'conv3.1.bn3.bias', 'conv3.1.bn3.running_mean', 'conv3.1.bn3.running_var', 'conv3.1.bn3.num_batches_tracked', 'conv3.1.se_module.fc1.weight', 'conv3.1.se_module.fc1.bias', 'conv3.1.se_module.fc2.weight', 'conv3.1.se_module.fc2.bias', 'conv3.2.conv1.weight', 'conv3.2.bn1.weight', 'conv3.2.bn1.bias', 'conv3.2.bn1.running_mean', 'conv3.2.bn1.running_var', 'conv3.2.bn1.num_batches_tracked', 'conv3.2.conv2.weight', 'conv3.2.bn2.weight', 'conv3.2.bn2.bias', 'conv3.2.bn2.running_mean', 'conv3.2.bn2.running_var', 'conv3.2.bn2.num_batches_tracked', 'conv3.2.conv3.weight', 'conv3.2.bn3.weight', 'conv3.2.bn3.bias', 'conv3.2.bn3.running_mean', 'conv3.2.bn3.running_var', 'conv3.2.bn3.num_batches_tracked', 'conv3.2.se_module.fc1.weight', 'conv3.2.se_module.fc1.bias', 'conv3.2.se_module.fc2.weight', 'conv3.2.se_module.fc2.bias', 'conv3.3.conv1.weight', 'conv3.3.bn1.weight', 'conv3.3.bn1.bias', 'conv3.3.bn1.running_mean', 'conv3.3.bn1.running_var', 'conv3.3.bn1.num_batches_tracked', 'conv3.3.conv2.weight', 'conv3.3.bn2.weight', 'conv3.3.bn2.bias', 'conv3.3.bn2.running_mean', 'conv3.3.bn2.running_var', 'conv3.3.bn2.num_batches_tracked', 'conv3.3.conv3.weight', 'conv3.3.bn3.weight', 'conv3.3.bn3.bias', 'conv3.3.bn3.running_mean', 'conv3.3.bn3.running_var', 'conv3.3.bn3.num_batches_tracked', 'conv3.3.se_module.fc1.weight', 'conv3.3.se_module.fc1.bias', 'conv3.3.se_module.fc2.weight', 'conv3.3.se_module.fc2.bias', 'conv4.0.conv1.weight', 'conv4.0.bn1.weight', 'conv4.0.bn1.bias', 'conv4.0.bn1.running_mean', 'conv4.0.bn1.running_var', 'conv4.0.bn1.num_batches_tracked', 'conv4.0.conv2.weight', 'conv4.0.bn2.weight', 'conv4.0.bn2.bias', 'conv4.0.bn2.running_mean', 'conv4.0.bn2.running_var', 'conv4.0.bn2.num_batches_tracked', 'conv4.0.conv3.weight', 'conv4.0.bn3.weight', 'conv4.0.bn3.bias', 'conv4.0.bn3.running_mean', 'conv4.0.bn3.running_var', 'conv4.0.bn3.num_batches_tracked', 'conv4.0.se_module.fc1.weight', 'conv4.0.se_module.fc1.bias', 'conv4.0.se_module.fc2.weight', 'conv4.0.se_module.fc2.bias', 'conv4.0.downsample.0.weight', 'conv4.0.downsample.1.weight', 'conv4.0.downsample.1.bias', 'conv4.0.downsample.1.running_mean', 'conv4.0.downsample.1.running_var', 'conv4.0.downsample.1.num_batches_tracked', 'conv4.1.conv1.weight', 'conv4.1.bn1.weight', 'conv4.1.bn1.bias', 'conv4.1.bn1.running_mean', 'conv4.1.bn1.running_var', 'conv4.1.bn1.num_batches_tracked', 'conv4.1.conv2.weight', 'conv4.1.bn2.weight', 'conv4.1.bn2.bias', 'conv4.1.bn2.running_mean', 'conv4.1.bn2.running_var', 'conv4.1.bn2.num_batches_tracked', 'conv4.1.conv3.weight', 'conv4.1.bn3.weight', 'conv4.1.bn3.bias', 'conv4.1.bn3.running_mean', 'conv4.1.bn3.running_var', 'conv4.1.bn3.num_batches_tracked', 'conv4.1.se_module.fc1.weight', 'conv4.1.se_module.fc1.bias', 'conv4.1.se_module.fc2.weight', 'conv4.1.se_module.fc2.bias', 'conv4.2.conv1.weight', 'conv4.2.bn1.weight', 'conv4.2.bn1.bias', 'conv4.2.bn1.running_mean', 'conv4.2.bn1.running_var', 'conv4.2.bn1.num_batches_tracked', 'conv4.2.conv2.weight', 'conv4.2.bn2.weight', 'conv4.2.bn2.bias', 'conv4.2.bn2.running_mean', 'conv4.2.bn2.running_var', 'conv4.2.bn2.num_batches_tracked', 'conv4.2.conv3.weight', 'conv4.2.bn3.weight', 'conv4.2.bn3.bias', 'conv4.2.bn3.running_mean', 'conv4.2.bn3.running_var', 'conv4.2.bn3.num_batches_tracked', 'conv4.2.se_module.fc1.weight', 'conv4.2.se_module.fc1.bias', 'conv4.2.se_module.fc2.weight', 'conv4.2.se_module.fc2.bias', 'conv4.3.conv1.weight', 'conv4.3.bn1.weight', 'conv4.3.bn1.bias', 'conv4.3.bn1.running_mean', 'conv4.3.bn1.running_var', 'conv4.3.bn1.num_batches_tracked', 'conv4.3.conv2.weight', 'conv4.3.bn2.weight', 'conv4.3.bn2.bias', 'conv4.3.bn2.running_mean', 'conv4.3.bn2.running_var', 'conv4.3.bn2.num_batches_tracked', 'conv4.3.conv3.weight', 'conv4.3.bn3.weight', 'conv4.3.bn3.bias', 'conv4.3.bn3.running_mean', 'conv4.3.bn3.running_var', 'conv4.3.bn3.num_batches_tracked', 'conv4.3.se_module.fc1.weight', 'conv4.3.se_module.fc1.bias', 'conv4.3.se_module.fc2.weight', 'conv4.3.se_module.fc2.bias', 'conv4.4.conv1.weight', 'conv4.4.bn1.weight', 'conv4.4.bn1.bias', 'conv4.4.bn1.running_mean', 'conv4.4.bn1.running_var', 'conv4.4.bn1.num_batches_tracked', 'conv4.4.conv2.weight', 'conv4.4.bn2.weight', 'conv4.4.bn2.bias', 'conv4.4.bn2.running_mean', 'conv4.4.bn2.running_var', 'conv4.4.bn2.num_batches_tracked', 'conv4.4.conv3.weight', 'conv4.4.bn3.weight', 'conv4.4.bn3.bias', 'conv4.4.bn3.running_mean', 'conv4.4.bn3.running_var', 'conv4.4.bn3.num_batches_tracked', 'conv4.4.se_module.fc1.weight', 'conv4.4.se_module.fc1.bias', 'conv4.4.se_module.fc2.weight', 'conv4.4.se_module.fc2.bias', 'conv4.5.conv1.weight', 'conv4.5.bn1.weight', 'conv4.5.bn1.bias', 'conv4.5.bn1.running_mean', 'conv4.5.bn1.running_var', 'conv4.5.bn1.num_batches_tracked', 'conv4.5.conv2.weight', 'conv4.5.bn2.weight', 'conv4.5.bn2.bias', 'conv4.5.bn2.running_mean', 'conv4.5.bn2.running_var', 'conv4.5.bn2.num_batches_tracked', 'conv4.5.conv3.weight', 'conv4.5.bn3.weight', 'conv4.5.bn3.bias', 'conv4.5.bn3.running_mean', 'conv4.5.bn3.running_var', 'conv4.5.bn3.num_batches_tracked', 'conv4.5.se_module.fc1.weight', 'conv4.5.se_module.fc1.bias', 'conv4.5.se_module.fc2.weight', 'conv4.5.se_module.fc2.bias', 'conv5.0.conv1.weight', 'conv5.0.bn1.weight', 'conv5.0.bn1.bias', 'conv5.0.bn1.running_mean', 'conv5.0.bn1.running_var', 'conv5.0.bn1.num_batches_tracked', 'conv5.0.conv2.weight', 'conv5.0.bn2.weight', 'conv5.0.bn2.bias', 'conv5.0.bn2.running_mean', 'conv5.0.bn2.running_var', 'conv5.0.bn2.num_batches_tracked', 'conv5.0.conv3.weight', 'conv5.0.bn3.weight', 'conv5.0.bn3.bias', 'conv5.0.bn3.running_mean', 'conv5.0.bn3.running_var', 'conv5.0.bn3.num_batches_tracked', 'conv5.0.se_module.fc1.weight', 'conv5.0.se_module.fc1.bias', 'conv5.0.se_module.fc2.weight', 'conv5.0.se_module.fc2.bias', 'conv5.0.downsample.0.weight', 'conv5.0.downsample.1.weight', 'conv5.0.downsample.1.bias', 'conv5.0.downsample.1.running_mean', 'conv5.0.downsample.1.running_var', 'conv5.0.downsample.1.num_batches_tracked', 'conv5.1.conv1.weight', 'conv5.1.bn1.weight', 'conv5.1.bn1.bias', 'conv5.1.bn1.running_mean', 'conv5.1.bn1.running_var', 'conv5.1.bn1.num_batches_tracked', 'conv5.1.conv2.weight', 'conv5.1.bn2.weight', 'conv5.1.bn2.bias', 'conv5.1.bn2.running_mean', 'conv5.1.bn2.running_var', 'conv5.1.bn2.num_batches_tracked', 'conv5.1.conv3.weight', 'conv5.1.bn3.weight', 'conv5.1.bn3.bias', 'conv5.1.bn3.running_mean', 'conv5.1.bn3.running_var', 'conv5.1.bn3.num_batches_tracked', 'conv5.1.se_module.fc1.weight', 'conv5.1.se_module.fc1.bias', 'conv5.1.se_module.fc2.weight', 'conv5.1.se_module.fc2.bias', 'conv5.2.conv1.weight', 'conv5.2.bn1.weight', 'conv5.2.bn1.bias', 'conv5.2.bn1.running_mean', 'conv5.2.bn1.running_var', 'conv5.2.bn1.num_batches_tracked', 'conv5.2.conv2.weight', 'conv5.2.bn2.weight', 'conv5.2.bn2.bias', 'conv5.2.bn2.running_mean', 'conv5.2.bn2.running_var', 'conv5.2.bn2.num_batches_tracked', 'conv5.2.conv3.weight', 'conv5.2.bn3.weight', 'conv5.2.bn3.bias', 'conv5.2.bn3.running_mean', 'conv5.2.bn3.running_var', 'conv5.2.bn3.num_batches_tracked', 'conv5.2.se_module.fc1.weight', 'conv5.2.se_module.fc1.bias', 'conv5.2.se_module.fc2.weight', 'conv5.2.se_module.fc2.bias'])"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["# # Congelar todas as camadas\n","# for param in model.parameters():\n","#     param.requires_grad = False\n","\n","# # Descongelar apenas a camada morfológica em 'conv10'\n","# for param in model.conv10[1].parameters():  # Assumindo que a camada morfológica é o segundo elemento em `conv10` após nossa modificação anterior\n","#     param.requires_grad = True"],"metadata":{"id":"0DZ0M08-oHFz","executionInfo":{"status":"ok","timestamp":1698172495393,"user_tz":180,"elapsed":614,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qFTYPcdBccNc","outputId":"c68f6081-63ca-4ef1-d630-7dc4c830559f","executionInfo":{"status":"ok","timestamp":1698185213499,"user_tz":180,"elapsed":12714356,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 0; lr 0.0002020; Loss 0.0804 (1.7066); cce_loss 0.0804 (1.7066); Dice 0.0000 (0.0001): 100%|██████████| 1227/1227 [28:24<00:00,  1.39s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 0; lr 0.0002020; Loss 1.7066; CCE_loss 1.7066; Dice 0.0001\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 1; lr 0.0002020; Loss 0.0387 (0.1017); cce_loss 0.0387 (0.1017); Dice 0.0016 (0.0002): 100%|██████████| 1227/1227 [13:41<00:00,  1.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 1; lr 0.0002020; Loss 0.1017; CCE_loss 0.1017; Dice 0.0002\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 2; lr 0.0002020; Loss 0.0929 (0.0952); cce_loss 0.0929 (0.0952); Dice 0.0210 (0.0168): 100%|██████████| 1227/1227 [12:50<00:00,  1.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 2; lr 0.0002020; Loss 0.0952; CCE_loss 0.0952; Dice 0.0168\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 3; lr 0.0002020; Loss 0.1332 (0.0931); cce_loss 0.1332 (0.0931); Dice 0.0798 (0.0433): 100%|██████████| 1227/1227 [12:22<00:00,  1.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 3; lr 0.0002020; Loss 0.0931; CCE_loss 0.0931; Dice 0.0433\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 4; lr 0.0002020; Loss 0.1487 (0.0879); cce_loss 0.1487 (0.0879); Dice 0.0371 (0.0441): 100%|██████████| 1227/1227 [12:24<00:00,  1.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 4; lr 0.0000505; Loss 0.0879; CCE_loss 0.0879; Dice 0.0441\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 5; lr 0.0000505; Loss 0.0606 (0.0803); cce_loss 0.0606 (0.0803); Dice 0.0696 (0.0523): 100%|██████████| 1227/1227 [12:18<00:00,  1.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 5; lr 0.0001010; Loss 0.0803; CCE_loss 0.0803; Dice 0.0523\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 6; lr 0.0001010; Loss 0.0859 (0.0776); cce_loss 0.0859 (0.0776); Dice 0.0714 (0.0565): 100%|██████████| 1227/1227 [12:13<00:00,  1.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 6; lr 0.0001010; Loss 0.0776; CCE_loss 0.0776; Dice 0.0565\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 91/91 [10:47<00:00,  7.12s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.7646602116125059, Dice: 1.0, F1: 0.6638003023035798, F1_0: 0.9031039470080767, F1_1: 0.4366234815798451, F1_2: 0.7170808490387911, F1_3: 0.8105168555579595\n","score: [0.7646602116125059, 1.0, 0.6638003023035798, 0.9031039470080767, 0.4366234815798451, 0.7170808490387911, 0.8105168555579595]\tscore_best: 0.7646602116125059\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 7; lr 0.0001010; Loss 0.0809 (0.0760); cce_loss 0.0809 (0.0760); Dice 0.0567 (0.0421): 100%|██████████| 1227/1227 [38:35<00:00,  1.89s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 7; lr 0.0001010; Loss 0.0760; CCE_loss 0.0760; Dice 0.0421\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 8; lr 0.0001010; Loss 0.1432 (0.0733); cce_loss 0.1432 (0.0733); Dice 0.0693 (0.0499): 100%|██████████| 1227/1227 [14:14<00:00,  1.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 8; lr 0.0001010; Loss 0.0733; CCE_loss 0.0733; Dice 0.0499\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 91/91 [09:38<00:00,  6.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Val Score: 0.6706470515680438, Dice: 1.0, F1: 0.5294957879543485, F1_0: 0.8212393732967991, F1_1: 0.2703886123867823, F1_2: 0.6813019533356246, F1_3: 0.8543029557099263\n","score: [0.6706470515680438, 1.0, 0.5294957879543485, 0.8212393732967991, 0.2703886123867823, 0.6813019533356246, 0.8543029557099263]\tscore_best: 0.7646602116125059\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1227 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:432: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","epoch: 9; lr 0.0001010; Loss 0.0433 (0.0740); cce_loss 0.0433 (0.0740); Dice 0.0573 (0.0660): 100%|██████████| 1227/1227 [34:15<00:00,  1.68s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 9; lr 0.0001010; Loss 0.0740; CCE_loss 0.0740; Dice 0.0660\n","Time: 211.893 min\n"]}],"source":["import ssl\n","\n","random_combinations = [\n","    (['after_conv10'],['opening'], [5], ['square']),\n","]\n","\n","for layers, ops, k_sizes, shape_types in random_combinations:\n","    # Crie um novo modelo aqui. Você pode usar algo como:\n","    ssl._create_default_https_context = ssl._create_unverified_context\n","    t0 = timeit.default_timer()\n","\n","    # for layer, morph_op, k_size, shape in zip(layers, ops, k_sizes, shape_types):\n","    #   if layer == 'after_conv2':\n","    #       in_channels = get_out_channels_from_conv2(model, 'conv2')\n","    #       print(f\"Conv2 out channels: {in_channels}\")  # Adicionando uma mensagem de print para diagnóstico\n","    #       morph_layer = get_morph_layer('opening', k_size, shape, in_channels)\n","    #       model.conv2 = nn.Sequential(model.conv2, morph_layer)\n","\n","    #       in_channels = get_out_channels_from_conv2(model, 'conv2')\n","    #       print(f\"Conv2 out channels after opening: {in_channels}\")  # Adicionando uma mensagem de print para diagnóstico\n","    #       morph_layer_2 = get_morph_layer('closing', k_size, shape, in_channels)\n","    #       model.conv2 = nn.Sequential(model.conv2, morph_layer_2)\n","\n","    #print(list(filter(None, [n if n in ['conv2.0.1.kernel','conv2.1.kernel','conv3.1.kernel'] else None for n in model.state_dict().keys()])))\n","\n","    params = model.parameters()\n","\n","    optimizer = AdamW(params, lr=0.000202, weight_decay=1e-6)     #0.002\n","\n","    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[5, 13, 19, 23, 28, 47, 50, 60, 70, 90, 110, 130, 150, 170, 180, 190], gamma=0.5)\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    model = nn.DataParallel(model).cuda()\n","\n","    seg_loss = ComboLoss({'dice': 0.5, 'focal': 2.0}, per_image=False).cuda()\n","    ce_loss = nn.CrossEntropyLoss().cuda()\n","    seg_lossSeesaw = None#SeesawLoss2().cuda()\n","\n","    best_score = 0\n","    torch.cuda.empty_cache()\n","    for epoch in range(10):\n","        train_metrics = train_epoch(epoch, seg_loss, ce_loss, seg_lossSeesaw, model, optimizer, scheduler, train_data_loader)\n","        torch.cuda.empty_cache()\n","        if epoch % 2 == 0 and epoch >= 5:\n","          torch.save(model.state_dict(), f'/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/after_conv10_{ops}_{k_sizes}_{shape_types}4.pth')\n","          best_score, d = evaluate_val(val_data_loader, best_score, model, snapshot_name, epoch)\n","        results = pd.read_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_3_top_train4.csv')\n","        try:\n","          results_iter = pd.DataFrame([[epoch, train_metrics[0], train_metrics[1].avg, train_metrics[2].avg, train_metrics[3].avg, layers, ops, k_sizes, shape_types, d[0], d[1], d[2], d[3], d[4], d[5], d[6]]],columns=results.columns)\n","        except:\n","          results_iter = pd.DataFrame([[epoch, train_metrics[0], train_metrics[1].avg, train_metrics[2].avg, train_metrics[3].avg, layers, ops, k_sizes, shape_types, None, None, None, None, None, None, None]],columns=results.columns)\n","        results = pd.concat([results,results_iter])\n","        d = None\n","        results.to_csv('/content/drive/MyDrive/Modeling Satelities Images Building Damaged/models/test_train_6/results_3_top_train4.csv', index=False)\n","\n","    try:\n","      #del model\n","      torch.cuda.empty_cache()\n","    except:\n","      None\n","\n","    elapsed = timeit.default_timer() - t0\n","    print('Time: {:.3f} min'.format(elapsed / 60))"]},{"cell_type":"code","source":["del model\n","torch.cuda.empty_cache()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":200},"id":"D2ULYI3y6pMz","executionInfo":{"status":"error","timestamp":1698171986031,"user_tz":180,"elapsed":544,"user":{"displayName":"Antonio Santos","userId":"03265893617080544823"}},"outputId":"d0d0a3b5-b38e-4e43-eb50-9d767bdbb4d4"},"execution_count":65,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-65-b0cec479cb08>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WiAQ0sh12alo"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["xZ0IEwYcRzAl","TlRP-duNXnpK","yUSNtuVeTUwl","pTW3yt84TQMf","FXa807PbTjxn"],"machine_shape":"hm","provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyMIBT37SQI+S8scObbnFBDQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}